--- MrFAST.c
+++ MrFAST.c
@@ -48,9 +48,7 @@
 #include <string.h>
 #include <math.h>
 #include <dirent.h>
-#include <xmmintrin.h>
-#include <emmintrin.h>
-#include <mmintrin.h>
+#include <vec128int.h>
 
 #include "Common.h"
 #include "Reads.h"
@@ -194,14 +192,14 @@
 {
   int i = 0;
 
-  MASK = _mm_insert_epi16(MASK,1,0);
-  MASK = _mm_insert_epi16(MASK,1,1);
-  MASK = _mm_insert_epi16(MASK,1,2);
-  MASK = _mm_insert_epi16(MASK,1,3);
-  MASK = _mm_insert_epi16(MASK,1,4);
-  MASK = _mm_insert_epi16(MASK,0,5);
-  MASK = _mm_insert_epi16(MASK,0,6);
-  MASK = _mm_insert_epi16(MASK,0,7);
+  MASK = vec_insert8sh(MASK,1,0);
+  MASK = vec_insert8sh(MASK,1,1);
+  MASK = vec_insert8sh(MASK,1,2);
+  MASK = vec_insert8sh(MASK,1,3);
+  MASK = vec_insert8sh(MASK,1,4);
+  MASK = vec_insert8sh(MASK,0,5);
+  MASK = vec_insert8sh(MASK,0,6);
+  MASK = vec_insert8sh(MASK,0,7);
 
   for(i = 0; i < errThreshold + 1; i++)
     {
@@ -253,23 +251,23 @@
   __m128i Result;
 
   /* initialize */
-  R0 = _mm_setzero_si128();
-  R1 = _mm_setzero_si128();
-  Diag = _mm_setzero_si128();
-  Side1 = _mm_setzero_si128();
-  Side2 = _mm_setzero_si128();
-  Down1 = _mm_setzero_si128();
-  Down2 = _mm_setzero_si128();
-  SeqA = _mm_setzero_si128();
-  SeqB = _mm_setzero_si128();
-  Result = _mm_setzero_si128();
+  R0 = vec_zero1q();
+  R1 = vec_zero1q();
+  Diag = vec_zero1q();
+  Side1 = vec_zero1q();
+  Side2 = vec_zero1q();
+  Down1 = vec_zero1q();
+  Down2 = vec_zero1q();
+  SeqA = vec_zero1q();
+  SeqB = vec_zero1q();
+  Result = vec_zero1q();
   /* end initialize */
 
-  R1 = _mm_xor_si128(R1, R1);
-  R0 = _mm_xor_si128(R0, R0);
+  R1 = vec_bitxor1q(R1, R1);
+  R0 = vec_bitxor1q(R0, R0);
 
-  Diag = _mm_xor_si128(Diag, Diag);
-  Diag = _mm_insert_epi16(Diag,minError,0);
+  Diag = vec_bitxor1q(Diag, Diag);
+  Diag = vec_insert8sh(Diag,minError,0);
 
   i0 = (a[0] != b[0]);
   i1 = min(i0, ( *(a-1)!=*b) ) + 1;
@@ -279,50 +277,50 @@
   i4 = min(i1, ( *(a-2)!=b[0] )+1) + 1;
   i5 = min(i2, (a[0] != *(b-2))+1) + 1;
 
-  R1 = _mm_insert_epi16(R1, 3, 0);
-  R1 = _mm_insert_epi16(R1, i1, 1);
-  R1 = _mm_insert_epi16(R1, i2, 2);
-  R1 = _mm_insert_epi16(R1, 3, 3);
+  R1 = vec_insert8sh(R1, 3, 0);
+  R1 = vec_insert8sh(R1, i1, 1);
+  R1 = vec_insert8sh(R1, i2, 2);
+  R1 = vec_insert8sh(R1, 3, 3);
 
-  R0 = _mm_insert_epi16(R0, 4, 0);
-  R0 = _mm_insert_epi16(R0, i4, 1);
-  R0 = _mm_insert_epi16(R0, i0, 2);
-  R0 = _mm_insert_epi16(R0, i5, 3);
-  R0 = _mm_insert_epi16(R0, 4, 4);
+  R0 = vec_insert8sh(R0, 4, 0);
+  R0 = vec_insert8sh(R0, i4, 1);
+  R0 = vec_insert8sh(R0, i0, 2);
+  R0 = vec_insert8sh(R0, i5, 3);
+  R0 = vec_insert8sh(R0, 4, 4);
 
-  Side2 = _mm_xor_si128(Side2, Side2);
-  Down2 = _mm_xor_si128(Down2, Down2);
-  Down1 = _mm_xor_si128(Down1, Down1);
-  Side1 = _mm_xor_si128(Side1, Side1);
+  Side2 = vec_bitxor1q(Side2, Side2);
+  Down2 = vec_bitxor1q(Down2, Down2);
+  Down1 = vec_bitxor1q(Down1, Down1);
+  Side1 = vec_bitxor1q(Side1, Side1);
 
-  Side2 = _mm_insert_epi16(Side2,minError,0);
-  Down1 = _mm_insert_epi16(Down1,minError,0);
+  Side2 = vec_insert8sh(Side2,minError,0);
+  Down1 = vec_insert8sh(Down1,minError,0);
 
-  Side1 = _mm_insert_epi16(Side1,1,0);
+  Side1 = vec_insert8sh(Side1,1,0);
 
   index = 0;
   for (j = 0; j < e; j++) {
-    Side2 = _mm_slli_si128(Side2, 2);
-    Side2 = _mm_insert_epi16(Side2,1,0);
+    Side2 = vec_shiftleftbytes1q(Side2, 2);
+    Side2 = vec_insert8sh(Side2,1,0);
 
-    Down1 = _mm_slli_si128(Down1, 2);
-    Down1 = _mm_insert_epi16(Down1,1,0);
+    Down1 = vec_shiftleftbytes1q(Down1, 2);
+    Down1 = vec_insert8sh(Down1,1,0);
 
-    Down2 = _mm_slli_si128(Down2, 2);
-    Down2 = _mm_insert_epi16(Down2,1,0);
+    Down2 = vec_shiftleftbytes1q(Down2, 2);
+    Down2 = vec_insert8sh(Down2,1,0);
 
-    Side1 = _mm_slli_si128(Side1, 2);
-    Side1 = _mm_insert_epi16(Side1,1,0);
+    Side1 = vec_shiftleftbytes1q(Side1, 2);
+    Side1 = vec_insert8sh(Side1,1,0);
 
-    SeqA = _mm_slli_si128(SeqA, 2);
-    SeqB = _mm_slli_si128(SeqB, 2);
-    SeqA = _mm_insert_epi16(SeqA,*(a-index),0);
-    SeqB = _mm_insert_epi16(SeqB,*(b-index),0);
+    SeqA = vec_shiftleftbytes1q(SeqA, 2);
+    SeqB = vec_shiftleftbytes1q(SeqB, 2);
+    SeqA = vec_insert8sh(SeqA,*(a-index),0);
+    SeqB = vec_insert8sh(SeqB,*(b-index),0);
     index++;
   }
 
-  Down2 = _mm_slli_si128(Down2, 2);
-  Down2 = _mm_insert_epi16(Down2,minError,0);
+  Down2 = vec_shiftleftbytes1q(Down2, 2);
+  Down2 = vec_insert8sh(Down2,minError,0);
 
   index = 4;
   i = 5;
@@ -330,58 +328,58 @@
   int loopEnd = 2 * lenb - (e - 1);
   for (; i <= loopEnd; i++) {
 
-    Diag = _mm_xor_si128(Diag, Diag);
+    Diag = vec_bitxor1q(Diag, Diag);
     if (i % 2 == 0) {
-      SeqA = _mm_slli_si128(SeqA, 2);
-      SeqB = _mm_slli_si128(SeqB, 2);
-      SeqA = _mm_insert_epi16(SeqA,*(a-(index)),0);
-      SeqB = _mm_insert_epi16(SeqB,*(b-(index)),0);
+      SeqA = vec_shiftleftbytes1q(SeqA, 2);
+      SeqB = vec_shiftleftbytes1q(SeqB, 2);
+      SeqA = vec_insert8sh(SeqA,*(a-(index)),0);
+      SeqB = vec_insert8sh(SeqB,*(b-(index)),0);
 
       index++;
 
-      tmp = _mm_shufflelo_epi16(SeqB,27);
-      tmp = _mm_slli_si128(tmp, 2);
-      tmpValue = _mm_extract_epi16(tmp, 5);
-      tmp = _mm_insert_epi16(tmp, tmpValue, 0);
-
-      Result = _mm_cmpeq_epi16(SeqA, tmp);
-      Diag = _mm_andnot_si128(Result, MASK);
-
-      R0 = _mm_min_epi16(_mm_add_epi16(R1,Side2), _mm_add_epi16(R0,Diag));
-      R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_slli_si128(R1,2) ,Down2));
-
-      if (_mm_extract_epi16(R0, 0) > errThreshold
-	  && _mm_extract_epi16(R0, 1) > errThreshold
-	  && _mm_extract_epi16(R0, 2) > errThreshold
-	  && _mm_extract_epi16(R0, 3) > errThreshold
-	  && _mm_extract_epi16(R0, 4) > errThreshold
-	  && _mm_extract_epi16(R1, 0) > errThreshold
-	  && _mm_extract_epi16(R1, 1) > errThreshold
-	  && _mm_extract_epi16(R1, 2) > errThreshold
-	  && _mm_extract_epi16(R1, 3) > errThreshold)
+      tmp = vec_permutelower4sh(SeqB,27);
+      tmp = vec_shiftleftbytes1q(tmp, 2);
+      tmpValue = vec_extract8sh(tmp, 5);
+      tmp = vec_insert8sh(tmp, tmpValue, 0);
+
+      Result = vec_compareeq8sh(SeqA, tmp);
+      Diag = vec_bitandnotleft1q(Result, MASK);
+
+      R0 = vec_min8sh(vec_add8sh(R1,Side2), vec_add8sh(R0,Diag));
+      R0 = vec_min8sh(R0, vec_add8sh(vec_shiftleftbytes1q(R1,2) ,Down2));
+
+      if (vec_extract8sh(R0, 0) > errThreshold
+	  && vec_extract8sh(R0, 1) > errThreshold
+	  && vec_extract8sh(R0, 2) > errThreshold
+	  && vec_extract8sh(R0, 3) > errThreshold
+	  && vec_extract8sh(R0, 4) > errThreshold
+	  && vec_extract8sh(R1, 0) > errThreshold
+	  && vec_extract8sh(R1, 1) > errThreshold
+	  && vec_extract8sh(R1, 2) > errThreshold
+	  && vec_extract8sh(R1, 3) > errThreshold)
 	return -1;
 
       if (i == 2 * lenb - e) {
-	tmp = _mm_srli_si128(R0,2);
+	tmp = vec_shiftrightbytes1q(R0,2);
 	for (k = 0; k < e - 1; k++)
-	  tmp = _mm_srli_si128(tmp,2);
-	minError = _mm_extract_epi16(tmp,0);
+	  tmp = vec_shiftrightbytes1q(tmp,2);
+	minError = vec_extract8sh(tmp,0);
       }
 
     }
 
     else {
-      Result = _mm_cmpeq_epi16(SeqA, _mm_shufflelo_epi16(SeqB,27));
-      Diag = _mm_andnot_si128(Result, MASK);
+      Result = vec_compareeq8sh(SeqA, vec_permutelower4sh(SeqB,27));
+      Diag = vec_bitandnotleft1q(Result, MASK);
 
-      R1 = _mm_min_epi16(_mm_add_epi16(_mm_srli_si128(R0,2),Side1), _mm_add_epi16(R1,Diag));
-      R1 = _mm_min_epi16(R1, _mm_add_epi16(R0 ,Down1));
+      R1 = vec_min8sh(vec_add8sh(vec_shiftrightbytes1q(R0,2),Side1), vec_add8sh(R1,Diag));
+      R1 = vec_min8sh(R1, vec_add8sh(R0 ,Down1));
 
       if (i >= 2 * lenb - e) {
-	tmp = _mm_srli_si128(R1,2);
+	tmp = vec_shiftrightbytes1q(R1,2);
 	for (k = 0; k < e - 2; k++)
-	  tmp = _mm_srli_si128(tmp,2);
-	minError = min(minError, _mm_extract_epi16(tmp,0));
+	  tmp = vec_shiftrightbytes1q(tmp,2);
+	minError = min(minError, vec_extract8sh(tmp,0));
       }
     }
 
@@ -391,83 +389,83 @@
   int tmpE = e;
   for (; j < 2 * (e - 2) + 1; j++) {
 
-    Diag = _mm_xor_si128(Diag, Diag);
+    Diag = vec_bitxor1q(Diag, Diag);
     //set the first element
     if (j == 0) {
       for (k = 0; k <= e - 1; k++) {
-	Diag = _mm_slli_si128(Diag, 2);
+	Diag = vec_shiftleftbytes1q(Diag, 2);
 	Diag =
-	  _mm_insert_epi16(Diag, *(b-(lenb-1-k)) != *(a-((i-lenb)-1+k)),0);
+	  vec_insert8sh(Diag, *(b-(lenb-1-k)) != *(a-((i-lenb)-1+k)),0);
       }
 
-      R0 = _mm_min_epi16(_mm_add_epi16(R1,Side2), _mm_add_epi16(R0,Diag));
-      R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_slli_si128(R1,2) ,Down2));
+      R0 = vec_min8sh(vec_add8sh(R1,Side2), vec_add8sh(R0,Diag));
+      R0 = vec_min8sh(R0, vec_add8sh(vec_shiftleftbytes1q(R1,2) ,Down2));
 
       tmpE--;
 
-      tmp = _mm_srli_si128(R0,2);
+      tmp = vec_shiftrightbytes1q(R0,2);
       for (k = 0; k < e - 2; k++)
-	tmp = _mm_srli_si128(tmp,2);
-      minError = min(minError, _mm_extract_epi16(tmp,0));
+	tmp = vec_shiftrightbytes1q(tmp,2);
+      minError = min(minError, vec_extract8sh(tmp,0));
     } else if (j % 2 == 0) {
       for (k = 0; k < tmpE; k++) {
-	Diag = _mm_slli_si128(Diag, 2);
+	Diag = vec_shiftleftbytes1q(Diag, 2);
 	Diag =
-	  _mm_insert_epi16(Diag, *(b-(lenb-1-k)) != *(a-((i-lenb)-1+k)),0);
+	  vec_insert8sh(Diag, *(b-(lenb-1-k)) != *(a-((i-lenb)-1+k)),0);
       }
 
-      R0 = _mm_min_epi16(_mm_add_epi16(R1,Side2), _mm_add_epi16(R0,Diag));
-      R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_slli_si128(R1,2) ,Down2));
+      R0 = vec_min8sh(vec_add8sh(R1,Side2), vec_add8sh(R0,Diag));
+      R0 = vec_min8sh(R0, vec_add8sh(vec_shiftleftbytes1q(R1,2) ,Down2));
 
       tmpE--;
 
-      tmp = _mm_srli_si128(R0,2);
+      tmp = vec_shiftrightbytes1q(R0,2);
       for (k = 0; k < tmpE - 1; k++)
-	tmp = _mm_srli_si128(tmp,2);
-      minError = min(minError, _mm_extract_epi16(tmp,0));
+	tmp = vec_shiftrightbytes1q(tmp,2);
+      minError = min(minError, vec_extract8sh(tmp,0));
     }
 
     else {
       for (k = 0; k < tmpE; k++) {
-	Diag = _mm_slli_si128(Diag, 2);
+	Diag = vec_shiftleftbytes1q(Diag, 2);
 	Diag =
-	  _mm_insert_epi16(Diag, *(b-(lenb-1-k)) != *(a-((i-lenb)-1+k)),0);
+	  vec_insert8sh(Diag, *(b-(lenb-1-k)) != *(a-((i-lenb)-1+k)),0);
       }
 
-      R1 = _mm_min_epi16(_mm_add_epi16(_mm_srli_si128(R0,2),Side1), _mm_add_epi16(R1,Diag));
-      R1 = _mm_min_epi16(R1, _mm_add_epi16(R0 ,Down1));
+      R1 = vec_min8sh(vec_add8sh(vec_shiftrightbytes1q(R0,2),Side1), vec_add8sh(R1,Diag));
+      R1 = vec_min8sh(R1, vec_add8sh(R0 ,Down1));
 
-      tmp = _mm_srli_si128(R1,2);
+      tmp = vec_shiftrightbytes1q(R1,2);
       for (k = 0; k < tmpE - 2; k++)
-	tmp = _mm_srli_si128(tmp,2);
-      minError = min(minError, _mm_extract_epi16(tmp,0));
+	tmp = vec_shiftrightbytes1q(tmp,2);
+      minError = min(minError, vec_extract8sh(tmp,0));
     }
     i++;
   }
   //Diag
 
-  Diag = _mm_xor_si128(Diag, Diag);
-  Diag = _mm_insert_epi16(Diag, 2*errThreshold, 0);
-  Diag = _mm_insert_epi16(Diag, *(a-(lenb+e-2)) != *(b-(lenb-1)), 1);
+  Diag = vec_bitxor1q(Diag, Diag);
+  Diag = vec_insert8sh(Diag, 2*errThreshold, 0);
+  Diag = vec_insert8sh(Diag, *(a-(lenb+e-2)) != *(b-(lenb-1)), 1);
 
-  Side1 = _mm_insert_epi16(Side1,1,0);
-  Side1 = _mm_insert_epi16(Side1,1,1);
+  Side1 = vec_insert8sh(Side1,1,0);
+  Side1 = vec_insert8sh(Side1,1,1);
 
-  Down1 = _mm_insert_epi16(Down1, 2*errThreshold, 0);
-  Down1 = _mm_insert_epi16(Down1, 1, 1);
+  Down1 = vec_insert8sh(Down1, 2*errThreshold, 0);
+  Down1 = vec_insert8sh(Down1, 1, 1);
 
-  R1 = _mm_min_epi16(_mm_add_epi16(R0,Side1), _mm_add_epi16(_mm_slli_si128(R1,2),Diag));
-  R1 = _mm_min_epi16(R1, _mm_add_epi16(_mm_slli_si128(R0,2),Down1));
+  R1 = vec_min8sh(vec_add8sh(R0,Side1), vec_add8sh(vec_shiftleftbytes1q(R1,2),Diag));
+  R1 = vec_min8sh(R1, vec_add8sh(vec_shiftleftbytes1q(R0,2),Down1));
 
-  minError = min(minError, _mm_extract_epi16(R1,1));
+  minError = min(minError, vec_extract8sh(R1,1));
 
-  Diag = _mm_insert_epi16(Diag, *(a-(lenb+e-1)) != *(b-(lenb-1)), 0);
-  Down1 = _mm_insert_epi16(Down1, 1, 0);
+  Diag = vec_insert8sh(Diag, *(a-(lenb+e-1)) != *(b-(lenb-1)), 0);
+  Down1 = vec_insert8sh(Down1, 1, 0);
 
-  R0 = _mm_min_epi16(_mm_add_epi16(R1,Down1), _mm_add_epi16(R0,Diag));
-  R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_srli_si128(R1,2) ,Side1));
+  R0 = vec_min8sh(vec_add8sh(R1,Down1), vec_add8sh(R0,Diag));
+  R0 = vec_min8sh(R0, vec_add8sh(vec_shiftrightbytes1q(R1,2) ,Side1));
 
-  minError = min(minError, _mm_extract_epi16(R0,0));
+  minError = min(minError, vec_extract8sh(R0,0));
 
   if (minError > mismatch)
     return -1;
@@ -514,23 +512,23 @@
   __m128i tmpSeqB;
 
   /* initialize */
-  R0 = _mm_setzero_si128();
-  R1 = _mm_setzero_si128();
-  Diag = _mm_setzero_si128();
-  Side1 = _mm_setzero_si128();
-  Side2 = _mm_setzero_si128();
-  Down1 = _mm_setzero_si128();
-  Down2 = _mm_setzero_si128();
-  SeqA = _mm_setzero_si128();
-  SeqB = _mm_setzero_si128();
-  Result = _mm_setzero_si128();
+  R0 = vec_zero1q();
+  R1 = vec_zero1q();
+  Diag = vec_zero1q();
+  Side1 = vec_zero1q();
+  Side2 = vec_zero1q();
+  Down1 = vec_zero1q();
+  Down2 = vec_zero1q();
+  SeqA = vec_zero1q();
+  SeqB = vec_zero1q();
+  Result = vec_zero1q();
   /* end initialize */
 
-  R1 = _mm_xor_si128(R1, R1);
-  R0 = _mm_xor_si128(R0, R0);
+  R1 = vec_bitxor1q(R1, R1);
+  R0 = vec_bitxor1q(R0, R0);
 
-  Diag = _mm_xor_si128(Diag, Diag);
-  Diag = _mm_insert_epi16(Diag,minError,0);
+  Diag = vec_bitxor1q(Diag, Diag);
+  Diag = vec_insert8sh(Diag,minError,0);
 
   i0 = (a[0] != b[0]);
   i1 = min(i0, (a[1]!=b[0])) + 1;
@@ -540,50 +538,50 @@
   i4 = min(i1, (a[2]!=b[0])+1) + 1;
   i5 = min(i2, (a[0]!=b[2])+1) + 1;
 
-  R1 = _mm_insert_epi16(R1, 3, 0);
-  R1 = _mm_insert_epi16(R1, i1, 1);
-  R1 = _mm_insert_epi16(R1, i2, 2);
-  R1 = _mm_insert_epi16(R1, 3, 3);
+  R1 = vec_insert8sh(R1, 3, 0);
+  R1 = vec_insert8sh(R1, i1, 1);
+  R1 = vec_insert8sh(R1, i2, 2);
+  R1 = vec_insert8sh(R1, 3, 3);
 
-  R0 = _mm_insert_epi16(R0, 4, 0);
-  R0 = _mm_insert_epi16(R0, i4, 1);
-  R0 = _mm_insert_epi16(R0, i0, 2);
-  R0 = _mm_insert_epi16(R0, i5, 3);
-  R0 = _mm_insert_epi16(R0, 4, 4);
+  R0 = vec_insert8sh(R0, 4, 0);
+  R0 = vec_insert8sh(R0, i4, 1);
+  R0 = vec_insert8sh(R0, i0, 2);
+  R0 = vec_insert8sh(R0, i5, 3);
+  R0 = vec_insert8sh(R0, 4, 4);
 
-  Side2 = _mm_xor_si128(Side2, Side2);
-  Down2 = _mm_xor_si128(Down2, Down2);
-  Down1 = _mm_xor_si128(Down1, Down1);
-  Side1 = _mm_xor_si128(Side1, Side1);
+  Side2 = vec_bitxor1q(Side2, Side2);
+  Down2 = vec_bitxor1q(Down2, Down2);
+  Down1 = vec_bitxor1q(Down1, Down1);
+  Side1 = vec_bitxor1q(Side1, Side1);
 
-  Side2 = _mm_insert_epi16(Side2,minError,0);
-  Down1 = _mm_insert_epi16(Down1,minError,0);
+  Side2 = vec_insert8sh(Side2,minError,0);
+  Down1 = vec_insert8sh(Down1,minError,0);
 
-  Side1 = _mm_insert_epi16(Side1,1,0);
+  Side1 = vec_insert8sh(Side1,1,0);
 
   index = 0;
   for (j = 0; j < e; j++) {
-    Side2 = _mm_slli_si128(Side2, 2);
-    Side2 = _mm_insert_epi16(Side2,1,0);
+    Side2 = vec_shiftleftbytes1q(Side2, 2);
+    Side2 = vec_insert8sh(Side2,1,0);
 
-    Down1 = _mm_slli_si128(Down1, 2);
-    Down1 = _mm_insert_epi16(Down1,1,0);
+    Down1 = vec_shiftleftbytes1q(Down1, 2);
+    Down1 = vec_insert8sh(Down1,1,0);
 
-    Down2 = _mm_slli_si128(Down2, 2);
-    Down2 = _mm_insert_epi16(Down2,1,0);
+    Down2 = vec_shiftleftbytes1q(Down2, 2);
+    Down2 = vec_insert8sh(Down2,1,0);
 
-    Side1 = _mm_slli_si128(Side1, 2);
-    Side1 = _mm_insert_epi16(Side1,1,0);
+    Side1 = vec_shiftleftbytes1q(Side1, 2);
+    Side1 = vec_insert8sh(Side1,1,0);
 
-    SeqA = _mm_slli_si128(SeqA, 2);
-    SeqB = _mm_slli_si128(SeqB, 2);
-    SeqA = _mm_insert_epi16(SeqA,a[index],0);
-    SeqB = _mm_insert_epi16(SeqB,b[index],0);
+    SeqA = vec_shiftleftbytes1q(SeqA, 2);
+    SeqB = vec_shiftleftbytes1q(SeqB, 2);
+    SeqA = vec_insert8sh(SeqA,a[index],0);
+    SeqB = vec_insert8sh(SeqB,b[index],0);
     index++;
   }
 
-  Down2 = _mm_slli_si128(Down2, 2);
-  Down2 = _mm_insert_epi16(Down2,minError,0);
+  Down2 = vec_shiftleftbytes1q(Down2, 2);
+  Down2 = vec_insert8sh(Down2,minError,0);
 
   index = 4;
   i = 5;
@@ -591,56 +589,56 @@
   int loopEnd = 2 * lenb - (e - 1);
   for (; i <= loopEnd; i++) {
     if (i % 2 == 0) {
-      tmpSeqA = _mm_slli_si128(SeqA, 2);
-      tmpSeqB = _mm_slli_si128(SeqB, 2);
-      SeqA = _mm_insert_epi16(tmpSeqA,a[index],0);
-      SeqB = _mm_insert_epi16(tmpSeqB,b[index],0);
+      tmpSeqA = vec_shiftleftbytes1q(SeqA, 2);
+      tmpSeqB = vec_shiftleftbytes1q(SeqB, 2);
+      SeqA = vec_insert8sh(tmpSeqA,a[index],0);
+      SeqB = vec_insert8sh(tmpSeqB,b[index],0);
 
       index++;
 
-      tmp = _mm_shufflelo_epi16(SeqB,27);
-      tmp = _mm_slli_si128(tmp, 2);
-      tmpValue = _mm_extract_epi16(tmp, 5);
-      tmp = _mm_insert_epi16(tmp, tmpValue, 0);
-
-      Result = _mm_cmpeq_epi16(SeqA, tmp);
-      Diag = _mm_andnot_si128(Result, MASK);
-
-      R0 = _mm_min_epi16(_mm_add_epi16(R1,Side2), _mm_add_epi16(R0,Diag));
-      R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_slli_si128(R1,2) ,Down2));
-
-      if (_mm_extract_epi16(R0, 0) > errThreshold
-	  && _mm_extract_epi16(R0, 1) > errThreshold
-	  && _mm_extract_epi16(R0, 2) > errThreshold
-	  && _mm_extract_epi16(R0, 3) > errThreshold
-	  && _mm_extract_epi16(R0, 4) > errThreshold
-	  && _mm_extract_epi16(R1, 0) > errThreshold
-	  && _mm_extract_epi16(R1, 1) > errThreshold
-	  && _mm_extract_epi16(R1, 2) > errThreshold
-	  && _mm_extract_epi16(R1, 3) > errThreshold)
+      tmp = vec_permutelower4sh(SeqB,27);
+      tmp = vec_shiftleftbytes1q(tmp, 2);
+      tmpValue = vec_extract8sh(tmp, 5);
+      tmp = vec_insert8sh(tmp, tmpValue, 0);
+
+      Result = vec_compareeq8sh(SeqA, tmp);
+      Diag = vec_bitandnotleft1q(Result, MASK);
+
+      R0 = vec_min8sh(vec_add8sh(R1,Side2), vec_add8sh(R0,Diag));
+      R0 = vec_min8sh(R0, vec_add8sh(vec_shiftleftbytes1q(R1,2) ,Down2));
+
+      if (vec_extract8sh(R0, 0) > errThreshold
+	  && vec_extract8sh(R0, 1) > errThreshold
+	  && vec_extract8sh(R0, 2) > errThreshold
+	  && vec_extract8sh(R0, 3) > errThreshold
+	  && vec_extract8sh(R0, 4) > errThreshold
+	  && vec_extract8sh(R1, 0) > errThreshold
+	  && vec_extract8sh(R1, 1) > errThreshold
+	  && vec_extract8sh(R1, 2) > errThreshold
+	  && vec_extract8sh(R1, 3) > errThreshold)
 	return -1;
 
       if (i == 2 * lenb - e) {
-	tmp = _mm_srli_si128(R0,2);
+	tmp = vec_shiftrightbytes1q(R0,2);
 	for (k = 0; k < e - 1; k++)
-	  tmp = _mm_srli_si128(tmp,2);
-	minError = _mm_extract_epi16(tmp,0);
+	  tmp = vec_shiftrightbytes1q(tmp,2);
+	minError = vec_extract8sh(tmp,0);
       }
 
     }
 
     else {
-      Result = _mm_cmpeq_epi16(SeqA, _mm_shufflelo_epi16(SeqB,27));
-      Diag = _mm_andnot_si128(Result, MASK);
+      Result = vec_compareeq8sh(SeqA, vec_permutelower4sh(SeqB,27));
+      Diag = vec_bitandnotleft1q(Result, MASK);
 
-      R1 = _mm_min_epi16(_mm_add_epi16(_mm_srli_si128(R0,2),Side1), _mm_add_epi16(R1,Diag));
-      R1 = _mm_min_epi16(R1, _mm_add_epi16(R0 ,Down1));
+      R1 = vec_min8sh(vec_add8sh(vec_shiftrightbytes1q(R0,2),Side1), vec_add8sh(R1,Diag));
+      R1 = vec_min8sh(R1, vec_add8sh(R0 ,Down1));
 
       if (i >= 2 * lenb - e) {
-	tmp = _mm_srli_si128(R1,2);
+	tmp = vec_shiftrightbytes1q(R1,2);
 	for (k = 0; k < e - 2; k++)
-	  tmp = _mm_srli_si128(tmp,2);
-	minError = min(minError, _mm_extract_epi16(tmp,0));
+	  tmp = vec_shiftrightbytes1q(tmp,2);
+	minError = min(minError, vec_extract8sh(tmp,0));
       }
     }
   }
@@ -649,80 +647,80 @@
   int tmpE = e;
   for (; j < 2 * (e - 2) + 1; j++) {
 
-    Diag = _mm_xor_si128(Diag, Diag);
+    Diag = vec_bitxor1q(Diag, Diag);
     //set the first element
     if (j == 0) {
       for (k = 0; k <= e - 1; k++) {
-	Diag = _mm_slli_si128(Diag, 2);
-	Diag = _mm_insert_epi16(Diag, b[lenb-1-k] != a[(i-lenb)-1+k],0);
+	Diag = vec_shiftleftbytes1q(Diag, 2);
+	Diag = vec_insert8sh(Diag, b[lenb-1-k] != a[(i-lenb)-1+k],0);
       }
 
-      R0 = _mm_min_epi16(_mm_add_epi16(R1,Side2), _mm_add_epi16(R0,Diag));
-      R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_slli_si128(R1,2) ,Down2));
+      R0 = vec_min8sh(vec_add8sh(R1,Side2), vec_add8sh(R0,Diag));
+      R0 = vec_min8sh(R0, vec_add8sh(vec_shiftleftbytes1q(R1,2) ,Down2));
 
       tmpE--;
 
-      tmp = _mm_srli_si128(R0,2);
+      tmp = vec_shiftrightbytes1q(R0,2);
       for (k = 0; k < e - 2; k++)
-	tmp = _mm_srli_si128(tmp,2);
-      minError = min(minError, _mm_extract_epi16(tmp,0));
+	tmp = vec_shiftrightbytes1q(tmp,2);
+      minError = min(minError, vec_extract8sh(tmp,0));
     } else if (j % 2 == 0) {
       for (k = 0; k < tmpE; k++) {
-	Diag = _mm_slli_si128(Diag, 2);
-	Diag = _mm_insert_epi16(Diag, b[lenb-1-k] != a[(i-lenb)-1+k],0);
+	Diag = vec_shiftleftbytes1q(Diag, 2);
+	Diag = vec_insert8sh(Diag, b[lenb-1-k] != a[(i-lenb)-1+k],0);
       }
 
-      R0 = _mm_min_epi16(_mm_add_epi16(R1,Side2), _mm_add_epi16(R0,Diag));
-      R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_slli_si128(R1,2) ,Down2));
+      R0 = vec_min8sh(vec_add8sh(R1,Side2), vec_add8sh(R0,Diag));
+      R0 = vec_min8sh(R0, vec_add8sh(vec_shiftleftbytes1q(R1,2) ,Down2));
 
       tmpE--;
 
-      tmp = _mm_srli_si128(R0,2);
+      tmp = vec_shiftrightbytes1q(R0,2);
       for (k = 0; k < tmpE - 1; k++)
-	tmp = _mm_srli_si128(tmp,2);
-      minError = min(minError, _mm_extract_epi16(tmp,0));
+	tmp = vec_shiftrightbytes1q(tmp,2);
+      minError = min(minError, vec_extract8sh(tmp,0));
     }
 
     else {
       for (k = 0; k < tmpE; k++) {
-	Diag = _mm_slli_si128(Diag, 2);
-	Diag = _mm_insert_epi16(Diag, b[lenb-1-k] != a[(i-lenb)-1+k],0);
+	Diag = vec_shiftleftbytes1q(Diag, 2);
+	Diag = vec_insert8sh(Diag, b[lenb-1-k] != a[(i-lenb)-1+k],0);
       }
 
-      R1 = _mm_min_epi16(_mm_add_epi16(_mm_srli_si128(R0,2),Side1), _mm_add_epi16(R1,Diag));
-      R1 = _mm_min_epi16(R1, _mm_add_epi16(R0 ,Down1));
+      R1 = vec_min8sh(vec_add8sh(vec_shiftrightbytes1q(R0,2),Side1), vec_add8sh(R1,Diag));
+      R1 = vec_min8sh(R1, vec_add8sh(R0 ,Down1));
 
-      tmp = _mm_srli_si128(R1,2);
+      tmp = vec_shiftrightbytes1q(R1,2);
       for (k = 0; k < tmpE - 2; k++)
-	tmp = _mm_srli_si128(tmp,2);
-      minError = min(minError, _mm_extract_epi16(tmp,0));
+	tmp = vec_shiftrightbytes1q(tmp,2);
+      minError = min(minError, vec_extract8sh(tmp,0));
     }
     i++;
   }
   //Diag
 
-  Diag = _mm_xor_si128(Diag, Diag);
-  Diag = _mm_insert_epi16(Diag, minError, 0);
-  Diag = _mm_insert_epi16(Diag, a[lenb+e-2] != b[lenb-1], 1);
+  Diag = vec_bitxor1q(Diag, Diag);
+  Diag = vec_insert8sh(Diag, minError, 0);
+  Diag = vec_insert8sh(Diag, a[lenb+e-2] != b[lenb-1], 1);
 
-  Side1 = _mm_insert_epi16(Side1,1,0);
-  Side1 = _mm_insert_epi16(Side1,1,1);
+  Side1 = vec_insert8sh(Side1,1,0);
+  Side1 = vec_insert8sh(Side1,1,1);
 
-  Down1 = _mm_insert_epi16(Down1, minError, 0);
-  Down1 = _mm_insert_epi16(Down1, 1, 1);
+  Down1 = vec_insert8sh(Down1, minError, 0);
+  Down1 = vec_insert8sh(Down1, 1, 1);
 
-  R1 = _mm_min_epi16(_mm_add_epi16(R0,Side1), _mm_add_epi16(_mm_slli_si128(R1,2),Diag));
-  R1 = _mm_min_epi16(R1, _mm_add_epi16(_mm_slli_si128(R0,2),Down1));
+  R1 = vec_min8sh(vec_add8sh(R0,Side1), vec_add8sh(vec_shiftleftbytes1q(R1,2),Diag));
+  R1 = vec_min8sh(R1, vec_add8sh(vec_shiftleftbytes1q(R0,2),Down1));
 
-  minError = min(minError, _mm_extract_epi16(R1,1));
+  minError = min(minError, vec_extract8sh(R1,1));
 
-  Diag = _mm_insert_epi16(Diag, a[lenb+e-1] != b[lenb-1], 0);
-  Down1 = _mm_insert_epi16(Down1, 1, 0);
+  Diag = vec_insert8sh(Diag, a[lenb+e-1] != b[lenb-1], 0);
+  Down1 = vec_insert8sh(Down1, 1, 0);
 
-  R0 = _mm_min_epi16(_mm_add_epi16(R1,Down1), _mm_add_epi16(R0,Diag));
-  R0 = _mm_min_epi16(R0, _mm_add_epi16(_mm_srli_si128(R1,2) ,Side1));
+  R0 = vec_min8sh(vec_add8sh(R1,Down1), vec_add8sh(R0,Diag));
+  R0 = vec_min8sh(R0, vec_add8sh(vec_shiftrightbytes1q(R1,2) ,Side1));
 
-  minError = min(minError, _mm_extract_epi16(R0,0));
+  minError = min(minError, vec_extract8sh(R0,0));
 
   if (minError > mismatch)
     return -1;
