--- ksw.c
+++ ksw.c
@@ -26,7 +26,11 @@
 #ifndef _NO_SSE2
 #include <stdlib.h>
 #include <stdint.h>
+#ifdef __PPC64__
+#include <vec128int.h>
+#else   /* assume x86_64 if not POWER8 LE */
 #include <emmintrin.h>
+#endif
 #include "ksw.h"
 
 #ifdef __GNUC__
@@ -97,6 +101,15 @@
 	uint64_t *b;
 	__m128i zero, gapoe, gape, shift, *H0, *H1, *E, *Hmax;
 
+#ifdef __PPC64__
+#define __max_16(ret, xx) do { \
+		(xx) = vec_max16ub((xx), vec_shiftrightbytes1q((xx), 8)); \
+		(xx) = vec_max16ub((xx), vec_shiftrightbytes1q((xx), 4)); \
+		(xx) = vec_max16ub((xx), vec_shiftrightbytes1q((xx), 2)); \
+		(xx) = vec_max16ub((xx), vec_shiftrightbytes1q((xx), 1)); \
+    	(ret) = vec_extract8sh((xx), 0) & 0x00ff; \
+	} while (0)
+#else
 #define __max_16(ret, xx) do { \
 		(xx) = _mm_max_epu8((xx), _mm_srli_si128((xx), 8)); \
 		(xx) = _mm_max_epu8((xx), _mm_srli_si128((xx), 4)); \
@@ -104,26 +117,45 @@
 		(xx) = _mm_max_epu8((xx), _mm_srli_si128((xx), 1)); \
     	(ret) = _mm_extract_epi16((xx), 0) & 0x00ff; \
 	} while (0)
+#endif
 
 	// initialization
 	m_b = n_b = 0; b = 0;
+#ifdef __PPC64__
+	zero = vec_splat4sw(0);
+	gapoe = vec_splat16sb(a->gapo + a->gape);
+	gape = vec_splat16sb(a->gape);
+	shift = vec_splat16sb(q->shift);
+#else
 	zero = _mm_set1_epi32(0);
 	gapoe = _mm_set1_epi8(a->gapo + a->gape);
 	gape = _mm_set1_epi8(a->gape);
 	shift = _mm_set1_epi8(q->shift);
+#endif
 	H0 = q->H0; H1 = q->H1; E = q->E; Hmax = q->Hmax;
 	slen = q->slen;
 	for (i = 0; i < slen; ++i) {
+#ifdef __PPC64__
+		vec_store1q(E + i, zero);
+		vec_store1q(H0 + i, zero);
+		vec_store1q(Hmax + i, zero);
+#else
 		_mm_store_si128(E + i, zero);
 		_mm_store_si128(H0 + i, zero);
 		_mm_store_si128(Hmax + i, zero);
+#endif
 	}
 	// the core loop
 	for (i = 0; i < tlen; ++i) {
 		int j, k, cmp, imax;
 		__m128i e, h, f = zero, max = zero, *S = q->qp + target[i] * slen; // s is the 1st score vector
+#ifdef __PPC64__
+		h = vec_load1q(H0 + slen - 1);  // h={2,5,8,11,14,17,-1,-1} in the above example
+		h = vec_shiftleftbytes1q(h, 1); // h=H(i-1,-1); << instead of >> assuming little-endian mode
+#else
 		h = _mm_load_si128(H0 + slen - 1); // h={2,5,8,11,14,17,-1,-1} in the above example
 		h = _mm_slli_si128(h, 1); // h=H(i-1,-1); << instead of >> because x64 is little-endian
+#endif
 		for (j = 0; LIKELY(j < slen); ++j) {
 			/* SW cells are computed in the following order:
 			 *   H(i,j)   = max{H(i-1,j-1)+S(i,j), E(i,j), F(i,j)}
@@ -131,6 +163,15 @@
 			 *   F(i,j+1) = max{H(i,j)-q, F(i,j)-r}
 			 */
 			// compute H'(i,j); note that at the beginning, h=H'(i-1,j-1)
+#ifdef __PPC64__
+			h = vec_addsaturating16ub(h, vec_load1q(S + j));
+			h = vec_subtractsaturating16ub(h, shift); // h=H'(i-1,j-1)+S(i,j)
+			e = vec_load1q(E + j); // e=E'(i,j)
+			h = vec_max16ub(h, e);
+			h = vec_max16ub(h, f); // h=H'(i,j)
+			max = vec_max16ub(max, h); // set max
+			vec_store1q(H1 + j, h); // save to H'(i,j)
+#else
 			h = _mm_adds_epu8(h, _mm_load_si128(S + j));
 			h = _mm_subs_epu8(h, shift); // h=H'(i-1,j-1)+S(i,j)
 			e = _mm_load_si128(E + j); // e=E'(i,j)
@@ -138,27 +179,61 @@
 			h = _mm_max_epu8(h, f); // h=H'(i,j)
 			max = _mm_max_epu8(max, h); // set max
 			_mm_store_si128(H1 + j, h); // save to H'(i,j)
+#endif
+
 			// now compute E'(i+1,j)
+#ifdef __PPC64__
+			h = vec_subtractsaturating16ub(h, gapoe); // h=H'(i,j)-gapo
+			e = vec_subtractsaturating16ub(e, gape); // e=E'(i,j)-gape
+			e = vec_max16ub(e, h); // e=E'(i+1,j)
+			vec_store1q(E + j, e); // save to E'(i+1,j)
+#else
 			h = _mm_subs_epu8(h, gapoe); // h=H'(i,j)-gapo
 			e = _mm_subs_epu8(e, gape); // e=E'(i,j)-gape
 			e = _mm_max_epu8(e, h); // e=E'(i+1,j)
 			_mm_store_si128(E + j, e); // save to E'(i+1,j)
+#endif
+
 			// now compute F'(i,j+1)
+#ifdef __PPC64__
+			f = vec_subtractsaturating16ub(f, gape);
+			f = vec_max16ub(f, h);
+#else
 			f = _mm_subs_epu8(f, gape);
 			f = _mm_max_epu8(f, h);
+#endif
+
 			// get H'(i-1,j) and prepare for the next j
+#ifdef __PPC64__
+			h = vec_load1q(H0 + j); // h=H'(i-1,j)
+#else
 			h = _mm_load_si128(H0 + j); // h=H'(i-1,j)
+#endif
+
 		}
 		// NB: we do not need to set E(i,j) as we disallow adjecent insertion and then deletion
 		for (k = 0; LIKELY(k < 16); ++k) { // this block mimics SWPS3; NB: H(i,j) updated in the lazy-F loop cannot exceed max
+#ifdef __PPC64__
+			f = vec_shiftleftbytes1q(f, 1);
+#else
 			f = _mm_slli_si128(f, 1);
+#endif
 			for (j = 0; LIKELY(j < slen); ++j) {
+#ifdef __PPC64__
+				h = vec_load1q(H1 + j);
+				h = vec_max16ub(h, f); // h=H'(i,j)
+				vec_store1q(H1 + j, h);
+				h = vec_subtractsaturating16ub(h, gapoe);
+				f = vec_subtractsaturating16ub(f, gape);
+				cmp = vec_extractupperbit16sb(vec_compareeq16sb(vec_subtractsaturating16ub(f, h), zero));
+#else
 				h = _mm_load_si128(H1 + j);
 				h = _mm_max_epu8(h, f); // h=H'(i,j)
 				_mm_store_si128(H1 + j, h);
 				h = _mm_subs_epu8(h, gapoe);
 				f = _mm_subs_epu8(f, gape);
 				cmp = _mm_movemask_epi8(_mm_cmpeq_epi8(_mm_subs_epu8(f, h), zero));
+#endif
 				if (UNLIKELY(cmp == 0xffff)) goto end_loop16;
 			}
 		}
@@ -177,7 +252,11 @@
 		if (imax > gmax) {
 			gmax = imax; te = i; // te is the end position on the target
 			for (j = 0; LIKELY(j < slen); ++j) // keep the H1 vector
+#ifdef __PPC64__
+				vec_store1q(Hmax + j, vec_load1q(H1 + j));
+#else
 				_mm_store_si128(Hmax + j, _mm_load_si128(H1 + j));
+#endif
 			if (gmax + q->shift >= 255) break;
 		}
 		S = H1; H1 = H0; H0 = S; // swap H0 and H1
@@ -207,32 +286,74 @@
 	uint64_t *b;
 	__m128i zero, gapoe, gape, *H0, *H1, *E, *Hmax;
 
+#ifdef __PPC64__
+#define __max_8(ret, xx) do { \
+		(xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 8)); \
+		(xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 4)); \
+		(xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 2)); \
+    	(ret) = vec_extract8sh((xx), 0); \
+	} while (0)
+#else
 #define __max_8(ret, xx) do { \
 		(xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 8)); \
 		(xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 4)); \
 		(xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 2)); \
     	(ret) = _mm_extract_epi16((xx), 0); \
 	} while (0)
+#endif
 
 	// initialization
 	m_b = n_b = 0; b = 0;
+#ifdef __PPC64__
+	zero = vec_splat4sw(0);
+	gapoe = vec_splat8sh(a->gapo + a->gape);
+	gape = vec_splat8sh(a->gape);
+#else
 	zero = _mm_set1_epi32(0);
 	gapoe = _mm_set1_epi16(a->gapo + a->gape);
 	gape = _mm_set1_epi16(a->gape);
+#endif
+
 	H0 = q->H0; H1 = q->H1; E = q->E; Hmax = q->Hmax;
 	slen = q->slen;
 	for (i = 0; i < slen; ++i) {
+#ifdef __PPC64__
+		vec_store1q(E + i, zero);
+		vec_store1q(H0 + i, zero);
+		vec_store1q(Hmax + i, zero);
+#else
 		_mm_store_si128(E + i, zero);
 		_mm_store_si128(H0 + i, zero);
 		_mm_store_si128(Hmax + i, zero);
+#endif
 	}
 	// the core loop
 	for (i = 0; i < tlen; ++i) {
 		int j, k, imax;
 		__m128i e, h, f = zero, max = zero, *S = q->qp + target[i] * slen; // s is the 1st score vector
+#ifdef __PPC64__
+		h = vec_load1q(H0 + slen - 1); // h={2,5,8,11,14,17,-1,-1} in the above example
+		h = vec_shiftleftbytes1q(h, 2);
+#else
 		h = _mm_load_si128(H0 + slen - 1); // h={2,5,8,11,14,17,-1,-1} in the above example
 		h = _mm_slli_si128(h, 2);
+#endif
 		for (j = 0; LIKELY(j < slen); ++j) {
+#ifdef __PPC64__
+			h = vec_addsaturating8sh(h, *S++);
+			e = vec_load1q(E + j);
+			h = vec_max8sh(h, e);
+			h = vec_max8sh(h, f);
+			max = vec_max8sh(max, h);
+			vec_store1q(H1 + j, h);
+			h = vec_subtractsaturating8uh(h, gapoe);
+			e = vec_subtractsaturating8uh(e, gape);
+			e = vec_max8sh(e, h);
+			vec_store1q(E + j, e);
+			f = vec_subtractsaturating8uh(f, gape);
+			f = vec_max8sh(f, h);
+			h = vec_load1q(H0 + j);
+#else
 			h = _mm_adds_epi16(h, *S++);
 			e = _mm_load_si128(E + j);
 			h = _mm_max_epi16(h, e);
@@ -246,16 +367,31 @@
 			f = _mm_subs_epu16(f, gape);
 			f = _mm_max_epi16(f, h);
 			h = _mm_load_si128(H0 + j);
+#endif
+
 		}
 		for (k = 0; LIKELY(k < 16); ++k) {
+#ifdef __PPC64__
+			f = vec_shiftleftbytes1q(f, 2);
+#else
 			f = _mm_slli_si128(f, 2);
+#endif
 			for (j = 0; LIKELY(j < slen); ++j) {
+#ifdef __PPC64__
+				h = vec_load1q(H1 + j);
+				h = vec_max8sh(h, f);
+				vec_store1q(H1 + j, h);
+				h = vec_subtractsaturating8uh(h, gapoe);
+				f = vec_subtractsaturating8uh(f, gape);
+				if(UNLIKELY(!vec_extractupperbit16sb(vec_comparegt8sh(f, h)))) goto end_loop8;
+#else
 				h = _mm_load_si128(H1 + j);
 				h = _mm_max_epi16(h, f);
 				_mm_store_si128(H1 + j, h);
 				h = _mm_subs_epu16(h, gapoe);
 				f = _mm_subs_epu16(f, gape);
 				if(UNLIKELY(!_mm_movemask_epi8(_mm_cmpgt_epi16(f, h)))) goto end_loop8;
+#endif
 			}
 		}
 end_loop8:
@@ -272,7 +408,11 @@
 		if (imax > gmax) {
 			gmax = imax; te = i;
 			for (j = 0; LIKELY(j < slen); ++j)
+#ifdef __PPC64__
+				vec_store1q(Hmax + j, vec_load1q(H1 + j));
+#else
 				_mm_store_si128(Hmax + j, _mm_load_si128(H1 + j));
+#endif
 		}
 		S = H1; H1 = H0; H0 = S;
 	}
