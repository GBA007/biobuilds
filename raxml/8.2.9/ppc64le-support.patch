--- axml.c
+++ axml.c
@@ -78,6 +78,11 @@
 */
 #endif
 
+#if defined(__SIM_VECLIB)
+#include <vec128int.h>
+#include <vecmisc.h>
+#endif
+
 #include "axml.h"
 #include "globalVariables.h"
 
@@ -4996,7 +5001,7 @@
   printf("Please send us all input files, the exact invocation, details of the HW and operating system,\n");
   printf("as well as all error messages printed to screen.\n\n\n");
 
-  printf("raxmlHPC[-SSE3|-AVX|-PTHREADS|-PTHREADS-SSE3|-PTHREADS-AVX|-HYBRID|-HYBRID-SSE3|HYBRID-AVX]\n");
+  printf("raxmlHPC[-SSE3|-AVX|-PTHREADS|-PTHREADS-SSE3|-PTHREADS-AVX|-HYBRID|-HYBRID-SSE3|HYBRID-AVX|-VECLIB]\n");
   printf("      -s sequenceFileName -n outputFileName -m substitutionModel\n");
   printf("      [-a weightFileName] [-A secondaryStructureSubstModel]\n");
   printf("      [-b bootstrapRandomNumberSeed] [-B wcCriterionThreshold]\n");
@@ -5931,7 +5936,7 @@
 	    break;
 	  case 'U':
 	    tr->saveMemory = TRUE;
-#if (!defined(__SIM_SSE3) && !defined(__AVX))	
+#if (!defined(__SIM_SSE3) && !defined(__SIM_VECLIB) && !defined(__AVX))	
 	    printf("\nmemory saving option -U does only work with the AVX and SSE3 vectorized versions of the code\n");
 	    printf("please remove this option and execute the program again\n");
 	    printf("exiting ....\n\n");
@@ -13710,6 +13715,8 @@
     
     _mm_setcsr( _mm_getcsr() | _MM_FLUSH_ZERO_ON);
     
+#elif defined(__SIM_VECLIB)
+    /* Above command sets flush to zero for SSE, but no POWER equivalent? */
 #endif 
     
     adef = (analdef *)rax_malloc(sizeof(analdef));
--- bipartitionList.c
+++ bipartitionList.c
@@ -45,10 +45,10 @@
 #include "axml.h"
 #include "rmq.h" //include range minimum queries for fast plausibility checker
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
-#include <xmmintrin.h>
-#include <pmmintrin.h>
+#include <vec128int.h>
+#include <vec128dp.h>
 
 #endif
 
--- evaluateGenericSpecial.c
+++ evaluateGenericSpecial.c
@@ -40,10 +40,9 @@
 #include "axml.h"
 
 
-#ifdef __SIM_SSE3
-#include <xmmintrin.h>
-#include <pmmintrin.h>
-/*#include <tmmintrin.h>*/
+#ifdef __SIM_VECLIB
+#include <vec128int.h>
+#include <vec128dp.h>
 #endif
 
 #ifdef _USE_PTHREADS
@@ -989,7 +988,7 @@
 }
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 
 
@@ -1028,20 +1027,20 @@
 
       diagptable = &diagptable_start[20 * cptr[i]];	           	 
 
-      __m128d tv = _mm_setzero_pd();	    
+      __m128d tv = vec_zero2dp();	    
 
       for(l = 0; l < 20; l+=2)
       {
-        __m128d lv = _mm_load_pd(&left[l]);
-        __m128d rv = _mm_load_pd(&right[l]);
-        __m128d mul = _mm_mul_pd(lv, rv);
-        __m128d dv = _mm_load_pd(&diagptable[l]);
+        __m128d lv = vec_load2dpaligned(&left[l]);
+        __m128d rv = vec_load2dpaligned(&right[l]);
+        __m128d mul = vec_multiply2dp(lv, rv);
+        __m128d dv = vec_load2dpaligned(&diagptable[l]);
 
-        tv = _mm_add_pd(tv, _mm_mul_pd(mul, dv));		   
+        tv = vec_add2dp(tv, vec_multiply2dp(mul, dv));		   
       }		 		
 
-      tv = _mm_hadd_pd(tv, tv);
-      _mm_storel_pd(&term, tv);
+      tv = vec_horizontaladd2dp(tv, tv);
+      vec_storelower1dpof2dp(&term, tv);
 
       if(fastScaling)
 	term = LOG(term);
@@ -1074,20 +1073,20 @@
 
       diagptable = &diagptable_start[20 * cptr[i]];	  	
 
-      __m128d tv = _mm_setzero_pd();	    
+      __m128d tv = vec_zero2dp();	    
 
       for(l = 0; l < 20; l+=2)
       {
-        __m128d lv = _mm_load_pd(&left[l]);
-        __m128d rv = _mm_load_pd(&right[l]);
-        __m128d mul = _mm_mul_pd(lv, rv);
-        __m128d dv = _mm_load_pd(&diagptable[l]);
+        __m128d lv = vec_load2dpaligned(&left[l]);
+        __m128d rv = vec_load2dpaligned(&right[l]);
+        __m128d mul = vec_multiply2dp(lv, rv);
+        __m128d dv = vec_load2dpaligned(&diagptable[l]);
 
-        tv = _mm_add_pd(tv, _mm_mul_pd(mul, dv));		   
+        tv = vec_add2dp(tv, vec_multiply2dp(mul, dv));		   
       }		 		
 
-      tv = _mm_hadd_pd(tv, tv);
-      _mm_storel_pd(&term, tv);
+      tv = vec_horizontaladd2dp(tv, tv);
+      vec_storelower1dpof2dp(&term, tv);
       
       if(fastScaling)
 	term = LOG(term);	 
@@ -1135,22 +1134,22 @@
 
       diagptable = &diagptable_start[4 * cptr[i]];
 
-      x1v1 =  _mm_load_pd(&x1[0]);
-      x1v2 =  _mm_load_pd(&x1[2]);
-      x2v1 =  _mm_load_pd(&x2[0]);
-      x2v2 =  _mm_load_pd(&x2[2]);
-      dv1  =  _mm_load_pd(&diagptable[0]);
-      dv2  =  _mm_load_pd(&diagptable[2]);
+      x1v1 =  vec_load2dpaligned(&x1[0]);
+      x1v2 =  vec_load2dpaligned(&x1[2]);
+      x2v1 =  vec_load2dpaligned(&x2[0]);
+      x2v2 =  vec_load2dpaligned(&x2[2]);
+      dv1  =  vec_load2dpaligned(&diagptable[0]);
+      dv2  =  vec_load2dpaligned(&diagptable[2]);
 
-      x1v1 = _mm_mul_pd(x1v1, x2v1);
-      x1v1 = _mm_mul_pd(x1v1, dv1);
+      x1v1 = vec_multiply2dp(x1v1, x2v1);
+      x1v1 = vec_multiply2dp(x1v1, dv1);
 
-      x1v2 = _mm_mul_pd(x1v2, x2v2);
-      x1v2 = _mm_mul_pd(x1v2, dv2);
+      x1v2 = vec_multiply2dp(x1v2, x2v2);
+      x1v2 = vec_multiply2dp(x1v2, dv2);
 
-      x1v1 = _mm_add_pd(x1v1, x1v2);
+      x1v1 = vec_add2dp(x1v1, x1v2);
 
-      _mm_store_pd(t, x1v1);
+      vec_store2dpto2dp(t, x1v1);
 
       if(fastScaling)
 	term = LOG(t[0] + t[1]);      
@@ -1185,22 +1184,22 @@
 
       diagptable = &diagptable_start[4 * cptr[i]];	
 
-      x1v1 =  _mm_load_pd(&x1[0]);
-      x1v2 =  _mm_load_pd(&x1[2]);
-      x2v1 =  _mm_load_pd(&x2[0]);
-      x2v2 =  _mm_load_pd(&x2[2]);
-      dv1  =  _mm_load_pd(&diagptable[0]);
-      dv2  =  _mm_load_pd(&diagptable[2]);
+      x1v1 =  vec_load2dpaligned(&x1[0]);
+      x1v2 =  vec_load2dpaligned(&x1[2]);
+      x2v1 =  vec_load2dpaligned(&x2[0]);
+      x2v2 =  vec_load2dpaligned(&x2[2]);
+      dv1  =  vec_load2dpaligned(&diagptable[0]);
+      dv2  =  vec_load2dpaligned(&diagptable[2]);
 
-      x1v1 = _mm_mul_pd(x1v1, x2v1);
-      x1v1 = _mm_mul_pd(x1v1, dv1);
+      x1v1 = vec_multiply2dp(x1v1, x2v1);
+      x1v1 = vec_multiply2dp(x1v1, dv1);
 
-      x1v2 = _mm_mul_pd(x1v2, x2v2);
-      x1v2 = _mm_mul_pd(x1v2, dv2);
+      x1v2 = vec_multiply2dp(x1v2, x2v2);
+      x1v2 = vec_multiply2dp(x1v2, dv2);
 
-      x1v1 = _mm_add_pd(x1v1, x1v2);
+      x1v1 = vec_add2dp(x1v1, x1v2);
 
-      _mm_store_pd(t, x1v1);
+      vec_store2dpto2dp(t, x1v1);
 
       if(fastScaling)
 	term = LOG(t[0] + t[1]);
@@ -1235,21 +1234,21 @@
 	  right = &(x2[20 * i]);
 	  
 	  diagptable = &diagptable_start[20 * cptr[i]];	           	 
-#ifdef __SIM_SSE3
-	  __m128d tv = _mm_setzero_pd();	    
+#ifdef __SIM_VECLIB
+	  __m128d tv = vec_zero2dp();	    
 	  
 	  for(l = 0; l < 20; l+=2)
 	    {
-	      __m128d lv = _mm_load_pd(&left[l]);
-	      __m128d rv = _mm_load_pd(&right[l]);
-	      __m128d mul = _mm_mul_pd(lv, rv);
-	      __m128d dv = _mm_load_pd(&diagptable[l]);
+	      __m128d lv = vec_load2dpaligned(&left[l]);
+	      __m128d rv = vec_load2dpaligned(&right[l]);
+	      __m128d mul = vec_multiply2dp(lv, rv);
+	      __m128d dv = vec_load2dpaligned(&diagptable[l]);
 	      
-	      tv = _mm_add_pd(tv, _mm_mul_pd(mul, dv));		   
+	      tv = vec_add2dp(tv, vec_multiply2dp(mul, dv));		   
 	    }		 		
 	  
-	  tv = _mm_hadd_pd(tv, tv);
-	  _mm_storel_pd(&term, tv);
+	  tv = vec_horizontaladd2dp(tv, tv);
+	  vec_storelower1dpof2dp(&term, tv);
 #else  
 	  for(l = 0, term = 0.0; l < 20; l++)
 	    term += left[l] * right[l] * diagptable[l];	 	  	  
@@ -1271,21 +1270,21 @@
 	  right = &x2[20 * i];
 	  
 	  diagptable = &diagptable_start[20 * cptr[i]];	  	
-#ifdef __SIM_SSE3
-	    __m128d tv = _mm_setzero_pd();	    
+#ifdef __SIM_VECLIB
+	    __m128d tv = vec_zero2dp();	    
 	      	    
 	    for(l = 0; l < 20; l+=2)
 	      {
-		__m128d lv = _mm_load_pd(&left[l]);
-		__m128d rv = _mm_load_pd(&right[l]);
-		__m128d mul = _mm_mul_pd(lv, rv);
-		__m128d dv = _mm_load_pd(&diagptable[l]);
+		__m128d lv = vec_load2dpaligned(&left[l]);
+		__m128d rv = vec_load2dpaligned(&right[l]);
+		__m128d mul = vec_multiply2dp(lv, rv);
+		__m128d dv = vec_load2dpaligned(&diagptable[l]);
 		
-		tv = _mm_add_pd(tv, _mm_mul_pd(mul, dv));		   
+		tv = vec_add2dp(tv, vec_multiply2dp(mul, dv));		   
 	      }		 		
 	      
-	      tv = _mm_hadd_pd(tv, tv);
-	      _mm_storel_pd(&term, tv);
+	      tv = vec_horizontaladd2dp(tv, tv);
+	      vec_storelower1dpof2dp(&term, tv);
 #else  
 	  for(l = 0, term = 0.0; l < 20; l++)
 	    term += left[l] * right[l] * diagptable[l];	
@@ -1469,7 +1468,7 @@
 {
   double  sum = 0.0, term;       
   int     i;
-#ifndef  __SIM_SSE3
+#ifndef  __SIM_VECLIB
   int j;  
 #endif
   double  *diagptable, *x1, *x2;                      	    
@@ -1478,7 +1477,7 @@
     {          
       for (i = 0; i < n; i++) 
 	{
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));	    		   	  
 #endif
 	  x1 = &(tipVector[2 * tipX1[i]]);
@@ -1486,8 +1485,8 @@
 	  
 	  diagptable = &(diagptable_start[2 * cptr[i]]);	    	    	  
 	
-#ifdef __SIM_SSE3	  
-	  _mm_store_pd(t, _mm_mul_pd(_mm_load_pd(x1), _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(diagptable))));
+#ifdef __SIM_VECLIB	  
+	  vec_store2dpto2dp(t, vec_multiply2dp(vec_load2dpaligned(x1), vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(diagptable))));
 	  
 	  if(fastScaling)
 	    term = LOG(FABS(t[0] + t[1]));
@@ -1510,15 +1509,15 @@
     {
       for (i = 0; i < n; i++) 
 	{	
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));	    		   	  
 #endif	          	
 	  x1 = &x1_start[2 * i];
 	  x2 = &x2_start[2 * i];
 	  
 	  diagptable = &diagptable_start[2 * cptr[i]];		  
-#ifdef __SIM_SSE3	  
-	  _mm_store_pd(t, _mm_mul_pd(_mm_load_pd(x1), _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(diagptable))));
+#ifdef __SIM_VECLIB	  
+	  vec_store2dpto2dp(t, vec_multiply2dp(vec_load2dpaligned(x1), vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(diagptable))));
 	  
 	  if(fastScaling)
 	    term = LOG(FABS(t[0] + t[1]));
@@ -1549,7 +1548,7 @@
 {
   double   sum = 0.0, term;    
   int     i, j;
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
   int k;
 #endif 
   double  *x1, *x2;             
@@ -1558,28 +1557,28 @@
     {          
       for (i = 0; i < n; i++)
 	{
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));
 	  __m128d termv, x1v, x2v, dv;
 #endif
 	  x1 = &(tipVector[2 * tipX1[i]]);	 
 	  x2 = &x2_start[8 * i];	          	  	
-#ifdef __SIM_SSE3	
-	  termv = _mm_set1_pd(0.0);	    	   
+#ifdef __SIM_VECLIB	
+	  termv = vec_splat2dp(0.0);	    	   
 	  
 	  for(j = 0; j < 4; j++)
 	    {
-	      x1v = _mm_load_pd(&x1[0]);
-	      x2v = _mm_load_pd(&x2[j * 2]);
-	      dv   = _mm_load_pd(&diagptable[j * 2]);
+	      x1v = vec_load2dpaligned(&x1[0]);
+	      x2v = vec_load2dpaligned(&x2[j * 2]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 2]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);	      	      
+	      termv = vec_add2dp(termv, x1v);	      	      
 	    }
 	  
-	  _mm_store_pd(t, termv);	        
+	  vec_store2dpto2dp(t, termv);	        
 	  
 	  if(fastScaling)
 	    term = LOG(0.25 * (FABS(t[0] + t[1])));
@@ -1603,29 +1602,29 @@
     {         
       for (i = 0; i < n; i++) 
 	{
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));
 	  __m128d termv, x1v, x2v, dv;
 #endif	  	 	  	  
 	  x1 = &x1_start[8 * i];
 	  x2 = &x2_start[8 * i];
 	  	  
-#ifdef __SIM_SSE3	
-	  termv = _mm_set1_pd(0.0);	    	   
+#ifdef __SIM_VECLIB	
+	  termv = vec_splat2dp(0.0);	    	   
 	  
 	  for(j = 0; j < 4; j++)
 	    {
-	      x1v = _mm_load_pd(&x1[j * 2]);
-	      x2v = _mm_load_pd(&x2[j * 2]);
-	      dv   = _mm_load_pd(&diagptable[j * 2]);
+	      x1v = vec_load2dpaligned(&x1[j * 2]);
+	      x2v = vec_load2dpaligned(&x2[j * 2]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 2]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);	      	      
+	      termv = vec_add2dp(termv, x1v);	      	      
 	    }
 	  
-	  _mm_store_pd(t, termv);
+	  vec_store2dpto2dp(t, termv);
 	  
 	  
 	  if(fastScaling)
@@ -1728,7 +1727,7 @@
 {
   double  sum = 0.0, term;       
   int     i;
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
   int j;  
 #endif
   double  *diagptable, *x1, *x2;                      	    
@@ -1737,7 +1736,7 @@
     {           
       for (i = 0; i < n; i++) 
 	{	
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));
 	  __m128d x1v1, x1v2, x2v1, x2v2, dv1, dv2;
 #endif
@@ -1746,23 +1745,23 @@
 	  
 	  diagptable = &diagptable_start[4 * cptr[i]];
 	  
-#ifdef __SIM_SSE3	    	  
-	  x1v1 =  _mm_load_pd(&x1[0]);
-	  x1v2 =  _mm_load_pd(&x1[2]);
-	  x2v1 =  _mm_load_pd(&x2[0]);
-	  x2v2 =  _mm_load_pd(&x2[2]);
-	  dv1  =  _mm_load_pd(&diagptable[0]);
-	  dv2  =  _mm_load_pd(&diagptable[2]);
+#ifdef __SIM_VECLIB	    	  
+	  x1v1 =  vec_load2dpaligned(&x1[0]);
+	  x1v2 =  vec_load2dpaligned(&x1[2]);
+	  x2v1 =  vec_load2dpaligned(&x2[0]);
+	  x2v2 =  vec_load2dpaligned(&x2[2]);
+	  dv1  =  vec_load2dpaligned(&diagptable[0]);
+	  dv2  =  vec_load2dpaligned(&diagptable[2]);
 	  
-	  x1v1 = _mm_mul_pd(x1v1, x2v1);
-	  x1v1 = _mm_mul_pd(x1v1, dv1);
+	  x1v1 = vec_multiply2dp(x1v1, x2v1);
+	  x1v1 = vec_multiply2dp(x1v1, dv1);
 	  
-	  x1v2 = _mm_mul_pd(x1v2, x2v2);
-	  x1v2 = _mm_mul_pd(x1v2, dv2);
+	  x1v2 = vec_multiply2dp(x1v2, x2v2);
+	  x1v2 = vec_multiply2dp(x1v2, dv2);
 	  
-	  x1v1 = _mm_add_pd(x1v1, x1v2);
+	  x1v1 = vec_add2dp(x1v1, x1v2);
 	  
-	  _mm_store_pd(t, x1v1);
+	  vec_store2dpto2dp(t, x1v1);
 	  
 	  if(fastScaling)
 	    term = LOG(FABS(t[0] + t[1]));
@@ -1798,7 +1797,7 @@
     {
       for (i = 0; i < n; i++) 
 	{ 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));
 	   __m128d x1v1, x1v2, x2v1, x2v2, dv1, dv2;
 #endif
@@ -1807,23 +1806,23 @@
 	  
 	  diagptable = &diagptable_start[4 * cptr[i]];	
 	  
-#ifdef __SIM_SSE3	  
-	  x1v1 =  _mm_load_pd(&x1[0]);
-	  x1v2 =  _mm_load_pd(&x1[2]);
-	  x2v1 =  _mm_load_pd(&x2[0]);
-	  x2v2 =  _mm_load_pd(&x2[2]);
-	  dv1  =  _mm_load_pd(&diagptable[0]);
-	  dv2  =  _mm_load_pd(&diagptable[2]);
+#ifdef __SIM_VECLIB	  
+	  x1v1 =  vec_load2dpaligned(&x1[0]);
+	  x1v2 =  vec_load2dpaligned(&x1[2]);
+	  x2v1 =  vec_load2dpaligned(&x2[0]);
+	  x2v2 =  vec_load2dpaligned(&x2[2]);
+	  dv1  =  vec_load2dpaligned(&diagptable[0]);
+	  dv2  =  vec_load2dpaligned(&diagptable[2]);
 	  
-	  x1v1 = _mm_mul_pd(x1v1, x2v1);
-	  x1v1 = _mm_mul_pd(x1v1, dv1);
+	  x1v1 = vec_multiply2dp(x1v1, x2v1);
+	  x1v1 = vec_multiply2dp(x1v1, dv1);
 	  
-	  x1v2 = _mm_mul_pd(x1v2, x2v2);
-	  x1v2 = _mm_mul_pd(x1v2, dv2);
+	  x1v2 = vec_multiply2dp(x1v2, x2v2);
+	  x1v2 = vec_multiply2dp(x1v2, dv2);
 	  
-	  x1v1 = _mm_add_pd(x1v1, x1v2);
+	  x1v1 = vec_add2dp(x1v1, x1v2);
 	  
-	  _mm_store_pd(t, x1v1);
+	  vec_store2dpto2dp(t, x1v1);
 	  
 	  if(fastScaling)
 	    term = LOG(FABS(t[0] + t[1]));
@@ -1847,7 +1846,7 @@
 } 
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 
 
@@ -1886,30 +1885,30 @@
 	    }
 	  
 	
-	  termv = _mm_set1_pd(0.0);	    	   
+	  termv = vec_splat2dp(0.0);	    	   
 	  
 	  for(j = 0; j < 4; j++)
 	    {
-	      x1v = _mm_load_pd(&x1[0]);
-	      x2v = _mm_load_pd(&x2[j * 4]);
-	      dv   = _mm_load_pd(&diagptable[j * 4]);
+	      x1v = vec_load2dpaligned(&x1[0]);
+	      x2v = vec_load2dpaligned(&x2[j * 4]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	      
-	      x1v = _mm_load_pd(&x1[2]);
-	      x2v = _mm_load_pd(&x2[j * 4 + 2]);
-	      dv   = _mm_load_pd(&diagptable[j * 4 + 2]);
+	      x1v = vec_load2dpaligned(&x1[2]);
+	      x2v = vec_load2dpaligned(&x2[j * 4 + 2]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4 + 2]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	    }
 	  
-	  _mm_store_pd(t, termv);	  	 
+	  vec_store2dpto2dp(t, termv);	  	 
 
 	  if(fastScaling)
 	    term = LOG(0.25 * FABS(t[0] + t[1]));
@@ -1944,30 +1943,30 @@
 	      x2_ptr += 16;
 	    }
 	
-	  termv = _mm_set1_pd(0.0);	  	 
+	  termv = vec_splat2dp(0.0);	  	 
 	  
 	  for(j = 0; j < 4; j++)
 	    {
-	      x1v = _mm_load_pd(&x1[j * 4]);
-	      x2v = _mm_load_pd(&x2[j * 4]);
-	      dv   = _mm_load_pd(&diagptable[j * 4]);
+	      x1v = vec_load2dpaligned(&x1[j * 4]);
+	      x2v = vec_load2dpaligned(&x2[j * 4]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	      
-	      x1v = _mm_load_pd(&x1[j * 4 + 2]);
-	      x2v = _mm_load_pd(&x2[j * 4 + 2]);
-	      dv   = _mm_load_pd(&diagptable[j * 4 + 2]);
+	      x1v = vec_load2dpaligned(&x1[j * 4 + 2]);
+	      x2v = vec_load2dpaligned(&x2[j * 4 + 2]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4 + 2]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	    }
 	  
-	  _mm_store_pd(t, termv);
+	  vec_store2dpto2dp(t, termv);
 
 	  if(fastScaling)
 	    term = LOG(0.25 * FABS(t[0] + t[1]));
@@ -1994,7 +1993,7 @@
 {
   double   sum = 0.0, term;    
   int     i, j;
-#ifndef __SIM_SSE3  
+#ifndef __SIM_VECLIB  
   int k;
 #endif
   double  *x1, *x2;             
@@ -2005,38 +2004,38 @@
     {          	
       for (i = 0; i < n; i++)
 	{
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));
 	  __m128d termv, x1v, x2v, dv;
 #endif
 	  x1 = &(tipVector[4 * tipX1[i]]);	 
 	  x2 = &x2_start[16 * i];	 
 	  
-#ifdef __SIM_SSE3	
-	  termv = _mm_set1_pd(0.0);	    	   
+#ifdef __SIM_VECLIB	
+	  termv = vec_splat2dp(0.0);	    	   
 	  
 	  for(j = 0; j < 4; j++)
 	    {
-	      x1v = _mm_load_pd(&x1[0]);
-	      x2v = _mm_load_pd(&x2[j * 4]);
-	      dv   = _mm_load_pd(&diagptable[j * 4]);
+	      x1v = vec_load2dpaligned(&x1[0]);
+	      x2v = vec_load2dpaligned(&x2[j * 4]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	      
-	      x1v = _mm_load_pd(&x1[2]);
-	      x2v = _mm_load_pd(&x2[j * 4 + 2]);
-	      dv   = _mm_load_pd(&diagptable[j * 4 + 2]);
+	      x1v = vec_load2dpaligned(&x1[2]);
+	      x2v = vec_load2dpaligned(&x2[j * 4 + 2]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4 + 2]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	    }
 	  
-	  _mm_store_pd(t, termv);
+	  vec_store2dpto2dp(t, termv);
 	  
 	  
 	  if(fastScaling)
@@ -2061,7 +2060,7 @@
     {        
       for (i = 0; i < n; i++) 
 	{
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  double t[2] __attribute__ ((aligned (BYTE_ALIGNMENT)));
 	  __m128d termv, x1v, x2v, dv;
 #endif
@@ -2069,31 +2068,31 @@
 	  x1 = &x1_start[16 * i];
 	  x2 = &x2_start[16 * i];	  	  
 	
-#ifdef __SIM_SSE3	
-	  termv = _mm_set1_pd(0.0);	  	 
+#ifdef __SIM_VECLIB	
+	  termv = vec_splat2dp(0.0);	  	 
 	  
 	  for(j = 0; j < 4; j++)
 	    {
-	      x1v = _mm_load_pd(&x1[j * 4]);
-	      x2v = _mm_load_pd(&x2[j * 4]);
-	      dv   = _mm_load_pd(&diagptable[j * 4]);
+	      x1v = vec_load2dpaligned(&x1[j * 4]);
+	      x2v = vec_load2dpaligned(&x2[j * 4]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	      
-	      x1v = _mm_load_pd(&x1[j * 4 + 2]);
-	      x2v = _mm_load_pd(&x2[j * 4 + 2]);
-	      dv   = _mm_load_pd(&diagptable[j * 4 + 2]);
+	      x1v = vec_load2dpaligned(&x1[j * 4 + 2]);
+	      x2v = vec_load2dpaligned(&x2[j * 4 + 2]);
+	      dv   = vec_load2dpaligned(&diagptable[j * 4 + 2]);
 	      
-	      x1v = _mm_mul_pd(x1v, x2v);
-	      x1v = _mm_mul_pd(x1v, dv);
+	      x1v = vec_multiply2dp(x1v, x2v);
+	      x1v = vec_multiply2dp(x1v, dv);
 	      
-	      termv = _mm_add_pd(termv, x1v);
+	      termv = vec_add2dp(termv, x1v);
 	    }
 	  
-	  _mm_store_pd(t, termv);
+	  vec_store2dpto2dp(t, termv);
 
 	  if(fastScaling)
 	    term = LOG(0.25 * FABS(t[0] + t[1]));
@@ -2214,8 +2213,8 @@
     {               
       for (i = 0; i < n; i++) 
 	{
-#ifdef __SIM_SSE3
-	  __m128d tv = _mm_setzero_pd();
+#ifdef __SIM_VECLIB
+	  __m128d tv = vec_zero2dp();
 	  left = &(tipVector[20 * tipX1[i]]);	  	  
 	  
 	  for(j = 0, term = 0.0; j < 4; j++)
@@ -2224,12 +2223,12 @@
 	      right = &(x2[80 * i + 20 * j]);
 	      for(l = 0; l < 20; l+=2)
 		{
-		  __m128d mul = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
-		  tv = _mm_add_pd(tv, _mm_mul_pd(mul, _mm_load_pd(&d[l])));		   
+		  __m128d mul = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
+		  tv = vec_add2dp(tv, vec_multiply2dp(mul, vec_load2dpaligned(&d[l])));		   
 		}		 		
 	    }
-	  tv = _mm_hadd_pd(tv, tv);
-	  _mm_storel_pd(&term, tv);
+	  tv = vec_horizontaladd2dp(tv, tv);
+	  vec_storelower1dpof2dp(&term, tv);
 	  
 #else
 	  left = &(tipVector[20 * tipX1[i]]);	  	  
@@ -2254,8 +2253,8 @@
     {
       for (i = 0; i < n; i++) 
 	{	  	 	             
-#ifdef __SIM_SSE3
-	  __m128d tv = _mm_setzero_pd();	 	  	  
+#ifdef __SIM_VECLIB
+	  __m128d tv = vec_zero2dp();	 	  	  
 	      
 	  for(j = 0, term = 0.0; j < 4; j++)
 	    {
@@ -2265,12 +2264,12 @@
 	      
 	      for(l = 0; l < 20; l+=2)
 		{
-		  __m128d mul = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
-		  tv = _mm_add_pd(tv, _mm_mul_pd(mul, _mm_load_pd(&d[l])));		   
+		  __m128d mul = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
+		  tv = vec_add2dp(tv, vec_multiply2dp(mul, vec_load2dpaligned(&d[l])));		   
 		}		 		
 	    }
-	  tv = _mm_hadd_pd(tv, tv);
-	  _mm_storel_pd(&term, tv);	  
+	  tv = vec_horizontaladd2dp(tv, tv);
+	  vec_storelower1dpof2dp(&term, tv);	  
 #else
 	  for(j = 0, term = 0.0; j < 4; j++)
 	    {
@@ -2363,7 +2362,7 @@
 
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 static double evaluateGTRGAMMAPROT_GAPPED_SAVE (int *ex1, int *ex2, int *wptr,
     double *x1, double *x2,  
@@ -2393,7 +2392,7 @@
         x2_ptr += 80;
       }
 
-      __m128d tv = _mm_setzero_pd();
+      __m128d tv = vec_zero2dp();
       left = &(tipVector[20 * tipX1[i]]);	  	  
 
       for(j = 0, term = 0.0; j < 4; j++)
@@ -2402,13 +2401,13 @@
         right = &(x2v[20 * j]);
         for(l = 0; l < 20; l+=2)
         {
-          __m128d mul = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
-          tv = _mm_add_pd(tv, _mm_mul_pd(mul, _mm_load_pd(&d[l])));		   
+          __m128d mul = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
+          tv = vec_add2dp(tv, vec_multiply2dp(mul, vec_load2dpaligned(&d[l])));		   
         }		 		
       }
 
-      tv = _mm_hadd_pd(tv, tv);
-      _mm_storel_pd(&term, tv);
+      tv = vec_horizontaladd2dp(tv, tv);
+      vec_storelower1dpof2dp(&term, tv);
 
 
       if(fastScaling)
@@ -2439,7 +2438,7 @@
         x2_ptr += 80;
       }
 
-      __m128d tv = _mm_setzero_pd();	 	  	  
+      __m128d tv = vec_zero2dp();	 	  	  
 
       for(j = 0, term = 0.0; j < 4; j++)
       {
@@ -2449,12 +2448,12 @@
 
         for(l = 0; l < 20; l+=2)
         {
-          __m128d mul = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
-          tv = _mm_add_pd(tv, _mm_mul_pd(mul, _mm_load_pd(&d[l])));		   
+          __m128d mul = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
+          tv = vec_add2dp(tv, vec_multiply2dp(mul, vec_load2dpaligned(&d[l])));		   
         }		 		
       }
-      tv = _mm_hadd_pd(tv, tv);
-      _mm_storel_pd(&term, tv);	  
+      tv = vec_horizontaladd2dp(tv, tv);
+      vec_storelower1dpof2dp(&term, tv);	  
 
       if(fastScaling)
         term = LOG(0.25 * term);
@@ -3227,7 +3226,7 @@
 		    case CAT:
 		     
 			  calcDiagptable(z, DNA_DATA, tr->partitionData[model].numberOfCategories, tr->partitionData[model].perSiteRates, tr->partitionData[model].EIGN, diagptable);
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 			  if(tr->saveMemory)
 			    {			     			      
 			      partitionLikelihood = evaluateGTRCAT_SAVE(ex1, ex2, tr->partitionData[model].rateCategory, tr->partitionData[model].wgt,
@@ -3263,7 +3262,7 @@
 #else
 		     			     	     		      
 		      calcDiagptable(z, DNA_DATA, 4, tr->partitionData[model].gammaRates, tr->partitionData[model].EIGN, diagptable);		    		    
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		      if(tr->saveMemory)						  			  
 			partitionLikelihood = evaluateGTRGAMMA_GAPPED_SAVE(ex1, ex2, tr->partitionData[model].wgt,
 									   x1_start, x2_start, tr->partitionData[model].tipVector,
@@ -3300,7 +3299,7 @@
 		    case CAT:	    
 		     	   
 		      calcDiagptable(z, AA_DATA, tr->partitionData[model].numberOfCategories, tr->partitionData[model].perSiteRates, tr->partitionData[model].EIGN, diagptable);
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		      if(tr->saveMemory)
 			{			 
 			  partitionLikelihood = evaluateGTRCATPROT_SAVE(ex1, ex2, tr->partitionData[model].rateCategory, tr->partitionData[model].wgt,
@@ -3327,7 +3326,7 @@
 		      else
 			{
 			  calcDiagptable(z, AA_DATA, 4, tr->partitionData[model].gammaRates, tr->partitionData[model].EIGN, diagptable);
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 			  if(tr->saveMemory)
 			    partitionLikelihood = evaluateGTRGAMMAPROT_GAPPED_SAVE(ex1, ex2, tr->partitionData[model].wgt,
 										   x1_start, x2_start, tr->partitionData[model].tipVector,
--- evaluatePartialGenericSpecial.c
+++ evaluatePartialGenericSpecial.c
@@ -40,16 +40,16 @@
 #include <string.h>
 #include "axml.h"
 
-#ifdef __SIM_SSE3
-#include <xmmintrin.h>
-#include <pmmintrin.h>
+#ifdef __SIM_VECLIB
+#include <vec128int.h>
+#include <vec128dp.h>
 #endif
 
 
 /********************** GTRCAT ***************************************/
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 static inline void computeVectorGTRCATPROT(double *lVector, int *eVector, double ki, int i, double qz, double rz,
 					   traversalInfo *ti, double *EIGN, double *EI, double *EV, double *tipVector, 
@@ -104,46 +104,46 @@
 
     for(l = 0; l < 20; l+=2)
       {
-	__m128d d1v = _mm_mul_pd(_mm_load_pd(&x1[l]), _mm_load_pd(&e1[l]));
-	__m128d d2v = _mm_mul_pd(_mm_load_pd(&x2[l]), _mm_load_pd(&e2[l]));
+	__m128d d1v = vec_multiply2dp(vec_load2dpaligned(&x1[l]), vec_load2dpaligned(&e1[l]));
+	__m128d d2v = vec_multiply2dp(vec_load2dpaligned(&x2[l]), vec_load2dpaligned(&e2[l]));
 	
-	_mm_store_pd(&d1[l], d1v);
-	_mm_store_pd(&d2[l], d2v);	
+	vec_store2dpto2dp(&d1[l], d1v);
+	vec_store2dpto2dp(&d2[l], d2v);	
       }
 
-    __m128d zero = _mm_setzero_pd();
+    __m128d zero = vec_zero2dp();
 
     for(l = 0; l < 20; l+=2)
-      _mm_store_pd(&x3[l], zero);
+      vec_store2dpto2dp(&x3[l], zero);
                 
     for(l = 0; l < 20; l++)
       { 	      
 	double *ev = &EV[l * 20];
-	__m128d ump_x1v = _mm_setzero_pd();
-	__m128d ump_x2v = _mm_setzero_pd();
+	__m128d ump_x1v = vec_zero2dp();
+	__m128d ump_x2v = vec_zero2dp();
 	__m128d x1px2v;
 
 	for(k = 0; k < 20; k+=2)
 	  {       
-	    __m128d eiv = _mm_load_pd(&EI[20 * l + k]);
-	    __m128d d1v = _mm_load_pd(&d1[k]);
-	    __m128d d2v = _mm_load_pd(&d2[k]);
-	    ump_x1v = _mm_add_pd(ump_x1v, _mm_mul_pd(d1v, eiv));
-	    ump_x2v = _mm_add_pd(ump_x2v, _mm_mul_pd(d2v, eiv));	  
+	    __m128d eiv = vec_load2dpaligned(&EI[20 * l + k]);
+	    __m128d d1v = vec_load2dpaligned(&d1[k]);
+	    __m128d d2v = vec_load2dpaligned(&d2[k]);
+	    ump_x1v = vec_add2dp(ump_x1v, vec_multiply2dp(d1v, eiv));
+	    ump_x2v = vec_add2dp(ump_x2v, vec_multiply2dp(d2v, eiv));	  
 	  }
 
-	ump_x1v = _mm_hadd_pd(ump_x1v, ump_x1v);
-	ump_x2v = _mm_hadd_pd(ump_x2v, ump_x2v);
+	ump_x1v = vec_horizontaladd2dp(ump_x1v, ump_x1v);
+	ump_x2v = vec_horizontaladd2dp(ump_x2v, ump_x2v);
 
-	x1px2v = _mm_mul_pd(ump_x1v, ump_x2v);
+	x1px2v = vec_multiply2dp(ump_x1v, ump_x2v);
 
 	for(k = 0; k < 20; k+=2)
 	  {
-	    __m128d ex3v = _mm_load_pd(&x3[k]);
-	    __m128d EVV  = _mm_load_pd(&ev[k]);
-	    ex3v = _mm_add_pd(ex3v, _mm_mul_pd(x1px2v, EVV));
+	    __m128d ex3v = vec_load2dpaligned(&x3[k]);
+	    __m128d EVV  = vec_load2dpaligned(&ev[k]);
+	    ex3v = vec_add2dp(ex3v, vec_multiply2dp(x1px2v, EVV));
 	    
-	    _mm_store_pd(&x3[k], ex3v);	   	   
+	    vec_store2dpto2dp(&x3[k], ex3v);	   	   
 	  }
       }                      
     
@@ -153,12 +153,12 @@
     
     if(scale)
       {	      
-	__m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+	__m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
 	for(l = 0; l < 20; l+=2)
 	  {
-	    __m128d ex3v = _mm_mul_pd(_mm_load_pd(&x3[l]),twoto);
-	    _mm_store_pd(&x3[l], ex3v);	
+	    __m128d ex3v = vec_multiply2dp(vec_load2dpaligned(&x3[l]),twoto);
+	    vec_store2dpto2dp(&x3[l], ex3v);	
 	  }
  	
 	/*
--- fastDNAparsimony.c
+++ fastDNAparsimony.c
@@ -73,6 +73,13 @@
 
 #endif
 
+#ifdef __SIM_VECLIB
+
+#include <vec128int.h>
+#include <vec128dp.h>
+
+#endif
+
 
 #include "axml.h"
 
@@ -112,6 +119,23 @@
 
 #endif
 
+#ifdef __SIM_VECLIB
+
+// 256-bit intrinsics not yet fully implemented, so use the 128-bit SSE3
+// equivalents instead
+#define INTS_PER_VECTOR 4
+#define INT_TYPE __m128i
+#define CAST __m128i*
+#define SET_ALL_BITS_ONE vec_set4sw(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF)
+#define SET_ALL_BITS_ZERO vec_set4sw(0x00000000, 0x00000000, 0x00000000, 0x00000000)
+#define VECTOR_LOAD vec_load1q
+#define VECTOR_BIT_AND vec_bitand1q
+#define VECTOR_BIT_OR  vec_bitor1q
+#define VECTOR_STORE  vec_store1q
+#define VECTOR_AND_NOT vec_bitandnotleft1q
+
+#endif
+
 extern double masterTime;
 extern char  workdir[1024];
 extern char run_id[128];
@@ -185,7 +209,7 @@
 
 
 
-#if (defined(__SIM_SSE3) || defined(__AVX))
+#if (defined(__SIM_VECLIB) || defined(__SIM_SSE3) || defined(__AVX))
 
 static inline unsigned int populationCount(INT_TYPE v_N)
 {
@@ -213,7 +237,7 @@
 
 #endif
 
-#if (defined(__SIM_SSE3) || defined(__AVX))
+#if (defined(__SIM_VECLIB) || defined(__SIM_SSE3) || defined(__AVX))
 
 void newviewParsimonyIterativeFast(tree *tr)
 {    
@@ -1550,7 +1574,7 @@
       if(entries % PCF != 0)
 	compressedEntries++;
 
-#if (defined(__SIM_SSE3) || defined(__AVX))
+#if (defined(__SIM_VECLIB) || defined(__SIM_SSE3) || defined(__AVX))
       if(compressedEntries % INTS_PER_VECTOR != 0)
 	compressedEntriesPadded = compressedEntries + (INTS_PER_VECTOR - (compressedEntries % INTS_PER_VECTOR));
       else
--- makenewzGenericSpecial.c
+++ makenewzGenericSpecial.c
@@ -43,10 +43,9 @@
 #include <string.h>
 #include "axml.h"
 
-#ifdef __SIM_SSE3
-#include <xmmintrin.h>
-#include <pmmintrin.h>
-/*#include <tmmintrin.h>*/
+#ifdef __SIM_VECLIB
+#include <vec128int.h>
+#include <vec128dp.h>
 #endif
 
 #ifdef _USE_PTHREADS
@@ -65,7 +64,7 @@
 {
   int i;
   
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
   int j;
 #endif
   double *x1, *x2;
@@ -78,11 +77,11 @@
 	  x1 = &(tipVector[2 * tipX1[i]]);
 	  x2 = &(tipVector[2 * tipX2[i]]);
 
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 2; j++)
 	    sum[i * 2 + j]     = x1[j] * x2[j];
 #else
-	  _mm_store_pd(&sum[i * 2], _mm_mul_pd( _mm_load_pd(x1), _mm_load_pd(x2)));
+	  vec_store2dpto2dp(&sum[i * 2], vec_multiply2dp( vec_load2dpaligned(x1), vec_load2dpaligned(x2)));
 #endif
 	}
       break;
@@ -92,11 +91,11 @@
 	  x1 = &(tipVector[2 * tipX1[i]]);
 	  x2 = &x2_start[2 * i];
 
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 2; j++)
 	    sum[i * 2 + j]     = x1[j] * x2[j];
 #else
-	  _mm_store_pd(&sum[i * 2], _mm_mul_pd( _mm_load_pd(x1), _mm_load_pd(x2)));  
+	  vec_store2dpto2dp(&sum[i * 2], vec_multiply2dp( vec_load2dpaligned(x1), vec_load2dpaligned(x2)));  
 #endif
 	}
       break;
@@ -105,11 +104,11 @@
 	{
 	  x1 = &x1_start[2 * i];
 	  x2 = &x2_start[2 * i];
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 2; j++)
 	    sum[i * 2 + j]     = x1[j] * x2[j];
 #else
-	  _mm_store_pd(&sum[i * 2], _mm_mul_pd( _mm_load_pd(x1), _mm_load_pd(x2)));   
+	  vec_store2dpto2dp(&sum[i * 2], vec_multiply2dp( vec_load2dpaligned(x1), vec_load2dpaligned(x2)));   
 #endif
 	}
       break;
@@ -117,7 +116,7 @@
       assert(0);
     }
 }
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 static void sumCAT(int tipCase, double *sum, double *x1_start, double *x2_start, double *tipVector,
 		   unsigned char *tipX1, unsigned char *tipX2, int n)
 {
@@ -185,8 +184,8 @@
         x1 = &(tipVector[4 * tipX1[i]]);
         x2 = &(tipVector[4 * tipX2[i]]);
 
-        _mm_store_pd( &sum[i*4 + 0], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[0] )));
-        _mm_store_pd( &sum[i*4 + 2], _mm_mul_pd( _mm_load_pd( &x1[2] ), _mm_load_pd( &x2[2] )));
+        vec_store2dpto2dp( &sum[i*4 + 0], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[0] )));
+        vec_store2dpto2dp( &sum[i*4 + 2], vec_multiply2dp( vec_load2dpaligned( &x1[2] ), vec_load2dpaligned( &x2[2] )));
       }
       break;
     case TIP_INNER:
@@ -201,8 +200,8 @@
           x2_ptr += 4;
         }
 
-        _mm_store_pd( &sum[i*4 + 0], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[0] )));
-        _mm_store_pd( &sum[i*4 + 2], _mm_mul_pd( _mm_load_pd( &x1[2] ), _mm_load_pd( &x2[2] )));
+        vec_store2dpto2dp( &sum[i*4 + 0], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[0] )));
+        vec_store2dpto2dp( &sum[i*4 + 2], vec_multiply2dp( vec_load2dpaligned( &x1[2] ), vec_load2dpaligned( &x2[2] )));
       }
       break;
     case INNER_INNER:
@@ -224,8 +223,8 @@
           x2_ptr += 4;
         }
 
-        _mm_store_pd( &sum[i*4 + 0], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[0] )));
-        _mm_store_pd( &sum[i*4 + 2], _mm_mul_pd( _mm_load_pd( &x1[2] ), _mm_load_pd( &x2[2] )));
+        vec_store2dpto2dp( &sum[i*4 + 0], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[0] )));
+        vec_store2dpto2dp( &sum[i*4 + 2], vec_multiply2dp( vec_load2dpaligned( &x1[2] ), vec_load2dpaligned( &x2[2] )));
 
       }    
       break;
@@ -251,8 +250,8 @@
 	  x1 = &(tipVector[4 * tipX1[i]]);
 	  x2 = &(tipVector[4 * tipX2[i]]);
 
-	  _mm_store_pd( &sum[i*4 + 0], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[0] )));
-	  _mm_store_pd( &sum[i*4 + 2], _mm_mul_pd( _mm_load_pd( &x1[2] ), _mm_load_pd( &x2[2] )));
+	  vec_store2dpto2dp( &sum[i*4 + 0], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[0] )));
+	  vec_store2dpto2dp( &sum[i*4 + 2], vec_multiply2dp( vec_load2dpaligned( &x1[2] ), vec_load2dpaligned( &x2[2] )));
 	}
       break;
     case TIP_INNER:
@@ -261,8 +260,8 @@
 	  x1 = &(tipVector[4 * tipX1[i]]);
 	  x2 = &x2_start[4 * i];
 
-	  _mm_store_pd( &sum[i*4 + 0], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[0] )));
-	  _mm_store_pd( &sum[i*4 + 2], _mm_mul_pd( _mm_load_pd( &x1[2] ), _mm_load_pd( &x2[2] )));
+	  vec_store2dpto2dp( &sum[i*4 + 0], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[0] )));
+	  vec_store2dpto2dp( &sum[i*4 + 2], vec_multiply2dp( vec_load2dpaligned( &x1[2] ), vec_load2dpaligned( &x2[2] )));
 	}
       break;
     case INNER_INNER:
@@ -271,8 +270,8 @@
 	  x1 = &x1_start[4 * i];
 	  x2 = &x2_start[4 * i];
 
-	  _mm_store_pd( &sum[i*4 + 0], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[0] )));
-	  _mm_store_pd( &sum[i*4 + 2], _mm_mul_pd( _mm_load_pd( &x1[2] ), _mm_load_pd( &x2[2] )));
+	  vec_store2dpto2dp( &sum[i*4 + 0], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[0] )));
+	  vec_store2dpto2dp( &sum[i*4 + 2], vec_multiply2dp( vec_load2dpaligned( &x1[2] ), vec_load2dpaligned( &x2[2] )));
 
 	}    
       break;
@@ -340,7 +339,7 @@
   rax_free(d_start);
 }
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 static void coreGTRCAT(int upper, int numberOfCategories, double *sum,
 			   volatile double *d1, volatile double *d2,
@@ -369,11 +368,11 @@
   e1[3] = EIGN[2];
   e2[3] = EIGN[2] * EIGN[2];
 
-  e1v[0]= _mm_load_pd(&e1[0]);
-  e1v[1]= _mm_load_pd(&e1[2]);
+  e1v[0]= vec_load2dpaligned(&e1[0]);
+  e1v[1]= vec_load2dpaligned(&e1[2]);
 
-  e2v[0]= _mm_load_pd(&e2[0]);
-  e2v[1]= _mm_load_pd(&e2[2]);
+  e2v[0]= vec_load2dpaligned(&e2[0]);
+  e2v[1]= vec_load2dpaligned(&e2[2]);
 
   d = d_start = (double *)rax_malloc(numberOfCategories * 4 * sizeof(double));
 
@@ -399,22 +398,22 @@
       
       d = &d_start[4 * cptr[i]];  
       
-      __m128d tmp_0v =_mm_mul_pd(_mm_load_pd(&d[0]),_mm_load_pd(&s[0]));
-      __m128d tmp_1v =_mm_mul_pd(_mm_load_pd(&d[2]),_mm_load_pd(&s[2]));
+      __m128d tmp_0v =vec_multiply2dp(vec_load2dpaligned(&d[0]),vec_load2dpaligned(&s[0]));
+      __m128d tmp_1v =vec_multiply2dp(vec_load2dpaligned(&d[2]),vec_load2dpaligned(&s[2]));
 
-      __m128d inv_Liv    = _mm_add_pd(tmp_0v, tmp_1v);      
+      __m128d inv_Liv    = vec_add2dp(tmp_0v, tmp_1v);      
             	  
-      __m128d dlnLidlzv   = _mm_add_pd(_mm_mul_pd(tmp_0v, e1v[0]), _mm_mul_pd(tmp_1v, e1v[1]));	  
-      __m128d d2lnLidlz2v = _mm_add_pd(_mm_mul_pd(tmp_0v, e2v[0]), _mm_mul_pd(tmp_1v, e2v[1]));
+      __m128d dlnLidlzv   = vec_add2dp(vec_multiply2dp(tmp_0v, e1v[0]), vec_multiply2dp(tmp_1v, e1v[1]));	  
+      __m128d d2lnLidlz2v = vec_add2dp(vec_multiply2dp(tmp_0v, e2v[0]), vec_multiply2dp(tmp_1v, e2v[1]));
 
 
-      inv_Liv   = _mm_hadd_pd(inv_Liv, inv_Liv);
-      dlnLidlzv = _mm_hadd_pd(dlnLidlzv, dlnLidlzv);
-      d2lnLidlz2v = _mm_hadd_pd(d2lnLidlz2v, d2lnLidlz2v);                 
+      inv_Liv   = vec_horizontaladd2dp(inv_Liv, inv_Liv);
+      dlnLidlzv = vec_horizontaladd2dp(dlnLidlzv, dlnLidlzv);
+      d2lnLidlz2v = vec_horizontaladd2dp(d2lnLidlz2v, d2lnLidlz2v);                 
  
-      _mm_storel_pd(&inv_Li, inv_Liv);     
-      _mm_storel_pd(&dlnLidlz, dlnLidlzv);                 
-      _mm_storel_pd(&d2lnLidlz2, d2lnLidlz2v);      
+      vec_storelower1dpof2dp(&inv_Li, inv_Liv);     
+      vec_storelower1dpof2dp(&dlnLidlz, dlnLidlzv);                 
+      vec_storelower1dpof2dp(&d2lnLidlz2, d2lnLidlz2v);      
 
       inv_Li = 1.0/FABS(inv_Li);
 
@@ -512,7 +511,7 @@
 
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 static void sumGTRCATPROT_SAVE(int tipCase, double *sumtable, double *x1, double *x2, double *tipVector,
     unsigned char *tipX1, unsigned char *tipX2, int n, 
     double *x1_gapColumn, double *x2_gapColumn, unsigned int *x1_gap, unsigned int *x2_gap)
@@ -539,9 +538,9 @@
 
         for(l = 0; l < 20; l+=2)
         {
-          __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
+          __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
 
-          _mm_store_pd(&sum[l], sumv);		 
+          vec_store2dpto2dp(&sum[l], sumv);		 
         }
 
       }
@@ -563,9 +562,9 @@
 
         for(l = 0; l < 20; l+=2)
         {
-          __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
+          __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
 
-          _mm_store_pd(&sum[l], sumv);		 
+          vec_store2dpto2dp(&sum[l], sumv);		 
         }
 
       }
@@ -593,9 +592,9 @@
 
         for(l = 0; l < 20; l+=2)
         {
-          __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
+          __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
 
-          _mm_store_pd(&sum[l], sumv);		 
+          vec_store2dpto2dp(&sum[l], sumv);		 
         }
       }
       break;
@@ -622,12 +621,12 @@
 	  left  = &(tipVector[20 * tipX1[i]]);
 	  right = &(tipVector[20 * tipX2[i]]);
 	  sum = &sumtable[20 * i];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  for(l = 0; l < 20; l+=2)
 	    {
-	      __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
+	      __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
 	      
-	      _mm_store_pd(&sum[l], sumv);		 
+	      vec_store2dpto2dp(&sum[l], sumv);		 
 	    }
 #else
 	  for(l = 0; l < 20; l++)
@@ -641,12 +640,12 @@
 	  left = &(tipVector[20 * tipX1[i]]);
 	  right = &x2[20 * i];
 	  sum = &sumtable[20 * i];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  for(l = 0; l < 20; l+=2)
 	    {
-	      __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
+	      __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
 	      
-	      _mm_store_pd(&sum[l], sumv);		 
+	      vec_store2dpto2dp(&sum[l], sumv);		 
 	    }
 #else
 	  for(l = 0; l < 20; l++)
@@ -660,12 +659,12 @@
 	  left  = &x1[20 * i];
 	  right = &x2[20 * i];
 	  sum = &sumtable[20 * i];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	  for(l = 0; l < 20; l+=2)
 	    {
-	      __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[l]), _mm_load_pd(&right[l]));
+	      __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[l]), vec_load2dpaligned(&right[l]));
 	      
-	      _mm_store_pd(&sum[l], sumv);		 
+	      vec_store2dpto2dp(&sum[l], sumv);		 
 	    }
 #else
 	  for(l = 0; l < 20; l++)
@@ -867,7 +866,7 @@
 }
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 static void coreGTRCATPROT(double *EIGN, double lz, int numberOfCategories, double *rptr, int *cptr, int upper,
 			   volatile double *ext_dlnLdlz,  volatile double *ext_d2lnLdlz2, double *sumtable, int *wgt)
@@ -908,33 +907,33 @@
 	wr1 = r * wgt[i],
 	wr2 = r * r * wgt[i];
 
-      __m128d a0 = _mm_setzero_pd();
-      __m128d a1 = _mm_setzero_pd();
-      __m128d a2 = _mm_setzero_pd();
+      __m128d a0 = vec_zero2dp();
+      __m128d a1 = vec_zero2dp();
+      __m128d a2 = vec_zero2dp();
 
       d1 = &d_start[20 * cptr[i]];
       sum = &sumtable[20 * i];
           
       for(l = 0; l < 20; l+=2)
 	{	  
-	  __m128d tmpv = _mm_mul_pd(_mm_load_pd(&d1[l]), _mm_load_pd(&sum[l]));
+	  __m128d tmpv = vec_multiply2dp(vec_load2dpaligned(&d1[l]), vec_load2dpaligned(&sum[l]));
 	  
-	  a0 = _mm_add_pd(a0, tmpv);
-	  __m128d sv = _mm_load_pd(&s[l]);	  
+	  a0 = vec_add2dp(a0, tmpv);
+	  __m128d sv = vec_load2dpaligned(&s[l]);	  
 	  
-	  a1 = _mm_add_pd(a1, _mm_mul_pd(tmpv, sv));
-	  __m128d ev = _mm_load_pd(&e[l]);	  
+	  a1 = vec_add2dp(a1, vec_multiply2dp(tmpv, sv));
+	  __m128d ev = vec_load2dpaligned(&e[l]);	  
 
-	  a2 = _mm_add_pd(a2, _mm_mul_pd(tmpv, ev));
+	  a2 = vec_add2dp(a2, vec_multiply2dp(tmpv, ev));
 	}
 
-      a0 = _mm_hadd_pd(a0, a0);
-      a1 = _mm_hadd_pd(a1, a1);
-      a2 = _mm_hadd_pd(a2, a2);
+      a0 = vec_horizontaladd2dp(a0, a0);
+      a1 = vec_horizontaladd2dp(a1, a1);
+      a2 = vec_horizontaladd2dp(a2, a2);
 
-      _mm_storel_pd(&inv_Li, a0);     
-      _mm_storel_pd(&dlnLidlz, a1);                 
-      _mm_storel_pd(&d2lnLidlz2, a2);
+      vec_storelower1dpof2dp(&inv_Li, a0);     
+      vec_storelower1dpof2dp(&dlnLidlz, a1);                 
+      vec_storelower1dpof2dp(&d2lnLidlz2, a2);
       
       inv_Li = 1.0/FABS(inv_Li);
 
@@ -1916,7 +1915,7 @@
 {
   double *x1, *x2, *sum;
   int i, j;
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
   int k;
 #endif
 
@@ -1931,13 +1930,13 @@
 	  x1 = &(tipVector[2 * tipX1[i]]);
 	  x2 = &(tipVector[2 * tipX2[i]]);
 	  sum = &sumtable[i * 8];
-#ifndef __SIM_SSE3	  
+#ifndef __SIM_VECLIB	  
 	  for(j = 0; j < 4; j++)
 	    for(k = 0; k < 2; k++)
 	      sum[j * 2 + k] = x1[k] * x2[k];
 #else
 	  for(j = 0; j < 4; j++)
-	    _mm_store_pd( &sum[j*2], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[0] )));	 
+	    vec_store2dpto2dp( &sum[j*2], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[0] )));	 
 #endif
 	}
       break;
@@ -1948,13 +1947,13 @@
 	  x2  = &x2_start[8 * i];
 	  sum = &sumtable[8 * i];
 
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)
 	    for(k = 0; k < 2; k++)
 	      sum[j * 2 + k] = x1[k] * x2[j * 2 + k];
 #else
 	  for(j = 0; j < 4; j++)
-	    _mm_store_pd( &sum[j*2], _mm_mul_pd( _mm_load_pd( &x1[0] ), _mm_load_pd( &x2[j * 2] )));
+	    vec_store2dpto2dp( &sum[j*2], vec_multiply2dp( vec_load2dpaligned( &x1[0] ), vec_load2dpaligned( &x2[j * 2] )));
 #endif
 	}
       break;
@@ -1964,13 +1963,13 @@
 	  x1  = &x1_start[8 * i];
 	  x2  = &x2_start[8 * i];
 	  sum = &sumtable[8 * i];
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)
 	    for(k = 0; k < 2; k++)
 	      sum[j * 2 + k] = x1[j * 2 + k] * x2[j * 2 + k];
 #else
 	  for(j = 0; j < 4; j++)
-	    _mm_store_pd( &sum[j*2], _mm_mul_pd( _mm_load_pd( &x1[j * 2] ), _mm_load_pd( &x2[j * 2] )));
+	    vec_store2dpto2dp( &sum[j*2], vec_multiply2dp( vec_load2dpaligned( &x1[j * 2] ), vec_load2dpaligned( &x2[j * 2] )));
 #endif
 	}
       break;
@@ -2002,14 +2001,14 @@
 	  x1 = &(tipVector[4 * tipX1[i]]);
 	  x2 = &(tipVector[4 * tipX2[i]]);
 	  sum = &sumtable[i * 16];
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k++)
 	      sum[j * 4 + k] = x1[k] * x2[k];
 #else
 	  for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k+=2)
-	      _mm_store_pd( &sum[j*4 + k], _mm_mul_pd( _mm_load_pd( &x1[k] ), _mm_load_pd( &x2[k] )));
+	      vec_store2dpto2dp( &sum[j*4 + k], vec_multiply2dp( vec_load2dpaligned( &x1[k] ), vec_load2dpaligned( &x2[k] )));
 #endif
 	}
       break;
@@ -2027,14 +2026,14 @@
 	    }
 	  
 	  sum = &sumtable[16 * i];
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)
 	    for(k = 0; k < 4; k++)
 	      sum[j * 4 + k] = x1[k] * x2[j * 4 + k];
 #else
 	  for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k+=2)
-	      _mm_store_pd( &sum[j*4 + k], _mm_mul_pd( _mm_load_pd( &x1[k] ), _mm_load_pd( &x2[j * 4 + k] )));
+	      vec_store2dpto2dp( &sum[j*4 + k], vec_multiply2dp( vec_load2dpaligned( &x1[k] ), vec_load2dpaligned( &x2[j * 4 + k] )));
 #endif
 	}
       break;
@@ -2059,14 +2058,14 @@
 
 	  sum = &sumtable[16 * i];
 	  
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)
 	    for(k = 0; k < 4; k++)
 	      sum[j * 4 + k] = x1[j * 4 + k] * x2[j * 4 + k];
 #else
 	   for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k+=2)
-	      _mm_store_pd( &sum[j*4 + k], _mm_mul_pd( _mm_load_pd( &x1[j * 4 + k] ), _mm_load_pd( &x2[j * 4 + k] )));
+	      vec_store2dpto2dp( &sum[j*4 + k], vec_multiply2dp( vec_load2dpaligned( &x1[j * 4 + k] ), vec_load2dpaligned( &x2[j * 4 + k] )));
 #endif
 	}
       break;
@@ -2096,14 +2095,14 @@
 	  x1 = &(tipVector[4 * tipX1[i]]);
 	  x2 = &(tipVector[4 * tipX2[i]]);
 	  sum = &sumtable[i * 16];
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k++)
 	      sum[j * 4 + k] = x1[k] * x2[k];
 #else
 	  for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k+=2)
-	      _mm_store_pd( &sum[j*4 + k], _mm_mul_pd( _mm_load_pd( &x1[k] ), _mm_load_pd( &x2[k] )));
+	      vec_store2dpto2dp( &sum[j*4 + k], vec_multiply2dp( vec_load2dpaligned( &x1[k] ), vec_load2dpaligned( &x2[k] )));
 #endif
 	}
       break;
@@ -2113,14 +2112,14 @@
 	  x1  = &(tipVector[4 * tipX1[i]]);
 	  x2  = &x2_start[16 * i];
 	  sum = &sumtable[16 * i];
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)
 	    for(k = 0; k < 4; k++)
 	      sum[j * 4 + k] = x1[k] * x2[j * 4 + k];
 #else
 	  for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k+=2)
-	      _mm_store_pd( &sum[j*4 + k], _mm_mul_pd( _mm_load_pd( &x1[k] ), _mm_load_pd( &x2[j * 4 + k] )));
+	      vec_store2dpto2dp( &sum[j*4 + k], vec_multiply2dp( vec_load2dpaligned( &x1[k] ), vec_load2dpaligned( &x2[j * 4 + k] )));
 #endif
 	}
       break;
@@ -2130,14 +2129,14 @@
 	  x1  = &x1_start[16 * i];
 	  x2  = &x2_start[16 * i];
 	  sum = &sumtable[16 * i];
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 	  for(j = 0; j < 4; j++)
 	    for(k = 0; k < 4; k++)
 	      sum[j * 4 + k] = x1[j * 4 + k] * x2[j * 4 + k];
 #else
 	   for(j = 0; j < 4; j++)	    
 	    for(k = 0; k < 4; k+=2)
-	      _mm_store_pd( &sum[j*4 + k], _mm_mul_pd( _mm_load_pd( &x1[j * 4 + k] ), _mm_load_pd( &x2[j * 4 + k] )));
+	      vec_store2dpto2dp( &sum[j*4 + k], vec_multiply2dp( vec_load2dpaligned( &x1[j * 4 + k] ), vec_load2dpaligned( &x2[j * 4 + k] )));
 #endif
 	}
       break;
@@ -2149,7 +2148,7 @@
 
 
 
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 static void coreGTRGAMMA_BINARY(const int upper, double *sumtable,
 				volatile double *d1,   volatile double *d2, double *EIGN, double *gammaRates, double lz, int *wrptr)
 {
@@ -2243,9 +2242,9 @@
 
   for (i = 0; i < upper; i++)
     { 
-      __m128d a0 = _mm_setzero_pd();
-      __m128d a1 = _mm_setzero_pd();
-      __m128d a2 = _mm_setzero_pd();
+      __m128d a0 = vec_zero2dp();
+      __m128d a1 = vec_zero2dp();
+      __m128d a2 = vec_zero2dp();
 
       sum = &sumtable[i * 8];         
 
@@ -2256,20 +2255,20 @@
 	    *d1 = &diagptable1[j * 2],
 	    *d2 = &diagptable2[j * 2];
   	 	 	 
-	  __m128d tmpv = _mm_mul_pd(_mm_load_pd(d0), _mm_load_pd(&sum[j * 2]));
-	  a0 = _mm_add_pd(a0, tmpv);
-	  a1 = _mm_add_pd(a1, _mm_mul_pd(tmpv, _mm_load_pd(d1)));
-	  a2 = _mm_add_pd(a2, _mm_mul_pd(tmpv, _mm_load_pd(d2)));
+	  __m128d tmpv = vec_multiply2dp(vec_load2dpaligned(d0), vec_load2dpaligned(&sum[j * 2]));
+	  a0 = vec_add2dp(a0, tmpv);
+	  a1 = vec_add2dp(a1, vec_multiply2dp(tmpv, vec_load2dpaligned(d1)));
+	  a2 = vec_add2dp(a2, vec_multiply2dp(tmpv, vec_load2dpaligned(d2)));
 	    	 	  
 	}
 
-      a0 = _mm_hadd_pd(a0, a0);
-      a1 = _mm_hadd_pd(a1, a1);
-      a2 = _mm_hadd_pd(a2, a2);
+      a0 = vec_horizontaladd2dp(a0, a0);
+      a1 = vec_horizontaladd2dp(a1, a1);
+      a2 = vec_horizontaladd2dp(a2, a2);
 
-      _mm_storel_pd(&inv_Li, a0);     
-      _mm_storel_pd(&dlnLidlz, a1);
-      _mm_storel_pd(&d2lnLidlz2, a2); 
+      vec_storelower1dpof2dp(&inv_Li, a0);     
+      vec_storelower1dpof2dp(&dlnLidlz, a1);
+      vec_storelower1dpof2dp(&d2lnLidlz2, a2); 
 
       inv_Li = 1.0 / FABS(inv_Li);
      
@@ -2288,7 +2287,7 @@
 
 #endif
 
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 static void coreGTRGAMMA(const int upper, double *sumtable,
 			 volatile double *d1,   volatile double *d2, double *EIGN, double *gammaRates, double lz, int *wrptr)
 {
@@ -2401,9 +2400,9 @@
 
   for (i = 0; i < upper; i++)
     { 
-      __m128d a0 = _mm_setzero_pd();
-      __m128d a1 = _mm_setzero_pd();
-      __m128d a2 = _mm_setzero_pd();
+      __m128d a0 = vec_zero2dp();
+      __m128d a1 = vec_zero2dp();
+      __m128d a2 = vec_zero2dp();
 
       sum = &sumtable[i * 16];         
 
@@ -2416,20 +2415,20 @@
   	 	 
 	  for(l = 0; l < 4; l+=2)
 	    {
-	      __m128d tmpv = _mm_mul_pd(_mm_load_pd(&d0[l]), _mm_load_pd(&sum[j * 4 + l]));
-	      a0 = _mm_add_pd(a0, tmpv);
-	      a1 = _mm_add_pd(a1, _mm_mul_pd(tmpv, _mm_load_pd(&d1[l])));
-	      a2 = _mm_add_pd(a2, _mm_mul_pd(tmpv, _mm_load_pd(&d2[l])));
+	      __m128d tmpv = vec_multiply2dp(vec_load2dpaligned(&d0[l]), vec_load2dpaligned(&sum[j * 4 + l]));
+	      a0 = vec_add2dp(a0, tmpv);
+	      a1 = vec_add2dp(a1, vec_multiply2dp(tmpv, vec_load2dpaligned(&d1[l])));
+	      a2 = vec_add2dp(a2, vec_multiply2dp(tmpv, vec_load2dpaligned(&d2[l])));
 	    }	 	  
 	}
 
-      a0 = _mm_hadd_pd(a0, a0);
-      a1 = _mm_hadd_pd(a1, a1);
-      a2 = _mm_hadd_pd(a2, a2);
+      a0 = vec_horizontaladd2dp(a0, a0);
+      a1 = vec_horizontaladd2dp(a1, a1);
+      a2 = vec_horizontaladd2dp(a2, a2);
 
-      _mm_storel_pd(&inv_Li, a0);     
-      _mm_storel_pd(&dlnLidlz, a1);
-      _mm_storel_pd(&d2lnLidlz2, a2);       
+      vec_storelower1dpof2dp(&inv_Li, a0);     
+      vec_storelower1dpof2dp(&dlnLidlz, a1);
+      vec_storelower1dpof2dp(&d2lnLidlz2, a2);       
 
       inv_Li = 1.0 / FABS(inv_Li);
      
@@ -2472,12 +2471,12 @@
 	  for(l = 0; l < 4; l++)
 	    {
 	      sum = &sumtable[i * 80 + l * 20];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	      for(k = 0; k < 20; k+=2)
 		{
-		  __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+		  __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 		  
-		  _mm_store_pd(&sum[k], sumv);		 
+		  vec_store2dpto2dp(&sum[k], sumv);		 
 		}
 #else
 	      for(k = 0; k < 20; k++)
@@ -2495,12 +2494,12 @@
 	    {
 	      right = &(x2[80 * i + l * 20]);
 	      sum = &sumtable[i * 80 + l * 20];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	      for(k = 0; k < 20; k+=2)
 		{
-		  __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+		  __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 		  
-		  _mm_store_pd(&sum[k], sumv);		 
+		  vec_store2dpto2dp(&sum[k], sumv);		 
 		}
 #else
 	      for(k = 0; k < 20; k++)
@@ -2518,12 +2517,12 @@
 	      right = &(x2[80 * i + l * 20]);
 	      sum   = &(sumtable[i * 80 + l * 20]);
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	      for(k = 0; k < 20; k+=2)
 		{
-		  __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+		  __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 		  
-		  _mm_store_pd(&sum[k], sumv);		 
+		  vec_store2dpto2dp(&sum[k], sumv);		 
 		}
 #else
 	      for(k = 0; k < 20; k++)
@@ -2554,12 +2553,12 @@
 	      right = &(tipVector[l][20 * tipX2[i]]);
 
 	      sum = &sumtable[i * 80 + l * 20];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	      for(k = 0; k < 20; k+=2)
 		{
-		  __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+		  __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 		  
-		  _mm_store_pd(&sum[k], sumv);		 
+		  vec_store2dpto2dp(&sum[k], sumv);		 
 		}
 #else
 	      for(k = 0; k < 20; k++)
@@ -2578,12 +2577,12 @@
 	      left = &(tipVector[l][20 * tipX1[i]]);
 	      right = &(x2[80 * i + l * 20]);
 	      sum = &sumtable[i * 80 + l * 20];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	      for(k = 0; k < 20; k+=2)
 		{
-		  __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+		  __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 		  
-		  _mm_store_pd(&sum[k], sumv);		 
+		  vec_store2dpto2dp(&sum[k], sumv);		 
 		}
 #else
 	      for(k = 0; k < 20; k++)
@@ -2601,12 +2600,12 @@
 	      right = &(x2[80 * i + l * 20]);
 	      sum   = &(sumtable[i * 80 + l * 20]);
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	      for(k = 0; k < 20; k+=2)
 		{
-		  __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+		  __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 		  
-		  _mm_store_pd(&sum[k], sumv);		 
+		  vec_store2dpto2dp(&sum[k], sumv);		 
 		}
 #else
 	      for(k = 0; k < 20; k++)
@@ -2621,7 +2620,7 @@
 }
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 static void sumGAMMAPROT_GAPPED_SAVE(int tipCase, double *sumtable, double *x1, double *x2, double *tipVector,
     unsigned char *tipX1, unsigned char *tipX2, int n, 
     double *x1_gapColumn, double *x2_gapColumn, unsigned int *x1_gap, unsigned int *x2_gap)
@@ -2650,9 +2649,9 @@
 
           for(k = 0; k < 20; k+=2)
           {
-            __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+            __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 
-            _mm_store_pd(&sum[k], sumv);		 
+            vec_store2dpto2dp(&sum[k], sumv);		 
           }
 
         }
@@ -2678,9 +2677,9 @@
 
           for(k = 0; k < 20; k+=2)
           {
-            __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+            __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 
-            _mm_store_pd(&sum[k], sumv);		 
+            vec_store2dpto2dp(&sum[k], sumv);		 
           }
         }
       }
@@ -2712,9 +2711,9 @@
 
           for(k = 0; k < 20; k+=2)
           {
-            __m128d sumv = _mm_mul_pd(_mm_load_pd(&left[k]), _mm_load_pd(&right[k]));
+            __m128d sumv = vec_multiply2dp(vec_load2dpaligned(&left[k]), vec_load2dpaligned(&right[k]));
 
-            _mm_store_pd(&sum[k], sumv);		 
+            vec_store2dpto2dp(&sum[k], sumv);		 
           }
         }
       }
@@ -3099,7 +3098,7 @@
     }
 }
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 static void coreGTRGAMMAPROT(double *gammaRates, double *EIGN, double *sumtable, int upper, int *wrptr,
 			      volatile double *ext_dlnLdlz,  volatile double *ext_d2lnLdlz2, double lz)
@@ -3133,9 +3132,9 @@
 
   for (i = 0; i < upper; i++)
     { 
-      __m128d a0 = _mm_setzero_pd();
-      __m128d a1 = _mm_setzero_pd();
-      __m128d a2 = _mm_setzero_pd();
+      __m128d a0 = vec_zero2dp();
+      __m128d a1 = vec_zero2dp();
+      __m128d a2 = vec_zero2dp();
 
       sum = &sumtable[i * 80];         
 
@@ -3148,20 +3147,20 @@
   	 	 
 	  for(l = 0; l < 20; l+=2)
 	    {
-	      __m128d tmpv = _mm_mul_pd(_mm_load_pd(&d0[l]), _mm_load_pd(&sum[j * 20 +l]));
-	      a0 = _mm_add_pd(a0, tmpv);
-	      a1 = _mm_add_pd(a1, _mm_mul_pd(tmpv, _mm_load_pd(&d1[l])));
-	      a2 = _mm_add_pd(a2, _mm_mul_pd(tmpv, _mm_load_pd(&d2[l])));
+	      __m128d tmpv = vec_multiply2dp(vec_load2dpaligned(&d0[l]), vec_load2dpaligned(&sum[j * 20 +l]));
+	      a0 = vec_add2dp(a0, tmpv);
+	      a1 = vec_add2dp(a1, vec_multiply2dp(tmpv, vec_load2dpaligned(&d1[l])));
+	      a2 = vec_add2dp(a2, vec_multiply2dp(tmpv, vec_load2dpaligned(&d2[l])));
 	    }	 	  
 	}
 
-      a0 = _mm_hadd_pd(a0, a0);
-      a1 = _mm_hadd_pd(a1, a1);
-      a2 = _mm_hadd_pd(a2, a2);
+      a0 = vec_horizontaladd2dp(a0, a0);
+      a1 = vec_horizontaladd2dp(a1, a1);
+      a2 = vec_horizontaladd2dp(a2, a2);
 
-      _mm_storel_pd(&inv_Li, a0);
-      _mm_storel_pd(&dlnLidlz, a1);
-      _mm_storel_pd(&d2lnLidlz2, a2);
+      vec_storelower1dpof2dp(&inv_Li, a0);
+      vec_storelower1dpof2dp(&dlnLidlz, a1);
+      vec_storelower1dpof2dp(&d2lnLidlz2, a2);
 
       inv_Li = 1.0 / FABS(inv_Li);
 
@@ -3225,27 +3224,27 @@
 	    *d2 = &diagptable2[j * 20];
 	  
 	  __m128d 
-	    a0 = _mm_setzero_pd(),
-	    a1 = _mm_setzero_pd(),
-	    a2 = _mm_setzero_pd();
+	    a0 = vec_zero2dp(),
+	    a1 = vec_zero2dp(),
+	    a2 = vec_zero2dp();
   	 	 
 	  for(l = 0; l < 20; l+=2)
 	    {
 	      __m128d 
-		tmpv = _mm_mul_pd(_mm_load_pd(&d0[l]), _mm_load_pd(&sum[j * 20 +l]));
+		tmpv = vec_multiply2dp(vec_load2dpaligned(&d0[l]), vec_load2dpaligned(&sum[j * 20 +l]));
 	      
-	      a0 = _mm_add_pd(a0, tmpv);
-	      a1 = _mm_add_pd(a1, _mm_mul_pd(tmpv, _mm_load_pd(&d1[l])));
-	      a2 = _mm_add_pd(a2, _mm_mul_pd(tmpv, _mm_load_pd(&d2[l])));
+	      a0 = vec_add2dp(a0, tmpv);
+	      a1 = vec_add2dp(a1, vec_multiply2dp(tmpv, vec_load2dpaligned(&d1[l])));
+	      a2 = vec_add2dp(a2, vec_multiply2dp(tmpv, vec_load2dpaligned(&d2[l])));
 	    }
 	  
-	  a0 = _mm_hadd_pd(a0, a0);
-	  a1 = _mm_hadd_pd(a1, a1);
-	  a2 = _mm_hadd_pd(a2, a2);
+	  a0 = vec_horizontaladd2dp(a0, a0);
+	  a1 = vec_horizontaladd2dp(a1, a1);
+	  a2 = vec_horizontaladd2dp(a2, a2);
 
-	  _mm_storel_pd(&l0, a0);
-	  _mm_storel_pd(&l1, a1);
-	  _mm_storel_pd(&l2, a2);
+	  vec_storelower1dpof2dp(&l0, a0);
+	  vec_storelower1dpof2dp(&l1, a1);
+	  vec_storelower1dpof2dp(&l2, a2);
 	  
 	  inv_Li     += weights[j] * l0;
 	  dlnLidlz   += weights[j] * l1;
@@ -4020,7 +4019,7 @@
 	      switch(tr->rateHetModel)
 		{
 		case CAT:		 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		  if(tr->saveMemory)
 		    {
 		      
@@ -4055,7 +4054,7 @@
 	      switch(tr->rateHetModel)
 		{
 		case CAT:	
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		  if(tr->saveMemory)	  
 		    sumGTRCATPROT_SAVE(tipCase, tr->partitionData[model].sumBuffer, x1_start, x2_start, tr->partitionData[model].tipVector,
 				       tipX1, tipX2, width, x1_gapColumn, x2_gapColumn, x1_gap, x2_gap);
@@ -4066,7 +4065,7 @@
 		  break;
 		case GAMMA:
 		case GAMMA_I:		  
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		  if(tr->saveMemory)
 		    sumGAMMAPROT_GAPPED_SAVE(tipCase, tr->partitionData[model].sumBuffer, x1_start, x2_start, tr->partitionData[model].tipVector, tipX1, tipX2,
 					     width, x1_gapColumn, x2_gapColumn, x1_gap, x2_gap);
--- newviewGenericSpecial.c
+++ newviewGenericSpecial.c
@@ -41,11 +41,11 @@
 #include <limits.h>
 #include "axml.h"
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 #include <stdint.h>
-#include <xmmintrin.h>
-#include <pmmintrin.h>
+#include <vec128int.h>
+#include <vec128dp.h>
 
 const union __attribute__ ((aligned (BYTE_ALIGNMENT)))
 {
@@ -886,7 +886,7 @@
       break;
     case DNA_DATA:
       {
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	double 
 	  d1[4] __attribute__ ((aligned (BYTE_ALIGNMENT))), 
 	  d2[4] __attribute__ ((aligned (BYTE_ALIGNMENT))),
@@ -924,11 +924,11 @@
 		d2[j+1] = EXP(rptr[i] * ez2[j]);
 	      }
 
-	    d1_0 = _mm_load_pd(&d1[0]);
-	    d1_1 = _mm_load_pd(&d1[2]);
+	    d1_0 = vec_load2dpaligned(&d1[0]);
+	    d1_1 = vec_load2dpaligned(&d1[2]);
 
-	    d2_0 = _mm_load_pd(&d2[0]);
-	    d2_1 = _mm_load_pd(&d2[2]);
+	    d2_0 = vec_load2dpaligned(&d2[0]);
+	    d2_1 = vec_load2dpaligned(&d2[2]);
 	    
 
 	    for(j = 0; j < 4; j++)
@@ -936,15 +936,15 @@
 		double *ll = &left[i * 16 + j * 4];
 		double *rr = &right[i * 16 + j * 4];	       
 
-		__m128d eev = _mm_load_pd(&EI_16[4 * j]);
+		__m128d eev = vec_load2dpaligned(&EI_16[4 * j]);
 		
-		_mm_store_pd(&ll[0], _mm_mul_pd(d1_0, eev));
-		_mm_store_pd(&rr[0], _mm_mul_pd(d2_0, eev));
+		vec_store2dpto2dp(&ll[0], vec_multiply2dp(d1_0, eev));
+		vec_store2dpto2dp(&rr[0], vec_multiply2dp(d2_0, eev));
 		
-		eev = _mm_load_pd(&EI_16[4 * j + 2]);
+		eev = vec_load2dpaligned(&EI_16[4 * j + 2]);
 		
-		_mm_store_pd(&ll[2], _mm_mul_pd(d1_1, eev));
-		_mm_store_pd(&rr[2], _mm_mul_pd(d2_1, eev));
+		vec_store2dpto2dp(&ll[2], vec_multiply2dp(d1_1, eev));
+		vec_store2dpto2dp(&rr[2], vec_multiply2dp(d2_1, eev));
 
 		
 	      }
@@ -968,26 +968,26 @@
 		  d2[j+1] = EXP(ez2[j]);
 		}	     
 	      
-	      d1_0 = _mm_load_pd(&d1[0]);
-	      d1_1 = _mm_load_pd(&d1[2]);
+	      d1_0 = vec_load2dpaligned(&d1[0]);
+	      d1_1 = vec_load2dpaligned(&d1[2]);
 	      
-	      d2_0 = _mm_load_pd(&d2[0]);
-	      d2_1 = _mm_load_pd(&d2[2]);
+	      d2_0 = vec_load2dpaligned(&d2[0]);
+	      d2_1 = vec_load2dpaligned(&d2[2]);
 	      	      
 	      for(j = 0; j < 4; j++)
 		{	       
 		  double *ll = &left[i * 16 + j * 4];
 		  double *rr = &right[i * 16 + j * 4];	       
 		  
-		  __m128d eev = _mm_load_pd(&EI_16[4 * j]);
+		  __m128d eev = vec_load2dpaligned(&EI_16[4 * j]);
 		  
-		  _mm_store_pd(&ll[0], _mm_mul_pd(d1_0, eev));
-		  _mm_store_pd(&rr[0], _mm_mul_pd(d2_0, eev));
+		  vec_store2dpto2dp(&ll[0], vec_multiply2dp(d1_0, eev));
+		  vec_store2dpto2dp(&rr[0], vec_multiply2dp(d2_0, eev));
 		  
-		  eev = _mm_load_pd(&EI_16[4 * j + 2]);
+		  eev = vec_load2dpaligned(&EI_16[4 * j + 2]);
 		  
-		  _mm_store_pd(&ll[2], _mm_mul_pd(d1_1, eev));
-		  _mm_store_pd(&rr[2], _mm_mul_pd(d2_1, eev));
+		  vec_store2dpto2dp(&ll[2], vec_multiply2dp(d1_1, eev));
+		  vec_store2dpto2dp(&rr[2], vec_multiply2dp(d2_1, eev));
 		  
 		  
 		}
@@ -1180,7 +1180,7 @@
 
 
 
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 
 static void newviewGTRCAT_BINARY( int tipCase,  double *EV,  int *cptr,
 				  double *x1_start,  double *x2_start,  double *x3_start,  double *tipVector,
@@ -1356,24 +1356,24 @@
 	    le =  &left[cptr[i] * 4];
 	    ri =  &right[cptr[i] * 4];
 
-	    _mm_store_pd(x3, _mm_setzero_pd());	    
+	    vec_store2dpto2dp(x3, vec_zero2dp());	    
 	    	     
 	    for(l = 0; l < 2; l++)
 	      {		 		 						   		  		 		 
-		__m128d al = _mm_mul_pd(_mm_load_pd(x1), _mm_load_pd(&le[l * 2]));
-		__m128d ar = _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(&ri[l * 2]));
+		__m128d al = vec_multiply2dp(vec_load2dpaligned(x1), vec_load2dpaligned(&le[l * 2]));
+		__m128d ar = vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(&ri[l * 2]));
 		
-		al = _mm_hadd_pd(al, al);
-		ar = _mm_hadd_pd(ar, ar);
+		al = vec_horizontaladd2dp(al, al);
+		ar = vec_horizontaladd2dp(ar, ar);
 		
-		al = _mm_mul_pd(al, ar);
+		al = vec_multiply2dp(al, ar);
 		
-		__m128d vv  = _mm_load_pd(x3);
-		__m128d EVV = _mm_load_pd(&EV[2 * l]);
+		__m128d vv  = vec_load2dpaligned(x3);
+		__m128d EVV = vec_load2dpaligned(&EV[2 * l]);
 		
-		vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+		vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 		
-		_mm_store_pd(x3, vv);		     	  		   		  
+		vec_store2dpto2dp(x3, vv);		     	  		   		  
 	      }	    	   
 	  }
       }
@@ -1389,41 +1389,41 @@
 	    le =  &left[cptr[i] * 4];
 	    ri =  &right[cptr[i] * 4];
 
-	    _mm_store_pd(x3, _mm_setzero_pd());	    
+	    vec_store2dpto2dp(x3, vec_zero2dp());	    
 	    	     
 	    for(l = 0; l < 2; l++)
 	      {		 		 						   		  		 		 
-		__m128d al = _mm_mul_pd(_mm_load_pd(x1), _mm_load_pd(&le[l * 2]));
-		__m128d ar = _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(&ri[l * 2]));
+		__m128d al = vec_multiply2dp(vec_load2dpaligned(x1), vec_load2dpaligned(&le[l * 2]));
+		__m128d ar = vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(&ri[l * 2]));
 		
-		al = _mm_hadd_pd(al, al);
-		ar = _mm_hadd_pd(ar, ar);
+		al = vec_horizontaladd2dp(al, al);
+		ar = vec_horizontaladd2dp(ar, ar);
 		
-		al = _mm_mul_pd(al, ar);
+		al = vec_multiply2dp(al, ar);
 		
-		__m128d vv  = _mm_load_pd(x3);
-		__m128d EVV = _mm_load_pd(&EV[2 * l]);
+		__m128d vv  = vec_load2dpaligned(x3);
+		__m128d EVV = vec_load2dpaligned(&EV[2 * l]);
 		
-		vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+		vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 		
-		_mm_store_pd(x3, vv);		     	  		   		  
+		vec_store2dpto2dp(x3, vv);		     	  		   		  
 	      }	 
 	    
-	    __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	    __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	 
 	    scale = 1;
 	    
-	    __m128d v1 = _mm_and_pd(_mm_load_pd(x3), absMask.m);
-	    v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	    if(_mm_movemask_pd( v1 ) != 3)
+	    __m128d v1 = vec_bitand2dp(vec_load2dpaligned(x3), absMask.m);
+	    v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	    if(vec_extractupperbit2dp( v1 ) != 3)
 	      scale = 0;	  	         
 	    
 	    if(scale)
 	      {
-		__m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+		__m128d twoto = vec_set2dp(twotothe256, twotothe256);
 		
-		__m128d ex3v = _mm_load_pd(x3);		  
-		_mm_store_pd(x3, _mm_mul_pd(ex3v,twoto));		    		   		  
+		__m128d ex3v = vec_load2dpaligned(x3);		  
+		vec_store2dpto2dp(x3, vec_multiply2dp(ex3v,twoto));		    		   		  
 		
 		if(useFastScaling)
 		  addScale += wgt[i];
@@ -1443,41 +1443,41 @@
 	  le = &left[cptr[i] * 4];
 	  ri = &right[cptr[i] * 4];
 
-	  _mm_store_pd(x3, _mm_setzero_pd());	    
+	  vec_store2dpto2dp(x3, vec_zero2dp());	    
 	  
 	  for(l = 0; l < 2; l++)
 	    {		 		 						   		  		 		 
-	      __m128d al = _mm_mul_pd(_mm_load_pd(x1), _mm_load_pd(&le[l * 2]));
-	      __m128d ar = _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(&ri[l * 2]));
+	      __m128d al = vec_multiply2dp(vec_load2dpaligned(x1), vec_load2dpaligned(&le[l * 2]));
+	      __m128d ar = vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(&ri[l * 2]));
 	      
-	      al = _mm_hadd_pd(al, al);
-	      ar = _mm_hadd_pd(ar, ar);
+	      al = vec_horizontaladd2dp(al, al);
+	      ar = vec_horizontaladd2dp(ar, ar);
 	      
-	      al = _mm_mul_pd(al, ar);
+	      al = vec_multiply2dp(al, ar);
 	      
-	      __m128d vv  = _mm_load_pd(x3);
-	      __m128d EVV = _mm_load_pd(&EV[2 * l]);
+	      __m128d vv  = vec_load2dpaligned(x3);
+	      __m128d EVV = vec_load2dpaligned(&EV[2 * l]);
 	      
-	      vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+	      vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 	      
-	      _mm_store_pd(x3, vv);		     	  		   		  
+	      vec_store2dpto2dp(x3, vv);		     	  		   		  
 	    }	 	 	 	  
 
-	  __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	  __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	 
 	  scale = 1;
 	  	  
-	  __m128d v1 = _mm_and_pd(_mm_load_pd(x3), absMask.m);
-	  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	  if(_mm_movemask_pd( v1 ) != 3)
+	  __m128d v1 = vec_bitand2dp(vec_load2dpaligned(x3), absMask.m);
+	  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	  if(vec_extractupperbit2dp( v1 ) != 3)
 	    scale = 0;	  	         
 	 
 	  if(scale)
 	    {
-	      __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+	      __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	      	    
-	      __m128d ex3v = _mm_load_pd(x3);		  
-	      _mm_store_pd(x3, _mm_mul_pd(ex3v,twoto));		    		   		  
+	      __m128d ex3v = vec_load2dpaligned(x3);		  
+	      vec_store2dpto2dp(x3, vec_multiply2dp(ex3v,twoto));		    		   		  
 	     
 	      if(useFastScaling)
 		addScale += wgt[i];
@@ -1498,7 +1498,7 @@
 
 #endif
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 static void newviewGTRGAMMA_BINARY(int tipCase,
 				   double *x1_start, double *x2_start, double *x3_start,
 				   double *EV, double *tipVector,
@@ -1523,24 +1523,24 @@
 	   {	     	     	    
 	     x3 = &(x3_start[8 * i + 2 * k]);	     
 	    	         
-	     _mm_store_pd(x3, _mm_setzero_pd());	    
+	     vec_store2dpto2dp(x3, vec_zero2dp());	    
 	    	     
 	     for(l = 0; l < 2; l++)
 	       {		 		 						   		  		 		 
-		 __m128d al = _mm_mul_pd(_mm_load_pd(x1), _mm_load_pd(&left[k * 4 + l * 2]));
-		 __m128d ar = _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(&right[k * 4 + l * 2]));
+		 __m128d al = vec_multiply2dp(vec_load2dpaligned(x1), vec_load2dpaligned(&left[k * 4 + l * 2]));
+		 __m128d ar = vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(&right[k * 4 + l * 2]));
 		 		       
-		 al = _mm_hadd_pd(al, al);
-		 ar = _mm_hadd_pd(ar, ar);
+		 al = vec_horizontaladd2dp(al, al);
+		 ar = vec_horizontaladd2dp(ar, ar);
 		   
-		 al = _mm_mul_pd(al, ar);
+		 al = vec_multiply2dp(al, ar);
 		   
-		 __m128d vv  = _mm_load_pd(x3);
-		 __m128d EVV = _mm_load_pd(&EV[2 * l]);
+		 __m128d vv  = vec_load2dpaligned(x3);
+		 __m128d EVV = vec_load2dpaligned(&EV[2 * l]);
 		 
-		 vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+		 vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 		 
-		 _mm_store_pd(x3, vv);		     	  		   		  
+		 vec_store2dpto2dp(x3, vv);		     	  		   		  
 	       }	     	    
 	   }
        }
@@ -1555,48 +1555,48 @@
 	     x2 = &(x2_start[8 * i + 2 * k]);
 	     x3 = &(x3_start[8 * i + 2 * k]);	     
 	    	         
-	     _mm_store_pd(x3, _mm_setzero_pd());	    
+	     vec_store2dpto2dp(x3, vec_zero2dp());	    
 	    	     
 	     for(l = 0; l < 2; l++)
 	       {		 		 						   		  		 		 
-		 __m128d al = _mm_mul_pd(_mm_load_pd(x1), _mm_load_pd(&left[k * 4 + l * 2]));
-		 __m128d ar = _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(&right[k * 4 + l * 2]));
+		 __m128d al = vec_multiply2dp(vec_load2dpaligned(x1), vec_load2dpaligned(&left[k * 4 + l * 2]));
+		 __m128d ar = vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(&right[k * 4 + l * 2]));
 		 		       
-		 al = _mm_hadd_pd(al, al);
-		 ar = _mm_hadd_pd(ar, ar);
+		 al = vec_horizontaladd2dp(al, al);
+		 ar = vec_horizontaladd2dp(ar, ar);
 		   
-		 al = _mm_mul_pd(al, ar);
+		 al = vec_multiply2dp(al, ar);
 		   
-		 __m128d vv  = _mm_load_pd(x3);
-		 __m128d EVV = _mm_load_pd(&EV[2 * l]);
+		 __m128d vv  = vec_load2dpaligned(x3);
+		 __m128d EVV = vec_load2dpaligned(&EV[2 * l]);
 		 
-		 vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+		 vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 		 
-		 _mm_store_pd(x3, vv);		     	  		   		  
+		 vec_store2dpto2dp(x3, vv);		     	  		   		  
 	       }	     	    
 	   }
 	
 	 x3 = &(x3_start[8 * i]);
-	 __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	 __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	 
 	 scale = 1;
 	 for(l = 0; scale && (l < 8); l += 2)
 	   {
-	     __m128d vv = _mm_load_pd(&x3[l]);
-	     __m128d v1 = _mm_and_pd(vv, absMask.m);
-	     v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	     if(_mm_movemask_pd( v1 ) != 3)
+	     __m128d vv = vec_load2dpaligned(&x3[l]);
+	     __m128d v1 = vec_bitand2dp(vv, absMask.m);
+	     v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	     if(vec_extractupperbit2dp( v1 ) != 3)
 	       scale = 0;
 	   }	    	         
 	 
 	 if(scale)
 	   {
-	     __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+	     __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	     
 	     for(l = 0; l < 8; l+=2)
 	       {
-		 __m128d ex3v = _mm_load_pd(&x3[l]);		  
-		 _mm_store_pd(&x3[l], _mm_mul_pd(ex3v,twoto));	
+		 __m128d ex3v = vec_load2dpaligned(&x3[l]);		  
+		 vec_store2dpto2dp(&x3[l], vec_multiply2dp(ex3v,twoto));	
 	       }		   		  
 	     
 	     if(useFastScaling)
@@ -1615,48 +1615,48 @@
 	     x2 = &(x2_start[8 * i + 2 * k]);
 	     x3 = &(x3_start[8 * i + 2 * k]);	     
 	    	         
-	     _mm_store_pd(x3, _mm_setzero_pd());	    
+	     vec_store2dpto2dp(x3, vec_zero2dp());	    
 	    	     
 	     for(l = 0; l < 2; l++)
 	       {		 		 						   		  		 		 
-		 __m128d al = _mm_mul_pd(_mm_load_pd(x1), _mm_load_pd(&left[k * 4 + l * 2]));
-		 __m128d ar = _mm_mul_pd(_mm_load_pd(x2), _mm_load_pd(&right[k * 4 + l * 2]));
+		 __m128d al = vec_multiply2dp(vec_load2dpaligned(x1), vec_load2dpaligned(&left[k * 4 + l * 2]));
+		 __m128d ar = vec_multiply2dp(vec_load2dpaligned(x2), vec_load2dpaligned(&right[k * 4 + l * 2]));
 		 		       
-		 al = _mm_hadd_pd(al, al);
-		 ar = _mm_hadd_pd(ar, ar);
+		 al = vec_horizontaladd2dp(al, al);
+		 ar = vec_horizontaladd2dp(ar, ar);
 		   
-		 al = _mm_mul_pd(al, ar);
+		 al = vec_multiply2dp(al, ar);
 		   
-		 __m128d vv  = _mm_load_pd(x3);
-		 __m128d EVV = _mm_load_pd(&EV[2 * l]);
+		 __m128d vv  = vec_load2dpaligned(x3);
+		 __m128d EVV = vec_load2dpaligned(&EV[2 * l]);
 		 
-		 vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+		 vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 		 
-		 _mm_store_pd(x3, vv);		     	  		   		  
+		 vec_store2dpto2dp(x3, vv);		     	  		   		  
 	       }	     	    
 	   }
 	
 	 x3 = &(x3_start[8 * i]);
-	 __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	 __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	 
 	 scale = 1;
 	 for(l = 0; scale && (l < 8); l += 2)
 	   {
-	     __m128d vv = _mm_load_pd(&x3[l]);
-	     __m128d v1 = _mm_and_pd(vv, absMask.m);
-	     v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	     if(_mm_movemask_pd( v1 ) != 3)
+	     __m128d vv = vec_load2dpaligned(&x3[l]);
+	     __m128d v1 = vec_bitand2dp(vv, absMask.m);
+	     v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	     if(vec_extractupperbit2dp( v1 ) != 3)
 	       scale = 0;
 	   }	    	         
 	 
 	 if(scale)
 	   {
-	     __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+	     __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	     
 	     for(l = 0; l < 8; l+=2)
 	       {
-		 __m128d ex3v = _mm_load_pd(&x3[l]);		  
-		 _mm_store_pd(&x3[l], _mm_mul_pd(ex3v,twoto));	
+		 __m128d ex3v = vec_load2dpaligned(&x3[l]);		  
+		 vec_store2dpto2dp(&x3[l], vec_multiply2dp(ex3v,twoto));	
 	       }		   		  
 	     
 	     if(useFastScaling)
@@ -1856,7 +1856,7 @@
 
 
 
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
 
 static void newviewGTRCAT( int tipCase,  double *EV,  int *cptr,
 			   double *x1_start,  double *x2_start,  double *x3_start,  double *tipVector,
@@ -2035,8 +2035,8 @@
     addScale = 0;
 
   __m128d
-    minlikelihood_sse = _mm_set1_pd( minlikelihood ),
-    sc = _mm_set1_pd(twotothe256),
+    minlikelihood_sse = vec_splat2dp( minlikelihood ),
+    sc = vec_splat2dp(twotothe256),
     EVV[8];  
 
   for(i = 0; i < 4; i++)
@@ -2044,7 +2044,7 @@
       EV_t[4 * j + i] = EV[4 * i + j];
 
   for(i = 0; i < 8; i++)
-    EVV[i] = _mm_load_pd(&EV_t[i * 2]);
+    EVV[i] = vec_load2dpaligned(&EV_t[i * 2]);
 
   {
     x1 = x1_gapColumn;	      
@@ -2054,72 +2054,72 @@
     le =  &left[maxCats * 16];	     	 
     ri =  &right[maxCats * 16];		   	  	  	  	         
 
-    __m128d x1_0 = _mm_load_pd( &x1[0] );
-    __m128d x1_2 = _mm_load_pd( &x1[2] );
+    __m128d x1_0 = vec_load2dpaligned( &x1[0] );
+    __m128d x1_2 = vec_load2dpaligned( &x1[2] );
 
-    __m128d left_k0_0 = _mm_load_pd( &le[0] );
-    __m128d left_k0_2 = _mm_load_pd( &le[2] );
-    __m128d left_k1_0 = _mm_load_pd( &le[4] );
-    __m128d left_k1_2 = _mm_load_pd( &le[6] );
-    __m128d left_k2_0 = _mm_load_pd( &le[8] );
-    __m128d left_k2_2 = _mm_load_pd( &le[10] );
-    __m128d left_k3_0 = _mm_load_pd( &le[12] );
-    __m128d left_k3_2 = _mm_load_pd( &le[14] );
+    __m128d left_k0_0 = vec_load2dpaligned( &le[0] );
+    __m128d left_k0_2 = vec_load2dpaligned( &le[2] );
+    __m128d left_k1_0 = vec_load2dpaligned( &le[4] );
+    __m128d left_k1_2 = vec_load2dpaligned( &le[6] );
+    __m128d left_k2_0 = vec_load2dpaligned( &le[8] );
+    __m128d left_k2_2 = vec_load2dpaligned( &le[10] );
+    __m128d left_k3_0 = vec_load2dpaligned( &le[12] );
+    __m128d left_k3_2 = vec_load2dpaligned( &le[14] );
 
-    left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-    left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+    left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+    left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 
-    left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-    left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+    left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+    left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 
-    left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-    left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-    left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+    left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+    left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+    left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 
-    left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-    left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+    left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+    left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 
-    left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-    left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+    left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+    left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 
-    left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-    left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-    left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+    left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+    left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+    left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 
-    __m128d x2_0 = _mm_load_pd( &x2[0] );
-    __m128d x2_2 = _mm_load_pd( &x2[2] );
+    __m128d x2_0 = vec_load2dpaligned( &x2[0] );
+    __m128d x2_2 = vec_load2dpaligned( &x2[2] );
 
-    __m128d right_k0_0 = _mm_load_pd( &ri[0] );
-    __m128d right_k0_2 = _mm_load_pd( &ri[2] );
-    __m128d right_k1_0 = _mm_load_pd( &ri[4] );
-    __m128d right_k1_2 = _mm_load_pd( &ri[6] );
-    __m128d right_k2_0 = _mm_load_pd( &ri[8] );
-    __m128d right_k2_2 = _mm_load_pd( &ri[10] );
-    __m128d right_k3_0 = _mm_load_pd( &ri[12] );
-    __m128d right_k3_2 = _mm_load_pd( &ri[14] );
+    __m128d right_k0_0 = vec_load2dpaligned( &ri[0] );
+    __m128d right_k0_2 = vec_load2dpaligned( &ri[2] );
+    __m128d right_k1_0 = vec_load2dpaligned( &ri[4] );
+    __m128d right_k1_2 = vec_load2dpaligned( &ri[6] );
+    __m128d right_k2_0 = vec_load2dpaligned( &ri[8] );
+    __m128d right_k2_2 = vec_load2dpaligned( &ri[10] );
+    __m128d right_k3_0 = vec_load2dpaligned( &ri[12] );
+    __m128d right_k3_2 = vec_load2dpaligned( &ri[14] );
 
-    right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-    right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+    right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+    right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 
-    right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-    right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+    right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+    right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 
-    right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-    right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-    right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+    right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+    right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+    right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 
-    right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-    right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+    right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+    right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 
-    right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-    right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+    right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+    right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 
-    right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-    right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-    right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+    right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+    right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+    right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 
-    __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-    __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );
+    __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+    __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );
 
     __m128d EV_t_l0_k0 = EVV[0];
     __m128d EV_t_l0_k2 = EVV[1];
@@ -2130,61 +2130,61 @@
     __m128d EV_t_l3_k0 = EVV[6];
     __m128d EV_t_l3_k2 = EVV[7];
 
-    EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-    EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+    EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+    EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 
-    EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-    EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+    EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+    EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 
-    EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+    EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 
-    EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-    EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+    EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+    EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 
-    EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-    EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-    EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+    EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+    EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+    EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 
-    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  
+    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  
     
     if(tipCase != TIP_TIP)
       {    
 	scale = 1;
 	
-	__m128d v1 = _mm_and_pd(EV_t_l0_k0, absMask.m);
+	__m128d v1 = vec_bitand2dp(EV_t_l0_k0, absMask.m);
 	
-	v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
+	v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
 	
-	if(_mm_movemask_pd( v1 ) != 3)
+	if(vec_extractupperbit2dp( v1 ) != 3)
 	  scale = 0;
 	else
 	  {
-	    v1 = _mm_and_pd(EV_t_l2_k0, absMask.m);
-	    v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	    if(_mm_movemask_pd( v1 ) != 3)
+	    v1 = vec_bitand2dp(EV_t_l2_k0, absMask.m);
+	    v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	    if(vec_extractupperbit2dp( v1 ) != 3)
 	      scale = 0;
 	  }
 
       if(scale)
 	{		      
-	  _mm_store_pd(&x3[0], _mm_mul_pd(EV_t_l0_k0, sc));
-	  _mm_store_pd(&x3[2], _mm_mul_pd(EV_t_l2_k0, sc));	      	      
+	  vec_store2dpto2dp(&x3[0], vec_multiply2dp(EV_t_l0_k0, sc));
+	  vec_store2dpto2dp(&x3[2], vec_multiply2dp(EV_t_l2_k0, sc));	      	      
 	  
 	  scaleGap = TRUE;	   
 	}	
       else
 	{
-	  _mm_store_pd(x3, EV_t_l0_k0);
-	  _mm_store_pd(&x3[2], EV_t_l2_k0);
+	  vec_store2dpto2dp(x3, EV_t_l0_k0);
+	  vec_store2dpto2dp(&x3[2], EV_t_l2_k0);
 	}
       }
     else
       {
-	_mm_store_pd(x3, EV_t_l0_k0);
-	_mm_store_pd(&x3[2], EV_t_l2_k0);
+	vec_store2dpto2dp(x3, EV_t_l0_k0);
+	vec_store2dpto2dp(&x3[2], EV_t_l2_k0);
       }
   }
   
@@ -2210,72 +2210,72 @@
           else	 	  
             ri =  &right[cptr[i] * 16];
 
-          __m128d x1_0 = _mm_load_pd( &x1[0] );
-          __m128d x1_2 = _mm_load_pd( &x1[2] );
+          __m128d x1_0 = vec_load2dpaligned( &x1[0] );
+          __m128d x1_2 = vec_load2dpaligned( &x1[2] );
 
-          __m128d left_k0_0 = _mm_load_pd( &le[0] );
-          __m128d left_k0_2 = _mm_load_pd( &le[2] );
-          __m128d left_k1_0 = _mm_load_pd( &le[4] );
-          __m128d left_k1_2 = _mm_load_pd( &le[6] );
-          __m128d left_k2_0 = _mm_load_pd( &le[8] );
-          __m128d left_k2_2 = _mm_load_pd( &le[10] );
-          __m128d left_k3_0 = _mm_load_pd( &le[12] );
-          __m128d left_k3_2 = _mm_load_pd( &le[14] );
+          __m128d left_k0_0 = vec_load2dpaligned( &le[0] );
+          __m128d left_k0_2 = vec_load2dpaligned( &le[2] );
+          __m128d left_k1_0 = vec_load2dpaligned( &le[4] );
+          __m128d left_k1_2 = vec_load2dpaligned( &le[6] );
+          __m128d left_k2_0 = vec_load2dpaligned( &le[8] );
+          __m128d left_k2_2 = vec_load2dpaligned( &le[10] );
+          __m128d left_k3_0 = vec_load2dpaligned( &le[12] );
+          __m128d left_k3_2 = vec_load2dpaligned( &le[14] );
 
-          left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-          left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+          left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+          left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 
-          left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-          left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+          left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+          left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 
-          left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-          left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-          left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+          left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+          left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+          left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 
-          left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-          left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+          left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+          left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 
-          left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-          left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+          left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+          left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 
-          left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-          left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-          left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+          left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+          left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+          left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 
-          __m128d x2_0 = _mm_load_pd( &x2[0] );
-          __m128d x2_2 = _mm_load_pd( &x2[2] );
+          __m128d x2_0 = vec_load2dpaligned( &x2[0] );
+          __m128d x2_2 = vec_load2dpaligned( &x2[2] );
 
-          __m128d right_k0_0 = _mm_load_pd( &ri[0] );
-          __m128d right_k0_2 = _mm_load_pd( &ri[2] );
-          __m128d right_k1_0 = _mm_load_pd( &ri[4] );
-          __m128d right_k1_2 = _mm_load_pd( &ri[6] );
-          __m128d right_k2_0 = _mm_load_pd( &ri[8] );
-          __m128d right_k2_2 = _mm_load_pd( &ri[10] );
-          __m128d right_k3_0 = _mm_load_pd( &ri[12] );
-          __m128d right_k3_2 = _mm_load_pd( &ri[14] );
+          __m128d right_k0_0 = vec_load2dpaligned( &ri[0] );
+          __m128d right_k0_2 = vec_load2dpaligned( &ri[2] );
+          __m128d right_k1_0 = vec_load2dpaligned( &ri[4] );
+          __m128d right_k1_2 = vec_load2dpaligned( &ri[6] );
+          __m128d right_k2_0 = vec_load2dpaligned( &ri[8] );
+          __m128d right_k2_2 = vec_load2dpaligned( &ri[10] );
+          __m128d right_k3_0 = vec_load2dpaligned( &ri[12] );
+          __m128d right_k3_2 = vec_load2dpaligned( &ri[14] );
 
-          right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-          right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+          right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+          right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 
-          right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-          right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+          right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+          right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 
-          right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-          right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-          right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+          right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+          right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+          right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 
-          right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-          right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+          right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+          right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 
-          right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-          right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+          right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+          right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 
-          right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-          right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-          right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+          right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+          right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+          right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 
-          __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-          __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );	  	  
+          __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+          __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );	  	  
 
           __m128d EV_t_l0_k0 = EVV[0];
           __m128d EV_t_l0_k2 = EVV[1];
@@ -2286,28 +2286,28 @@
           __m128d EV_t_l3_k0 = EVV[6];
           __m128d EV_t_l3_k2 = EVV[7];
 
-          EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-          EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-          EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+          EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+          EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+          EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 
-          EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-          EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+          EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+          EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 
-          EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-          EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+          EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+          EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 
-          EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-          EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-          EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+          EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+          EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+          EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 
-          EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-          EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-          EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+          EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+          EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+          EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 
-          EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );	 
+          EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );	 
 
-          _mm_store_pd(x3, EV_t_l0_k0);
-          _mm_store_pd(&x3[2], EV_t_l2_k0);	  	 	   	    
+          vec_store2dpto2dp(x3, EV_t_l0_k0);
+          vec_store2dpto2dp(&x3[2], EV_t_l2_k0);	  	 	   	    
 
           x3_ptr += 4;
         }
@@ -2350,72 +2350,72 @@
             x2_ptr += 4;
           }	  	  	  	  
 
-          __m128d x1_0 = _mm_load_pd( &x1[0] );
-          __m128d x1_2 = _mm_load_pd( &x1[2] );
+          __m128d x1_0 = vec_load2dpaligned( &x1[0] );
+          __m128d x1_2 = vec_load2dpaligned( &x1[2] );
 
-          __m128d left_k0_0 = _mm_load_pd( &le[0] );
-          __m128d left_k0_2 = _mm_load_pd( &le[2] );
-          __m128d left_k1_0 = _mm_load_pd( &le[4] );
-          __m128d left_k1_2 = _mm_load_pd( &le[6] );
-          __m128d left_k2_0 = _mm_load_pd( &le[8] );
-          __m128d left_k2_2 = _mm_load_pd( &le[10] );
-          __m128d left_k3_0 = _mm_load_pd( &le[12] );
-          __m128d left_k3_2 = _mm_load_pd( &le[14] );
+          __m128d left_k0_0 = vec_load2dpaligned( &le[0] );
+          __m128d left_k0_2 = vec_load2dpaligned( &le[2] );
+          __m128d left_k1_0 = vec_load2dpaligned( &le[4] );
+          __m128d left_k1_2 = vec_load2dpaligned( &le[6] );
+          __m128d left_k2_0 = vec_load2dpaligned( &le[8] );
+          __m128d left_k2_2 = vec_load2dpaligned( &le[10] );
+          __m128d left_k3_0 = vec_load2dpaligned( &le[12] );
+          __m128d left_k3_2 = vec_load2dpaligned( &le[14] );
 
-          left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-          left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+          left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+          left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 
-          left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-          left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+          left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+          left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 
-          left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-          left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-          left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+          left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+          left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+          left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 
-          left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-          left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+          left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+          left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 
-          left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-          left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+          left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+          left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 
-          left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-          left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-          left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+          left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+          left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+          left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 
-          __m128d x2_0 = _mm_load_pd( &x2[0] );
-          __m128d x2_2 = _mm_load_pd( &x2[2] );
+          __m128d x2_0 = vec_load2dpaligned( &x2[0] );
+          __m128d x2_2 = vec_load2dpaligned( &x2[2] );
 
-          __m128d right_k0_0 = _mm_load_pd( &ri[0] );
-          __m128d right_k0_2 = _mm_load_pd( &ri[2] );
-          __m128d right_k1_0 = _mm_load_pd( &ri[4] );
-          __m128d right_k1_2 = _mm_load_pd( &ri[6] );
-          __m128d right_k2_0 = _mm_load_pd( &ri[8] );
-          __m128d right_k2_2 = _mm_load_pd( &ri[10] );
-          __m128d right_k3_0 = _mm_load_pd( &ri[12] );
-          __m128d right_k3_2 = _mm_load_pd( &ri[14] );
+          __m128d right_k0_0 = vec_load2dpaligned( &ri[0] );
+          __m128d right_k0_2 = vec_load2dpaligned( &ri[2] );
+          __m128d right_k1_0 = vec_load2dpaligned( &ri[4] );
+          __m128d right_k1_2 = vec_load2dpaligned( &ri[6] );
+          __m128d right_k2_0 = vec_load2dpaligned( &ri[8] );
+          __m128d right_k2_2 = vec_load2dpaligned( &ri[10] );
+          __m128d right_k3_0 = vec_load2dpaligned( &ri[12] );
+          __m128d right_k3_2 = vec_load2dpaligned( &ri[14] );
 
-          right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-          right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+          right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+          right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 
-          right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-          right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+          right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+          right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 
-          right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-          right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-          right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+          right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+          right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+          right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 
-          right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-          right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+          right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+          right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 
-          right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-          right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+          right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+          right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 
-          right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-          right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-          right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+          right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+          right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+          right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 
-          __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-          __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );
+          __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+          __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );
 
           __m128d EV_t_l0_k0 = EVV[0];
           __m128d EV_t_l0_k2 = EVV[1];
@@ -2427,44 +2427,44 @@
           __m128d EV_t_l3_k2 = EVV[7];
 
 
-          EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-          EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-          EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+          EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+          EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+          EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 
-          EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-          EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+          EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+          EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 
-          EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-          EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+          EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+          EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 
-          EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-          EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-          EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+          EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+          EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+          EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 
-          EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-          EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-          EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+          EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+          EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+          EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 
-          EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  
+          EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  
 
           scale = 1;
 
-          __m128d v1 = _mm_and_pd(EV_t_l0_k0, absMask.m);
-          v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-          if(_mm_movemask_pd( v1 ) != 3)
+          __m128d v1 = vec_bitand2dp(EV_t_l0_k0, absMask.m);
+          v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+          if(vec_extractupperbit2dp( v1 ) != 3)
             scale = 0;
           else
           {
-            v1 = _mm_and_pd(EV_t_l2_k0, absMask.m);
-            v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-            if(_mm_movemask_pd( v1 ) != 3)
+            v1 = vec_bitand2dp(EV_t_l2_k0, absMask.m);
+            v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+            if(vec_extractupperbit2dp( v1 ) != 3)
               scale = 0;
           }
 
           if(scale)
           {		      
-            _mm_store_pd(&x3[0], _mm_mul_pd(EV_t_l0_k0, sc));
-            _mm_store_pd(&x3[2], _mm_mul_pd(EV_t_l2_k0, sc));	      	      
+            vec_store2dpto2dp(&x3[0], vec_multiply2dp(EV_t_l0_k0, sc));
+            vec_store2dpto2dp(&x3[2], vec_multiply2dp(EV_t_l2_k0, sc));	      	      
 
 	    if(useFastScaling)
 	      addScale += wgt[i];
@@ -2473,8 +2473,8 @@
           }	
           else
           {
-            _mm_store_pd(x3, EV_t_l0_k0);
-            _mm_store_pd(&x3[2], EV_t_l2_k0);
+            vec_store2dpto2dp(x3, EV_t_l0_k0);
+            vec_store2dpto2dp(&x3[2], EV_t_l2_k0);
           }
 
           x3_ptr += 4;
@@ -2523,72 +2523,72 @@
 		  x2_ptr += 4;
 		}	 	  	  	  
 
-	      __m128d x1_0 = _mm_load_pd( &x1[0] );
-	      __m128d x1_2 = _mm_load_pd( &x1[2] );
+	      __m128d x1_0 = vec_load2dpaligned( &x1[0] );
+	      __m128d x1_2 = vec_load2dpaligned( &x1[2] );
 	      
-	      __m128d left_k0_0 = _mm_load_pd( &le[0] );
-	      __m128d left_k0_2 = _mm_load_pd( &le[2] );
-	      __m128d left_k1_0 = _mm_load_pd( &le[4] );
-	      __m128d left_k1_2 = _mm_load_pd( &le[6] );
-	      __m128d left_k2_0 = _mm_load_pd( &le[8] );
-	      __m128d left_k2_2 = _mm_load_pd( &le[10] );
-	      __m128d left_k3_0 = _mm_load_pd( &le[12] );
-	      __m128d left_k3_2 = _mm_load_pd( &le[14] );
+	      __m128d left_k0_0 = vec_load2dpaligned( &le[0] );
+	      __m128d left_k0_2 = vec_load2dpaligned( &le[2] );
+	      __m128d left_k1_0 = vec_load2dpaligned( &le[4] );
+	      __m128d left_k1_2 = vec_load2dpaligned( &le[6] );
+	      __m128d left_k2_0 = vec_load2dpaligned( &le[8] );
+	      __m128d left_k2_2 = vec_load2dpaligned( &le[10] );
+	      __m128d left_k3_0 = vec_load2dpaligned( &le[12] );
+	      __m128d left_k3_2 = vec_load2dpaligned( &le[14] );
 	      
-	      left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-	      left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+	      left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+	      left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 	      
-	      left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-	      left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+	      left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+	      left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 	      
-	      left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-	      left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-	      left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+	      left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+	      left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+	      left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 	      
-	      left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-	      left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+	      left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+	      left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 	      
-	      left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-	      left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+	      left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+	      left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 	      
-	      left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-	      left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-	      left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+	      left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+	      left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+	      left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 	      
-	      __m128d x2_0 = _mm_load_pd( &x2[0] );
-	      __m128d x2_2 = _mm_load_pd( &x2[2] );
+	      __m128d x2_0 = vec_load2dpaligned( &x2[0] );
+	      __m128d x2_2 = vec_load2dpaligned( &x2[2] );
 	      
-	      __m128d right_k0_0 = _mm_load_pd( &ri[0] );
-	      __m128d right_k0_2 = _mm_load_pd( &ri[2] );
-	      __m128d right_k1_0 = _mm_load_pd( &ri[4] );
-	      __m128d right_k1_2 = _mm_load_pd( &ri[6] );
-	      __m128d right_k2_0 = _mm_load_pd( &ri[8] );
-	      __m128d right_k2_2 = _mm_load_pd( &ri[10] );
-	      __m128d right_k3_0 = _mm_load_pd( &ri[12] );
-	      __m128d right_k3_2 = _mm_load_pd( &ri[14] );
+	      __m128d right_k0_0 = vec_load2dpaligned( &ri[0] );
+	      __m128d right_k0_2 = vec_load2dpaligned( &ri[2] );
+	      __m128d right_k1_0 = vec_load2dpaligned( &ri[4] );
+	      __m128d right_k1_2 = vec_load2dpaligned( &ri[6] );
+	      __m128d right_k2_0 = vec_load2dpaligned( &ri[8] );
+	      __m128d right_k2_2 = vec_load2dpaligned( &ri[10] );
+	      __m128d right_k3_0 = vec_load2dpaligned( &ri[12] );
+	      __m128d right_k3_2 = vec_load2dpaligned( &ri[14] );
 	      
-	      right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-	      right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+	      right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+	      right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 	      
-	      right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-	      right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+	      right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+	      right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 	      
-	      right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-	      right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-	      right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+	      right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+	      right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+	      right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 	      
-	      right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-	      right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+	      right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+	      right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 	      
-	      right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-	      right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+	      right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+	      right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 	      
-	      right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-	      right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-	      right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+	      right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+	      right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+	      right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 	      
-	      __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-	      __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );
+	      __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+	      __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );
 	      
 	      __m128d EV_t_l0_k0 = EVV[0];
 	      __m128d EV_t_l0_k2 = EVV[1];
@@ -2599,44 +2599,44 @@
 	      __m128d EV_t_l3_k0 = EVV[6];
 	      __m128d EV_t_l3_k2 = EVV[7];
 
-	      EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	      EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	      EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	      EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	      EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	      EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 	      
-	      EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	      EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	      EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	      EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 	      
-	      EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	      EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	      EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	      EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 	      
-	      EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	      EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	      EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	      EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	      EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	      EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 	      
-	      EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-	      EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-	      EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	      EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+	      EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+	      EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 	      
-	      EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  	 
+	      EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  	 
 	      
 	      scale = 1;
 	      
-	      __m128d v1 = _mm_and_pd(EV_t_l0_k0, absMask.m);
-	      v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	      if(_mm_movemask_pd( v1 ) != 3)
+	      __m128d v1 = vec_bitand2dp(EV_t_l0_k0, absMask.m);
+	      v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	      if(vec_extractupperbit2dp( v1 ) != 3)
 		scale = 0;
 	      else
 		{
-		  v1 = _mm_and_pd(EV_t_l2_k0, absMask.m);
-		  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-		  if(_mm_movemask_pd( v1 ) != 3)
+		  v1 = vec_bitand2dp(EV_t_l2_k0, absMask.m);
+		  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+		  if(vec_extractupperbit2dp( v1 ) != 3)
 		    scale = 0;
 		}
 
 	      if(scale)
 		{		      
-		  EV_t_l0_k0 = _mm_mul_pd(EV_t_l0_k0, sc);
-		  EV_t_l2_k0 = _mm_mul_pd(EV_t_l2_k0, sc);	      	      
+		  EV_t_l0_k0 = vec_multiply2dp(EV_t_l0_k0, sc);
+		  EV_t_l2_k0 = vec_multiply2dp(EV_t_l2_k0, sc);	      	      
 		  
 		  if(useFastScaling)
 		    addScale += wgt[i];
@@ -2644,8 +2644,8 @@
 		    ex3[i] += 1;            
 		}	
 
-	      _mm_store_pd(&x3[0], EV_t_l0_k0);
-	      _mm_store_pd(&x3[2], EV_t_l2_k0);
+	      vec_store2dpto2dp(&x3[0], EV_t_l0_k0);
+	      vec_store2dpto2dp(&x3[2], EV_t_l2_k0);
 	      
 	      x3_ptr += 4;
 	    }
@@ -2680,8 +2680,8 @@
     addScale = 0;
    
   __m128d
-    minlikelihood_sse = _mm_set1_pd( minlikelihood ),
-    sc = _mm_set1_pd(twotothe256),
+    minlikelihood_sse = vec_splat2dp( minlikelihood ),
+    sc = vec_splat2dp(twotothe256),
     EVV[8];  
   
   for(i = 0; i < 4; i++)
@@ -2689,7 +2689,7 @@
       EV_t[4 * j + i] = EV[4 * i + j];
   
   for(i = 0; i < 8; i++)
-    EVV[i] = _mm_load_pd(&EV_t[i * 2]);
+    EVV[i] = vec_load2dpaligned(&EV_t[i * 2]);
   
   switch(tipCase)
     {
@@ -2704,72 +2704,72 @@
 	  le =  &left[cptr[i] * 16];
 	  ri =  &right[cptr[i] * 16];
 	  
-	  __m128d x1_0 = _mm_load_pd( &x1[0] );
-	  __m128d x1_2 = _mm_load_pd( &x1[2] );
+	  __m128d x1_0 = vec_load2dpaligned( &x1[0] );
+	  __m128d x1_2 = vec_load2dpaligned( &x1[2] );
 	  
-	  __m128d left_k0_0 = _mm_load_pd( &le[0] );
-	  __m128d left_k0_2 = _mm_load_pd( &le[2] );
-	  __m128d left_k1_0 = _mm_load_pd( &le[4] );
-	  __m128d left_k1_2 = _mm_load_pd( &le[6] );
-	  __m128d left_k2_0 = _mm_load_pd( &le[8] );
-	  __m128d left_k2_2 = _mm_load_pd( &le[10] );
-	  __m128d left_k3_0 = _mm_load_pd( &le[12] );
-	  __m128d left_k3_2 = _mm_load_pd( &le[14] );
+	  __m128d left_k0_0 = vec_load2dpaligned( &le[0] );
+	  __m128d left_k0_2 = vec_load2dpaligned( &le[2] );
+	  __m128d left_k1_0 = vec_load2dpaligned( &le[4] );
+	  __m128d left_k1_2 = vec_load2dpaligned( &le[6] );
+	  __m128d left_k2_0 = vec_load2dpaligned( &le[8] );
+	  __m128d left_k2_2 = vec_load2dpaligned( &le[10] );
+	  __m128d left_k3_0 = vec_load2dpaligned( &le[12] );
+	  __m128d left_k3_2 = vec_load2dpaligned( &le[14] );
 	  
-	  left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-	  left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+	  left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+	  left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 	  
-	  left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-	  left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+	  left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+	  left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 	  
-	  left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-	  left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-	  left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+	  left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+	  left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+	  left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 	  
-	  left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-	  left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+	  left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+	  left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 	  
-	  left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-	  left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+	  left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+	  left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 	  
-	  left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-	  left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-	  left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+	  left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+	  left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+	  left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 	  
-	  __m128d x2_0 = _mm_load_pd( &x2[0] );
-	  __m128d x2_2 = _mm_load_pd( &x2[2] );
+	  __m128d x2_0 = vec_load2dpaligned( &x2[0] );
+	  __m128d x2_2 = vec_load2dpaligned( &x2[2] );
 	  
-	  __m128d right_k0_0 = _mm_load_pd( &ri[0] );
-	  __m128d right_k0_2 = _mm_load_pd( &ri[2] );
-	  __m128d right_k1_0 = _mm_load_pd( &ri[4] );
-	  __m128d right_k1_2 = _mm_load_pd( &ri[6] );
-	  __m128d right_k2_0 = _mm_load_pd( &ri[8] );
-	  __m128d right_k2_2 = _mm_load_pd( &ri[10] );
-	  __m128d right_k3_0 = _mm_load_pd( &ri[12] );
-	  __m128d right_k3_2 = _mm_load_pd( &ri[14] );
+	  __m128d right_k0_0 = vec_load2dpaligned( &ri[0] );
+	  __m128d right_k0_2 = vec_load2dpaligned( &ri[2] );
+	  __m128d right_k1_0 = vec_load2dpaligned( &ri[4] );
+	  __m128d right_k1_2 = vec_load2dpaligned( &ri[6] );
+	  __m128d right_k2_0 = vec_load2dpaligned( &ri[8] );
+	  __m128d right_k2_2 = vec_load2dpaligned( &ri[10] );
+	  __m128d right_k3_0 = vec_load2dpaligned( &ri[12] );
+	  __m128d right_k3_2 = vec_load2dpaligned( &ri[14] );
 	  
-	  right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-	  right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+	  right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+	  right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 	  
-	  right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-	  right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+	  right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+	  right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 	  
-	  right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-	  right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-	  right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+	  right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+	  right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+	  right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 	  
-	  right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-	  right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+	  right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+	  right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 	  
-	  right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-	  right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+	  right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+	  right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 	  
-	  right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-	  right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-	  right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+	  right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+	  right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+	  right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 	  
-	  __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-	  __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );	  	  
+	  __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+	  __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );	  	  
 
 	  __m128d EV_t_l0_k0 = EVV[0];
 	  __m128d EV_t_l0_k2 = EVV[1];
@@ -2780,28 +2780,28 @@
 	  __m128d EV_t_l3_k0 = EVV[6];
 	  __m128d EV_t_l3_k2 = EVV[7];
 	  
-	  EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	  EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	  EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	  EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	  EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	  EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 	  
-	  EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	  EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	  EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	  EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 	  
-	  EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	  EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	  EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	  EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 	  
-	  EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	  EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	  EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	  EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	  EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	  EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 	  	  
-	  EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-	  EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-	  EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	  EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+	  EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+	  EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 	  
-	  EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );	 
+	  EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );	 
 	  	  
-	  _mm_store_pd(x3, EV_t_l0_k0);
-	  _mm_store_pd(&x3[2], EV_t_l2_k0);	  	 	   	    
+	  vec_store2dpto2dp(x3, EV_t_l0_k0);
+	  vec_store2dpto2dp(&x3[2], EV_t_l2_k0);	  	 	   	    
 	}
       break;
     case TIP_INNER:      
@@ -2814,72 +2814,72 @@
 	  le =  &left[cptr[i] * 16];
 	  ri =  &right[cptr[i] * 16];
 
-	  __m128d x1_0 = _mm_load_pd( &x1[0] );
-	  __m128d x1_2 = _mm_load_pd( &x1[2] );
+	  __m128d x1_0 = vec_load2dpaligned( &x1[0] );
+	  __m128d x1_2 = vec_load2dpaligned( &x1[2] );
 	  
-	  __m128d left_k0_0 = _mm_load_pd( &le[0] );
-	  __m128d left_k0_2 = _mm_load_pd( &le[2] );
-	  __m128d left_k1_0 = _mm_load_pd( &le[4] );
-	  __m128d left_k1_2 = _mm_load_pd( &le[6] );
-	  __m128d left_k2_0 = _mm_load_pd( &le[8] );
-	  __m128d left_k2_2 = _mm_load_pd( &le[10] );
-	  __m128d left_k3_0 = _mm_load_pd( &le[12] );
-	  __m128d left_k3_2 = _mm_load_pd( &le[14] );
+	  __m128d left_k0_0 = vec_load2dpaligned( &le[0] );
+	  __m128d left_k0_2 = vec_load2dpaligned( &le[2] );
+	  __m128d left_k1_0 = vec_load2dpaligned( &le[4] );
+	  __m128d left_k1_2 = vec_load2dpaligned( &le[6] );
+	  __m128d left_k2_0 = vec_load2dpaligned( &le[8] );
+	  __m128d left_k2_2 = vec_load2dpaligned( &le[10] );
+	  __m128d left_k3_0 = vec_load2dpaligned( &le[12] );
+	  __m128d left_k3_2 = vec_load2dpaligned( &le[14] );
 	  
-	  left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-	  left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+	  left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+	  left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 	  
-	  left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-	  left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+	  left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+	  left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 	  
-	  left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-	  left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-	  left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+	  left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+	  left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+	  left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 	  
-	  left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-	  left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+	  left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+	  left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 	  
-	  left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-	  left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+	  left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+	  left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 	  
-	  left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-	  left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-	  left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+	  left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+	  left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+	  left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 	  
-	  __m128d x2_0 = _mm_load_pd( &x2[0] );
-	  __m128d x2_2 = _mm_load_pd( &x2[2] );
+	  __m128d x2_0 = vec_load2dpaligned( &x2[0] );
+	  __m128d x2_2 = vec_load2dpaligned( &x2[2] );
 	  
-	  __m128d right_k0_0 = _mm_load_pd( &ri[0] );
-	  __m128d right_k0_2 = _mm_load_pd( &ri[2] );
-	  __m128d right_k1_0 = _mm_load_pd( &ri[4] );
-	  __m128d right_k1_2 = _mm_load_pd( &ri[6] );
-	  __m128d right_k2_0 = _mm_load_pd( &ri[8] );
-	  __m128d right_k2_2 = _mm_load_pd( &ri[10] );
-	  __m128d right_k3_0 = _mm_load_pd( &ri[12] );
-	  __m128d right_k3_2 = _mm_load_pd( &ri[14] );
+	  __m128d right_k0_0 = vec_load2dpaligned( &ri[0] );
+	  __m128d right_k0_2 = vec_load2dpaligned( &ri[2] );
+	  __m128d right_k1_0 = vec_load2dpaligned( &ri[4] );
+	  __m128d right_k1_2 = vec_load2dpaligned( &ri[6] );
+	  __m128d right_k2_0 = vec_load2dpaligned( &ri[8] );
+	  __m128d right_k2_2 = vec_load2dpaligned( &ri[10] );
+	  __m128d right_k3_0 = vec_load2dpaligned( &ri[12] );
+	  __m128d right_k3_2 = vec_load2dpaligned( &ri[14] );
 	  
-	  right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-	  right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+	  right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+	  right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 	  
-	  right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-	  right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+	  right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+	  right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 	  
-	  right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-	  right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-	  right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+	  right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+	  right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+	  right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 	  
-	  right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-	  right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+	  right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+	  right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 	  
-	  right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-	  right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+	  right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+	  right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 	  
-	  right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-	  right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-	  right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+	  right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+	  right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+	  right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 	  
-	  __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-	  __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );
+	  __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+	  __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );
 	  
 	  __m128d EV_t_l0_k0 = EVV[0];
 	  __m128d EV_t_l0_k2 = EVV[1];
@@ -2891,44 +2891,44 @@
 	  __m128d EV_t_l3_k2 = EVV[7];
 	 
 	  
-	  EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	  EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	  EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	  EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	  EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	  EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 	  
-	  EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	  EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	  EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	  EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 	  
-	  EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	  EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	  EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	  EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 	  
-	  EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	  EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	  EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	  EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	  EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	  EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 	  	  
-	  EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-	  EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-	  EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	  EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+	  EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+	  EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 	  
-	  EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  
+	  EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  
 	 
 	  scale = 1;
 	  	  	  	    
-	  __m128d v1 = _mm_and_pd(EV_t_l0_k0, absMask.m);
-	  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	  if(_mm_movemask_pd( v1 ) != 3)
+	  __m128d v1 = vec_bitand2dp(EV_t_l0_k0, absMask.m);
+	  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	  if(vec_extractupperbit2dp( v1 ) != 3)
 	    scale = 0;
 	  else
 	    {
-	      v1 = _mm_and_pd(EV_t_l2_k0, absMask.m);
-	      v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	      if(_mm_movemask_pd( v1 ) != 3)
+	      v1 = vec_bitand2dp(EV_t_l2_k0, absMask.m);
+	      v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	      if(vec_extractupperbit2dp( v1 ) != 3)
 		scale = 0;
 	    }
 	  	  
 	  if(scale)
 	    {		      
-	      _mm_store_pd(&x3[0], _mm_mul_pd(EV_t_l0_k0, sc));
-	      _mm_store_pd(&x3[2], _mm_mul_pd(EV_t_l2_k0, sc));	      	      
+	      vec_store2dpto2dp(&x3[0], vec_multiply2dp(EV_t_l0_k0, sc));
+	      vec_store2dpto2dp(&x3[2], vec_multiply2dp(EV_t_l2_k0, sc));	      	      
 	      
 	      if(useFastScaling)
 		addScale += wgt[i];
@@ -2937,8 +2937,8 @@
 	    }	
 	  else
 	    {
-	      _mm_store_pd(x3, EV_t_l0_k0);
-	      _mm_store_pd(&x3[2], EV_t_l2_k0);
+	      vec_store2dpto2dp(x3, EV_t_l0_k0);
+	      vec_store2dpto2dp(&x3[2], EV_t_l2_k0);
 	    }
 	 
 	  	  
@@ -2954,72 +2954,72 @@
 	  le =  &left[cptr[i] * 16];
 	  ri =  &right[cptr[i] * 16];
 
-	  __m128d x1_0 = _mm_load_pd( &x1[0] );
-	  __m128d x1_2 = _mm_load_pd( &x1[2] );
+	  __m128d x1_0 = vec_load2dpaligned( &x1[0] );
+	  __m128d x1_2 = vec_load2dpaligned( &x1[2] );
 	  
-	  __m128d left_k0_0 = _mm_load_pd( &le[0] );
-	  __m128d left_k0_2 = _mm_load_pd( &le[2] );
-	  __m128d left_k1_0 = _mm_load_pd( &le[4] );
-	  __m128d left_k1_2 = _mm_load_pd( &le[6] );
-	  __m128d left_k2_0 = _mm_load_pd( &le[8] );
-	  __m128d left_k2_2 = _mm_load_pd( &le[10] );
-	  __m128d left_k3_0 = _mm_load_pd( &le[12] );
-	  __m128d left_k3_2 = _mm_load_pd( &le[14] );
+	  __m128d left_k0_0 = vec_load2dpaligned( &le[0] );
+	  __m128d left_k0_2 = vec_load2dpaligned( &le[2] );
+	  __m128d left_k1_0 = vec_load2dpaligned( &le[4] );
+	  __m128d left_k1_2 = vec_load2dpaligned( &le[6] );
+	  __m128d left_k2_0 = vec_load2dpaligned( &le[8] );
+	  __m128d left_k2_2 = vec_load2dpaligned( &le[10] );
+	  __m128d left_k3_0 = vec_load2dpaligned( &le[12] );
+	  __m128d left_k3_2 = vec_load2dpaligned( &le[14] );
 	  
-	  left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-	  left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+	  left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+	  left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 	  
-	  left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-	  left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+	  left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+	  left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 	  
-	  left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-	  left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-	  left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+	  left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+	  left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+	  left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 	  
-	  left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-	  left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+	  left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+	  left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 	  
-	  left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-	  left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+	  left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+	  left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 	  
-	  left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-	  left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-	  left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+	  left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+	  left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+	  left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 	  
-	  __m128d x2_0 = _mm_load_pd( &x2[0] );
-	  __m128d x2_2 = _mm_load_pd( &x2[2] );
+	  __m128d x2_0 = vec_load2dpaligned( &x2[0] );
+	  __m128d x2_2 = vec_load2dpaligned( &x2[2] );
 	  
-	  __m128d right_k0_0 = _mm_load_pd( &ri[0] );
-	  __m128d right_k0_2 = _mm_load_pd( &ri[2] );
-	  __m128d right_k1_0 = _mm_load_pd( &ri[4] );
-	  __m128d right_k1_2 = _mm_load_pd( &ri[6] );
-	  __m128d right_k2_0 = _mm_load_pd( &ri[8] );
-	  __m128d right_k2_2 = _mm_load_pd( &ri[10] );
-	  __m128d right_k3_0 = _mm_load_pd( &ri[12] );
-	  __m128d right_k3_2 = _mm_load_pd( &ri[14] );
+	  __m128d right_k0_0 = vec_load2dpaligned( &ri[0] );
+	  __m128d right_k0_2 = vec_load2dpaligned( &ri[2] );
+	  __m128d right_k1_0 = vec_load2dpaligned( &ri[4] );
+	  __m128d right_k1_2 = vec_load2dpaligned( &ri[6] );
+	  __m128d right_k2_0 = vec_load2dpaligned( &ri[8] );
+	  __m128d right_k2_2 = vec_load2dpaligned( &ri[10] );
+	  __m128d right_k3_0 = vec_load2dpaligned( &ri[12] );
+	  __m128d right_k3_2 = vec_load2dpaligned( &ri[14] );
 	  
-	  right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-	  right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+	  right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+	  right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 	  
-	  right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-	  right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+	  right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+	  right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 	  
-	  right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-	  right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-	  right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+	  right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+	  right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+	  right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 	  
-	  right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-	  right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+	  right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+	  right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 	  
-	  right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-	  right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+	  right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+	  right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 	  
-	  right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-	  right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-	  right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+	  right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+	  right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+	  right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 	  
-	  __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-	  __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );
+	  __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+	  __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );
 	  
 	  __m128d EV_t_l0_k0 = EVV[0];
 	  __m128d EV_t_l0_k2 = EVV[1];
@@ -3031,44 +3031,44 @@
 	  __m128d EV_t_l3_k2 = EVV[7];
 	 
 	  
-	  EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	  EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	  EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	  EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	  EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	  EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 	  
-	  EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	  EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	  EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	  EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 	  
-	  EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	  EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	  EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	  EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 	  
-	  EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	  EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	  EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	  EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	  EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	  EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 	  	  
-	  EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-	  EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-	  EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	  EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+	  EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+	  EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 	  
-	  EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  	 
+	  EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );	  	 	    		  	 
 
 	  scale = 1;
 	  	  
-	  __m128d v1 = _mm_and_pd(EV_t_l0_k0, absMask.m);
-	  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	  if(_mm_movemask_pd( v1 ) != 3)
+	  __m128d v1 = vec_bitand2dp(EV_t_l0_k0, absMask.m);
+	  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	  if(vec_extractupperbit2dp( v1 ) != 3)
 	    scale = 0;
 	  else
 	    {
-	      v1 = _mm_and_pd(EV_t_l2_k0, absMask.m);
-	      v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	      if(_mm_movemask_pd( v1 ) != 3)
+	      v1 = vec_bitand2dp(EV_t_l2_k0, absMask.m);
+	      v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	      if(vec_extractupperbit2dp( v1 ) != 3)
 		scale = 0;
 	    }
 	  	  
 	  if(scale)
 	    {		      
-	      _mm_store_pd(&x3[0], _mm_mul_pd(EV_t_l0_k0, sc));
-	      _mm_store_pd(&x3[2], _mm_mul_pd(EV_t_l2_k0, sc));	      	      
+	      vec_store2dpto2dp(&x3[0], vec_multiply2dp(EV_t_l0_k0, sc));
+	      vec_store2dpto2dp(&x3[2], vec_multiply2dp(EV_t_l2_k0, sc));	      	      
 	      
 	      if(useFastScaling)
 		addScale += wgt[i];
@@ -3077,8 +3077,8 @@
 	    }	
 	  else
 	    {
-	      _mm_store_pd(x3, EV_t_l0_k0);
-	      _mm_store_pd(&x3[2], EV_t_l2_k0);
+	      vec_store2dpto2dp(x3, EV_t_l0_k0);
+	      vec_store2dpto2dp(&x3[2], EV_t_l2_k0);
 	    }
 	  	  
 	}
@@ -3099,7 +3099,7 @@
 
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 static void newviewGTRGAMMA(int tipCase,
 			    double *x1_start, double *x2_start, double *x3_start,
@@ -3132,7 +3132,7 @@
       EV_t[4 * l + k] = EV[4 * k + l];
 
   for(k = 0; k < 8; k++)
-    EVV[k] = _mm_load_pd(&EV_t[k * 2]);
+    EVV[k] = vec_load2dpaligned(&EV_t[k * 2]);
    
   switch(tipCase)
     {
@@ -3143,8 +3143,8 @@
 
 	for (i = 1; i < 16; i++)
 	  {
-	    __m128d x1_1 = _mm_load_pd(&(tipVector[i*4]));
-	    __m128d x1_2 = _mm_load_pd(&(tipVector[i*4 + 2]));	   
+	    __m128d x1_1 = vec_load2dpaligned(&(tipVector[i*4]));
+	    __m128d x1_2 = vec_load2dpaligned(&(tipVector[i*4 + 2]));	   
 
 
 	    if(mask32[i] & x1_presenceMap)
@@ -3152,16 +3152,16 @@
 		for (j = 0; j < 4; j++)
 		  for (k = 0; k < 4; k++)
 		    {		 
-		      __m128d left1 = _mm_load_pd(&left[j*16 + k*4]);
-		      __m128d left2 = _mm_load_pd(&left[j*16 + k*4 + 2]);
+		      __m128d left1 = vec_load2dpaligned(&left[j*16 + k*4]);
+		      __m128d left2 = vec_load2dpaligned(&left[j*16 + k*4 + 2]);
 		      
-		      __m128d acc = _mm_setzero_pd();
+		      __m128d acc = vec_zero2dp();
 		      
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left1, x1_1));
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left2, x1_2));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left1, x1_1));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left2, x1_2));
 		      
-		      acc = _mm_hadd_pd(acc, acc);
-		      _mm_storel_pd(&umpX1[i*16 + j*4 + k], acc);
+		      acc = vec_horizontaladd2dp(acc, acc);
+		      vec_storelower1dpof2dp(&umpX1[i*16 + j*4 + k], acc);
 		    }
 	      }
 
@@ -3171,16 +3171,16 @@
 		for (j = 0; j < 4; j++)
 		  for (k = 0; k < 4; k++)
 		    {
-		      __m128d left1 = _mm_load_pd(&right[j*16 + k*4]);
-		      __m128d left2 = _mm_load_pd(&right[j*16 + k*4 + 2]);
+		      __m128d left1 = vec_load2dpaligned(&right[j*16 + k*4]);
+		      __m128d left2 = vec_load2dpaligned(&right[j*16 + k*4 + 2]);
 		      
-		      __m128d acc = _mm_setzero_pd();
+		      __m128d acc = vec_zero2dp();
 		      
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left1, x1_1));
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left2, x1_2));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left1, x1_1));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left2, x1_2));
 		      
-		      acc = _mm_hadd_pd(acc, acc);
-		      _mm_storel_pd(&umpX2[i*16 + j*4 + k], acc);
+		      acc = vec_horizontaladd2dp(acc, acc);
+		      vec_storelower1dpof2dp(&umpX2[i*16 + j*4 + k], acc);
 		      
 		    }
 	      }
@@ -3196,19 +3196,19 @@
 	    
 	    for (j = 0; j < 4; j++)
 	       {				 		  		  		   
-		 __m128d uX1_k0_sse = _mm_load_pd( &uX1[j * 4] );
-		 __m128d uX1_k2_sse = _mm_load_pd( &uX1[j * 4 + 2] );
+		 __m128d uX1_k0_sse = vec_load2dpaligned( &uX1[j * 4] );
+		 __m128d uX1_k2_sse = vec_load2dpaligned( &uX1[j * 4 + 2] );
 		 				  
 		   
-		 __m128d uX2_k0_sse = _mm_load_pd( &uX2[j * 4] );
-		 __m128d uX2_k2_sse = _mm_load_pd( &uX2[j * 4 + 2] );
+		 __m128d uX2_k0_sse = vec_load2dpaligned( &uX2[j * 4] );
+		 __m128d uX2_k2_sse = vec_load2dpaligned( &uX2[j * 4 + 2] );
  		 
 
 		
 		 /* multiply left * right */		
 		 
-		 __m128d x1px2_k0 = _mm_mul_pd( uX1_k0_sse, uX2_k0_sse );
-		 __m128d x1px2_k2 = _mm_mul_pd( uX1_k2_sse, uX2_k2_sse );
+		 __m128d x1px2_k0 = vec_multiply2dp( uX1_k0_sse, uX2_k0_sse );
+		 __m128d x1px2_k2 = vec_multiply2dp( uX1_k2_sse, uX2_k2_sse );
 		 
 		 
 		 
@@ -3224,28 +3224,28 @@
 		 __m128d EV_t_l3_k0 = EVV[6]; 
 		 __m128d EV_t_l3_k2 = EVV[7];
 		 
-		 EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-		 EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-		 EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+		 EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+		 EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+		 EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 		 
-		 EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-		 EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+		 EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+		 EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 		 
-		 EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-		 EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+		 EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+		 EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 		 
-		 EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-		 EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-		 EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+		 EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+		 EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+		 EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 		 
-		 EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-		 EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-		 EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+		 EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+		 EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+		 EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 		 
-		 EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+		 EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 		 
-		 _mm_store_pd( &x3[j * 4 + 0], EV_t_l0_k0 );
-		 _mm_store_pd( &x3[j * 4 + 2], EV_t_l2_k0 );
+		 vec_store2dpto2dp( &x3[j * 4 + 0], EV_t_l0_k0 );
+		 vec_store2dpto2dp( &x3[j * 4 + 2], EV_t_l2_k0 );
 	       }
 	  }
       }
@@ -3259,29 +3259,29 @@
 	  {
 	    if(mask32[i] & x1_presenceMap)
 	      {
-		__m128d x1_1 = _mm_load_pd(&(tipVector[i*4]));
-		__m128d x1_2 = _mm_load_pd(&(tipVector[i*4 + 2]));	   
+		__m128d x1_1 = vec_load2dpaligned(&(tipVector[i*4]));
+		__m128d x1_2 = vec_load2dpaligned(&(tipVector[i*4 + 2]));	   
 		
 		for (j = 0; j < 4; j++)
 		  for (k = 0; k < 4; k++)
 		    {		 
-		      __m128d left1 = _mm_load_pd(&left[j*16 + k*4]);
-		      __m128d left2 = _mm_load_pd(&left[j*16 + k*4 + 2]);
+		      __m128d left1 = vec_load2dpaligned(&left[j*16 + k*4]);
+		      __m128d left2 = vec_load2dpaligned(&left[j*16 + k*4 + 2]);
 		      
-		      __m128d acc = _mm_setzero_pd();
+		      __m128d acc = vec_zero2dp();
 		      
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left1, x1_1));
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left2, x1_2));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left1, x1_1));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left2, x1_2));
 		      
-		      acc = _mm_hadd_pd(acc, acc);
-		      _mm_storel_pd(&umpX1[i*16 + j*4 + k], acc);		 
+		      acc = vec_horizontaladd2dp(acc, acc);
+		      vec_storelower1dpof2dp(&umpX1[i*16 + j*4 + k], acc);		 
 		    }
 	      }	   
 	  }
 
 	 for (i = 0; i < n; i++)
 	   {
-	     __m128d maxv =_mm_setzero_pd();
+	     __m128d maxv =vec_zero2dp();
 	     
 	     x2 = &x2_start[i * 16];
 	     x3 = &x3_start[i * 16];
@@ -3299,57 +3299,57 @@
 		 double *right_k1_p = &right[j*16 + 1*4];
 		 double *right_k2_p = &right[j*16 + 2*4];
 		 double *right_k3_p = &right[j*16 + 3*4];
-		 __m128d x2_0 = _mm_load_pd( &x2_p[0] );
-		 __m128d x2_2 = _mm_load_pd( &x2_p[2] );
+		 __m128d x2_0 = vec_load2dpaligned( &x2_p[0] );
+		 __m128d x2_2 = vec_load2dpaligned( &x2_p[2] );
 
-		 __m128d right_k0_0 = _mm_load_pd( &right_k0_p[0] );
-		 __m128d right_k0_2 = _mm_load_pd( &right_k0_p[2] );
-		 __m128d right_k1_0 = _mm_load_pd( &right_k1_p[0] );
-		 __m128d right_k1_2 = _mm_load_pd( &right_k1_p[2] );
-		 __m128d right_k2_0 = _mm_load_pd( &right_k2_p[0] );
-		 __m128d right_k2_2 = _mm_load_pd( &right_k2_p[2] );
-		 __m128d right_k3_0 = _mm_load_pd( &right_k3_p[0] );
-		 __m128d right_k3_2 = _mm_load_pd( &right_k3_p[2] );
+		 __m128d right_k0_0 = vec_load2dpaligned( &right_k0_p[0] );
+		 __m128d right_k0_2 = vec_load2dpaligned( &right_k0_p[2] );
+		 __m128d right_k1_0 = vec_load2dpaligned( &right_k1_p[0] );
+		 __m128d right_k1_2 = vec_load2dpaligned( &right_k1_p[2] );
+		 __m128d right_k2_0 = vec_load2dpaligned( &right_k2_p[0] );
+		 __m128d right_k2_2 = vec_load2dpaligned( &right_k2_p[2] );
+		 __m128d right_k3_0 = vec_load2dpaligned( &right_k3_p[0] );
+		 __m128d right_k3_2 = vec_load2dpaligned( &right_k3_p[2] );
 
 
 
-		 right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-		 right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+		 right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+		 right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 
-		 right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-		 right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+		 right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+		 right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 
-		 right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-		 right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-		 right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+		 right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+		 right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+		 right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 
 
-		 right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-		 right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+		 right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+		 right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 
 
-		 right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-		 right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+		 right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+		 right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 
-		 right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-		 right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-		 right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);
+		 right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+		 right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+		 right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);
 
 		 {
 		  
 		   /* load left side from tip vector */
 		  
 		   
-		   __m128d uX1_k0_sse = _mm_load_pd( &uX1[j * 4] );
-		   __m128d uX1_k2_sse = _mm_load_pd( &uX1[j * 4 + 2] );
+		   __m128d uX1_k0_sse = vec_load2dpaligned( &uX1[j * 4] );
+		   __m128d uX1_k2_sse = vec_load2dpaligned( &uX1[j * 4 + 2] );
 		 
 		 
 		   
 		   /* multiply left * right */
 		  
 		   
-		   __m128d x1px2_k0 = _mm_mul_pd( uX1_k0_sse, right_k0_0 );
-		   __m128d x1px2_k2 = _mm_mul_pd( uX1_k2_sse, right_k2_0 );
+		   __m128d x1px2_k0 = vec_multiply2dp( uX1_k0_sse, right_k0_0 );
+		   __m128d x1px2_k2 = vec_multiply2dp( uX1_k2_sse, right_k2_0 );
 		   
 		   
 		   
@@ -3365,51 +3365,51 @@
 		   __m128d EV_t_l3_k2 = EVV[7];
 
 		   
-		   EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-		   EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-		   EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+		   EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+		   EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+		   EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 		   
-		   EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-		   EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+		   EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+		   EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 		   
-		   EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-		   EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+		   EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+		   EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 		   
-		   EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-		   EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-		   EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+		   EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+		   EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+		   EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 		   		   
-		   EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-		   EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-		   EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+		   EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+		   EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+		   EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 		   
-		   EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+		   EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 		   
 		   values[j * 2]     = EV_t_l0_k0;
 		   values[j * 2 + 1] = EV_t_l2_k0;		   		   
 		   
-		   maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l0_k0, absMask.m));
-		   maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l2_k0, absMask.m));		   
+		   maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l0_k0, absMask.m));
+		   maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l2_k0, absMask.m));		   
 		 }
 	       }
 
 	     
-	     _mm_store_pd(maxima, maxv);
+	     vec_store2dpto2dp(maxima, maxv);
 
 	     max = MAX(maxima[0], maxima[1]);
 
 	     if(max < minlikelihood)
 	       {
-		 __m128d sv = _mm_set1_pd(twotothe256);
+		 __m128d sv = vec_splat2dp(twotothe256);
 	       		       	   	 	     
-		 _mm_store_pd(&x3[0], _mm_mul_pd(values[0], sv));	   
-		 _mm_store_pd(&x3[2], _mm_mul_pd(values[1], sv));
-		 _mm_store_pd(&x3[4], _mm_mul_pd(values[2], sv));
-		 _mm_store_pd(&x3[6], _mm_mul_pd(values[3], sv));
-		 _mm_store_pd(&x3[8], _mm_mul_pd(values[4], sv));	   
-		 _mm_store_pd(&x3[10], _mm_mul_pd(values[5], sv));
-		 _mm_store_pd(&x3[12], _mm_mul_pd(values[6], sv));
-		 _mm_store_pd(&x3[14], _mm_mul_pd(values[7], sv));	     
+		 vec_store2dpto2dp(&x3[0], vec_multiply2dp(values[0], sv));	   
+		 vec_store2dpto2dp(&x3[2], vec_multiply2dp(values[1], sv));
+		 vec_store2dpto2dp(&x3[4], vec_multiply2dp(values[2], sv));
+		 vec_store2dpto2dp(&x3[6], vec_multiply2dp(values[3], sv));
+		 vec_store2dpto2dp(&x3[8], vec_multiply2dp(values[4], sv));	   
+		 vec_store2dpto2dp(&x3[10], vec_multiply2dp(values[5], sv));
+		 vec_store2dpto2dp(&x3[12], vec_multiply2dp(values[6], sv));
+		 vec_store2dpto2dp(&x3[14], vec_multiply2dp(values[7], sv));	     
 		 
 		 if(useFastScaling)
 		   addScale += wgt[i];
@@ -3418,14 +3418,14 @@
 	       }
 	     else
 	       {
-		 _mm_store_pd(&x3[0], values[0]);	   
-		 _mm_store_pd(&x3[2], values[1]);
-		 _mm_store_pd(&x3[4], values[2]);
-		 _mm_store_pd(&x3[6], values[3]);
-		 _mm_store_pd(&x3[8], values[4]);	   
-		 _mm_store_pd(&x3[10], values[5]);
-		 _mm_store_pd(&x3[12], values[6]);
-		 _mm_store_pd(&x3[14], values[7]);
+		 vec_store2dpto2dp(&x3[0], values[0]);	   
+		 vec_store2dpto2dp(&x3[2], values[1]);
+		 vec_store2dpto2dp(&x3[4], values[2]);
+		 vec_store2dpto2dp(&x3[6], values[3]);
+		 vec_store2dpto2dp(&x3[8], values[4]);	   
+		 vec_store2dpto2dp(&x3[10], values[5]);
+		 vec_store2dpto2dp(&x3[12], values[6]);
+		 vec_store2dpto2dp(&x3[14], values[7]);
 	       }
 	   }
       }
@@ -3433,7 +3433,7 @@
     case INNER_INNER:     
      for (i = 0; i < n; i++)
        {
-	 __m128d maxv =_mm_setzero_pd();
+	 __m128d maxv =vec_zero2dp();
 	 
 
 	 x1 = &x1_start[i * 16];
@@ -3449,37 +3449,37 @@
 	     double *left_k2_p = &left[j*16 + 2*4];
 	     double *left_k3_p = &left[j*16 + 3*4];
 	     
-	     __m128d x1_0 = _mm_load_pd( &x1_p[0] );
-	     __m128d x1_2 = _mm_load_pd( &x1_p[2] );
+	     __m128d x1_0 = vec_load2dpaligned( &x1_p[0] );
+	     __m128d x1_2 = vec_load2dpaligned( &x1_p[2] );
 	     
-	     __m128d left_k0_0 = _mm_load_pd( &left_k0_p[0] );
-	     __m128d left_k0_2 = _mm_load_pd( &left_k0_p[2] );
-	     __m128d left_k1_0 = _mm_load_pd( &left_k1_p[0] );
-	     __m128d left_k1_2 = _mm_load_pd( &left_k1_p[2] );
-	     __m128d left_k2_0 = _mm_load_pd( &left_k2_p[0] );
-	     __m128d left_k2_2 = _mm_load_pd( &left_k2_p[2] );
-	     __m128d left_k3_0 = _mm_load_pd( &left_k3_p[0] );
-	     __m128d left_k3_2 = _mm_load_pd( &left_k3_p[2] );
+	     __m128d left_k0_0 = vec_load2dpaligned( &left_k0_p[0] );
+	     __m128d left_k0_2 = vec_load2dpaligned( &left_k0_p[2] );
+	     __m128d left_k1_0 = vec_load2dpaligned( &left_k1_p[0] );
+	     __m128d left_k1_2 = vec_load2dpaligned( &left_k1_p[2] );
+	     __m128d left_k2_0 = vec_load2dpaligned( &left_k2_p[0] );
+	     __m128d left_k2_2 = vec_load2dpaligned( &left_k2_p[2] );
+	     __m128d left_k3_0 = vec_load2dpaligned( &left_k3_p[0] );
+	     __m128d left_k3_2 = vec_load2dpaligned( &left_k3_p[2] );
 	     
-	     left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-	     left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+	     left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+	     left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 	     
-	     left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-	     left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+	     left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+	     left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 	     
-	     left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-	     left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-	     left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+	     left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+	     left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+	     left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 	     
-	     left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-	     left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+	     left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+	     left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 	     
-	     left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-	     left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+	     left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+	     left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 	     
-	     left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-	     left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-	     left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+	     left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+	     left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+	     left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 	     
 	     
 	     
@@ -3490,45 +3490,45 @@
 	     double *right_k1_p = &right[j*16 + 1*4];
 	     double *right_k2_p = &right[j*16 + 2*4];
 	     double *right_k3_p = &right[j*16 + 3*4];
-	     __m128d x2_0 = _mm_load_pd( &x2_p[0] );
-	     __m128d x2_2 = _mm_load_pd( &x2_p[2] );
+	     __m128d x2_0 = vec_load2dpaligned( &x2_p[0] );
+	     __m128d x2_2 = vec_load2dpaligned( &x2_p[2] );
 	     
-	     __m128d right_k0_0 = _mm_load_pd( &right_k0_p[0] );
-	     __m128d right_k0_2 = _mm_load_pd( &right_k0_p[2] );
-	     __m128d right_k1_0 = _mm_load_pd( &right_k1_p[0] );
-	     __m128d right_k1_2 = _mm_load_pd( &right_k1_p[2] );
-	     __m128d right_k2_0 = _mm_load_pd( &right_k2_p[0] );
-	     __m128d right_k2_2 = _mm_load_pd( &right_k2_p[2] );
-	     __m128d right_k3_0 = _mm_load_pd( &right_k3_p[0] );
-	     __m128d right_k3_2 = _mm_load_pd( &right_k3_p[2] );
+	     __m128d right_k0_0 = vec_load2dpaligned( &right_k0_p[0] );
+	     __m128d right_k0_2 = vec_load2dpaligned( &right_k0_p[2] );
+	     __m128d right_k1_0 = vec_load2dpaligned( &right_k1_p[0] );
+	     __m128d right_k1_2 = vec_load2dpaligned( &right_k1_p[2] );
+	     __m128d right_k2_0 = vec_load2dpaligned( &right_k2_p[0] );
+	     __m128d right_k2_2 = vec_load2dpaligned( &right_k2_p[2] );
+	     __m128d right_k3_0 = vec_load2dpaligned( &right_k3_p[0] );
+	     __m128d right_k3_2 = vec_load2dpaligned( &right_k3_p[2] );
 	     
-	     right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-	     right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+	     right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+	     right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 	     
-	     right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-	     right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+	     right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+	     right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 	     
-	     right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-	     right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-	     right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+	     right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+	     right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+	     right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 	     
-	     right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-	     right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+	     right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+	     right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 	     
 	     
-	     right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-	     right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+	     right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+	     right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 	     
-	     right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-	     right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-	     right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+	     right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+	     right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+	     right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 
              
              /* multiply left * right */
             
 
-	     __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-	     __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );
+	     __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+	     __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );
 
 
              
@@ -3544,52 +3544,52 @@
 	     __m128d EV_t_l3_k2 = EVV[7];
 
 
-	    EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	    EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	    EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	    EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 
-	    EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	    EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	    EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	    EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 
-	    EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	    EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 
-	    EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	    EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	    EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	    EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 
 
-	    EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-            EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-            EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	    EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+            EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+            EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 
-            EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+            EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 
 	    
 	    values[j * 2] = EV_t_l0_k0;
 	    values[j * 2 + 1] = EV_t_l2_k0;            	   	    
 
-	    maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l0_k0, absMask.m));
-	    maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l2_k0, absMask.m));
+	    maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l0_k0, absMask.m));
+	    maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l2_k0, absMask.m));
            }
 	 	 
 	 
-	 _mm_store_pd(maxima, maxv);
+	 vec_store2dpto2dp(maxima, maxv);
 	 
 	 max = MAX(maxima[0], maxima[1]);
 	 
 	 if(max < minlikelihood)
 	   {
-	     __m128d sv = _mm_set1_pd(twotothe256);
+	     __m128d sv = vec_splat2dp(twotothe256);
 	       		       	   	 	     
-	     _mm_store_pd(&x3[0], _mm_mul_pd(values[0], sv));	   
-	     _mm_store_pd(&x3[2], _mm_mul_pd(values[1], sv));
-	     _mm_store_pd(&x3[4], _mm_mul_pd(values[2], sv));
-	     _mm_store_pd(&x3[6], _mm_mul_pd(values[3], sv));
-	     _mm_store_pd(&x3[8], _mm_mul_pd(values[4], sv));	   
-	     _mm_store_pd(&x3[10], _mm_mul_pd(values[5], sv));
-	     _mm_store_pd(&x3[12], _mm_mul_pd(values[6], sv));
-	     _mm_store_pd(&x3[14], _mm_mul_pd(values[7], sv));	     
+	     vec_store2dpto2dp(&x3[0], vec_multiply2dp(values[0], sv));	   
+	     vec_store2dpto2dp(&x3[2], vec_multiply2dp(values[1], sv));
+	     vec_store2dpto2dp(&x3[4], vec_multiply2dp(values[2], sv));
+	     vec_store2dpto2dp(&x3[6], vec_multiply2dp(values[3], sv));
+	     vec_store2dpto2dp(&x3[8], vec_multiply2dp(values[4], sv));	   
+	     vec_store2dpto2dp(&x3[10], vec_multiply2dp(values[5], sv));
+	     vec_store2dpto2dp(&x3[12], vec_multiply2dp(values[6], sv));
+	     vec_store2dpto2dp(&x3[14], vec_multiply2dp(values[7], sv));	     
 	     
 	     if(useFastScaling)
 	       addScale += wgt[i];
@@ -3598,14 +3598,14 @@
 	   }
 	 else
 	   {
-	     _mm_store_pd(&x3[0], values[0]);	   
-	     _mm_store_pd(&x3[2], values[1]);
-	     _mm_store_pd(&x3[4], values[2]);
-	     _mm_store_pd(&x3[6], values[3]);
-	     _mm_store_pd(&x3[8], values[4]);	   
-	     _mm_store_pd(&x3[10], values[5]);
-	     _mm_store_pd(&x3[12], values[6]);
-	     _mm_store_pd(&x3[14], values[7]);
+	     vec_store2dpto2dp(&x3[0], values[0]);	   
+	     vec_store2dpto2dp(&x3[2], values[1]);
+	     vec_store2dpto2dp(&x3[4], values[2]);
+	     vec_store2dpto2dp(&x3[6], values[3]);
+	     vec_store2dpto2dp(&x3[8], values[4]);	   
+	     vec_store2dpto2dp(&x3[10], values[5]);
+	     vec_store2dpto2dp(&x3[12], values[6]);
+	     vec_store2dpto2dp(&x3[14], values[7]);
 	   }	 
        }
    
@@ -3657,7 +3657,7 @@
       EV_t[4 * l + k] = EV[4 * k + l];
 
   for(k = 0; k < 8; k++)
-    EVV[k] = _mm_load_pd(&EV_t[k * 2]);      
+    EVV[k] = vec_load2dpaligned(&EV_t[k * 2]);      
  
   
 
@@ -3670,24 +3670,24 @@
 
 	for (i = 1; i < 16; i++)
 	  {	    
-	    __m128d x1_1 = _mm_load_pd(&(tipVector[i*4]));
-	    __m128d x1_2 = _mm_load_pd(&(tipVector[i*4 + 2]));	   
+	    __m128d x1_1 = vec_load2dpaligned(&(tipVector[i*4]));
+	    __m128d x1_2 = vec_load2dpaligned(&(tipVector[i*4 + 2]));	   
 	    
 	    if((mask32[i] & x1_presenceMap) || i == 15)
 	      {
 		for (j = 0; j < 4; j++)
 		  for (k = 0; k < 4; k++)
 		    {			 	 
-		      __m128d left1 = _mm_load_pd(&left[j*16 + k*4]);
-		      __m128d left2 = _mm_load_pd(&left[j*16 + k*4 + 2]);
+		      __m128d left1 = vec_load2dpaligned(&left[j*16 + k*4]);
+		      __m128d left2 = vec_load2dpaligned(&left[j*16 + k*4 + 2]);
 		      
-		      __m128d acc = _mm_setzero_pd();
+		      __m128d acc = vec_zero2dp();
 		      
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left1, x1_1));
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left2, x1_2));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left1, x1_1));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left2, x1_2));
 		      
-		      acc = _mm_hadd_pd(acc, acc);
-		      _mm_storel_pd(&umpX1[i*16 + j*4 + k], acc);
+		      acc = vec_horizontaladd2dp(acc, acc);
+		      vec_storelower1dpof2dp(&umpX1[i*16 + j*4 + k], acc);
 		    }
 	      }
 	  
@@ -3696,16 +3696,16 @@
 		for (j = 0; j < 4; j++)
 		  for (k = 0; k < 4; k++)
 		    {
-		      __m128d left1 = _mm_load_pd(&right[j*16 + k*4]);
-		      __m128d left2 = _mm_load_pd(&right[j*16 + k*4 + 2]);
+		      __m128d left1 = vec_load2dpaligned(&right[j*16 + k*4]);
+		      __m128d left2 = vec_load2dpaligned(&right[j*16 + k*4 + 2]);
 		      
-		      __m128d acc = _mm_setzero_pd();
+		      __m128d acc = vec_zero2dp();
 		      
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left1, x1_1));
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left2, x1_2));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left1, x1_1));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left2, x1_2));
 		      
-		      acc = _mm_hadd_pd(acc, acc);
-		      _mm_storel_pd(&umpX2[i*16 + j*4 + k], acc);
+		      acc = vec_horizontaladd2dp(acc, acc);
+		      vec_storelower1dpof2dp(&umpX2[i*16 + j*4 + k], acc);
 		      
 		    }
 	      }   	
@@ -3716,14 +3716,14 @@
 	
 	for (j = 0; j < 4; j++)
 	  {				 		  		  		   
-	    __m128d uX1_k0_sse = _mm_load_pd( &uX1[j * 4] );
-	    __m128d uX1_k2_sse = _mm_load_pd( &uX1[j * 4 + 2] );
+	    __m128d uX1_k0_sse = vec_load2dpaligned( &uX1[j * 4] );
+	    __m128d uX1_k2_sse = vec_load2dpaligned( &uX1[j * 4 + 2] );
 	    	    
-	    __m128d uX2_k0_sse = _mm_load_pd( &uX2[j * 4] );
-	    __m128d uX2_k2_sse = _mm_load_pd( &uX2[j * 4 + 2] );
+	    __m128d uX2_k0_sse = vec_load2dpaligned( &uX2[j * 4] );
+	    __m128d uX2_k2_sse = vec_load2dpaligned( &uX2[j * 4 + 2] );
 	    
-	    __m128d x1px2_k0 = _mm_mul_pd( uX1_k0_sse, uX2_k0_sse );
-	    __m128d x1px2_k2 = _mm_mul_pd( uX1_k2_sse, uX2_k2_sse );		    		    		   
+	    __m128d x1px2_k0 = vec_multiply2dp( uX1_k0_sse, uX2_k0_sse );
+	    __m128d x1px2_k2 = vec_multiply2dp( uX1_k2_sse, uX2_k2_sse );		    		    		   
 	    
 	    __m128d EV_t_l0_k0 = EVV[0];
 	    __m128d EV_t_l0_k2 = EVV[1];
@@ -3734,28 +3734,28 @@
 	    __m128d EV_t_l3_k0 = EVV[6]; 
 	    __m128d EV_t_l3_k2 = EVV[7];
 	    
-	    EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	    EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	    EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	    EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 	    
-	    EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	    EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	    EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	    EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 	    
-	    EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	    EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 	    
-	    EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	    EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	    EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	    EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 	    
-	    EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-	    EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-	    EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	    EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+	    EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+	    EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 	    
-	    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+	    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 	    	  
-	    _mm_store_pd( &x3_gapColumn[j * 4 + 0], EV_t_l0_k0 );
-	    _mm_store_pd( &x3_gapColumn[j * 4 + 2], EV_t_l2_k0 );	   
+	    vec_store2dpto2dp( &x3_gapColumn[j * 4 + 0], EV_t_l0_k0 );
+	    vec_store2dpto2dp( &x3_gapColumn[j * 4 + 2], EV_t_l2_k0 );	   
 	  }  
 	
        
@@ -3770,17 +3770,17 @@
 		
 		for (j = 0; j < 4; j++)
 		  {				 		  		  		   
-		    __m128d uX1_k0_sse = _mm_load_pd( &uX1[j * 4] );
-		    __m128d uX1_k2_sse = _mm_load_pd( &uX1[j * 4 + 2] );
+		    __m128d uX1_k0_sse = vec_load2dpaligned( &uX1[j * 4] );
+		    __m128d uX1_k2_sse = vec_load2dpaligned( &uX1[j * 4 + 2] );
 		    
 		    
-		    __m128d uX2_k0_sse = _mm_load_pd( &uX2[j * 4] );
-		    __m128d uX2_k2_sse = _mm_load_pd( &uX2[j * 4 + 2] );
+		    __m128d uX2_k0_sse = vec_load2dpaligned( &uX2[j * 4] );
+		    __m128d uX2_k2_sse = vec_load2dpaligned( &uX2[j * 4 + 2] );
 		    		    		    
 		    /* multiply left * right */		   
 		    
-		    __m128d x1px2_k0 = _mm_mul_pd( uX1_k0_sse, uX2_k0_sse );
-		    __m128d x1px2_k2 = _mm_mul_pd( uX1_k2_sse, uX2_k2_sse );
+		    __m128d x1px2_k0 = vec_multiply2dp( uX1_k0_sse, uX2_k0_sse );
+		    __m128d x1px2_k2 = vec_multiply2dp( uX1_k2_sse, uX2_k2_sse );
 		    
 		    		   
 		    /* multiply with EV matrix (!?) */		   
@@ -3794,28 +3794,28 @@
 		    __m128d EV_t_l3_k0 = EVV[6]; 
 		    __m128d EV_t_l3_k2 = EVV[7];
 		    
-		    EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-		    EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-		    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+		    EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+		    EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+		    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 		    
-		    EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-		    EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+		    EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+		    EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 		    
-		    EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-		    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+		    EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+		    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 		    
-		    EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-		    EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-		    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+		    EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+		    EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+		    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 		    
-		    EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-		    EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-		    EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+		    EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+		    EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+		    EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 		    
-		    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+		    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 		    
-		    _mm_store_pd( &x3[j * 4 + 0], EV_t_l0_k0 );
-		    _mm_store_pd( &x3[j * 4 + 2], EV_t_l2_k0 );
+		    vec_store2dpto2dp( &x3[j * 4 + 0], EV_t_l0_k0 );
+		    vec_store2dpto2dp( &x3[j * 4 + 2], EV_t_l2_k0 );
 		  }
 		
 		x3 += 16;
@@ -3833,28 +3833,28 @@
 	  {
 	    if((mask32[i] & x1_presenceMap) || i == 15)
 	      {
-		__m128d x1_1 = _mm_load_pd(&(tipVector[i*4]));
-		__m128d x1_2 = _mm_load_pd(&(tipVector[i*4 + 2]));	   
+		__m128d x1_1 = vec_load2dpaligned(&(tipVector[i*4]));
+		__m128d x1_2 = vec_load2dpaligned(&(tipVector[i*4 + 2]));	   
 		
 		for (j = 0; j < 4; j++)
 		  for (k = 0; k < 4; k++)
 		    {		 
-		      __m128d left1 = _mm_load_pd(&left[j*16 + k*4]);
-		      __m128d left2 = _mm_load_pd(&left[j*16 + k*4 + 2]);
+		      __m128d left1 = vec_load2dpaligned(&left[j*16 + k*4]);
+		      __m128d left2 = vec_load2dpaligned(&left[j*16 + k*4 + 2]);
 		      
-		      __m128d acc = _mm_setzero_pd();
+		      __m128d acc = vec_zero2dp();
 		      
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left1, x1_1));
-		      acc = _mm_add_pd(acc, _mm_mul_pd(left2, x1_2));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left1, x1_1));
+		      acc = vec_add2dp(acc, vec_multiply2dp(left2, x1_2));
 		      
-		      acc = _mm_hadd_pd(acc, acc);
-		      _mm_storel_pd(&umpX1[i*16 + j*4 + k], acc);		 
+		      acc = vec_horizontaladd2dp(acc, acc);
+		      vec_storelower1dpof2dp(&umpX1[i*16 + j*4 + k], acc);		 
 		    }
 	      }
 	  }
 
 	{
-	  __m128d maxv =_mm_setzero_pd();
+	  __m128d maxv =vec_zero2dp();
 	  
 	  scaleGap = 0;
 	  
@@ -3870,43 +3870,43 @@
 	      double *right_k1_p = &right[j*16 + 1*4];
 	      double *right_k2_p = &right[j*16 + 2*4];
 	      double *right_k3_p = &right[j*16 + 3*4];
-	      __m128d x2_0 = _mm_load_pd( &x2_p[0] );
-	      __m128d x2_2 = _mm_load_pd( &x2_p[2] );
+	      __m128d x2_0 = vec_load2dpaligned( &x2_p[0] );
+	      __m128d x2_2 = vec_load2dpaligned( &x2_p[2] );
 	      
-	      __m128d right_k0_0 = _mm_load_pd( &right_k0_p[0] );
-	      __m128d right_k0_2 = _mm_load_pd( &right_k0_p[2] );
-	      __m128d right_k1_0 = _mm_load_pd( &right_k1_p[0] );
-	      __m128d right_k1_2 = _mm_load_pd( &right_k1_p[2] );
-	      __m128d right_k2_0 = _mm_load_pd( &right_k2_p[0] );
-	      __m128d right_k2_2 = _mm_load_pd( &right_k2_p[2] );
-	      __m128d right_k3_0 = _mm_load_pd( &right_k3_p[0] );
-	      __m128d right_k3_2 = _mm_load_pd( &right_k3_p[2] );
+	      __m128d right_k0_0 = vec_load2dpaligned( &right_k0_p[0] );
+	      __m128d right_k0_2 = vec_load2dpaligned( &right_k0_p[2] );
+	      __m128d right_k1_0 = vec_load2dpaligned( &right_k1_p[0] );
+	      __m128d right_k1_2 = vec_load2dpaligned( &right_k1_p[2] );
+	      __m128d right_k2_0 = vec_load2dpaligned( &right_k2_p[0] );
+	      __m128d right_k2_2 = vec_load2dpaligned( &right_k2_p[2] );
+	      __m128d right_k3_0 = vec_load2dpaligned( &right_k3_p[0] );
+	      __m128d right_k3_2 = vec_load2dpaligned( &right_k3_p[2] );
 	      	      
-	      right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-	      right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+	      right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+	      right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 	      
-	      right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-	      right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+	      right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+	      right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 	      
-	      right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-	      right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-	      right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+	      right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+	      right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+	      right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 	      	       
-	      right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-	      right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+	      right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+	      right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 	      	       
-	      right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-	      right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+	      right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+	      right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 	      
-	      right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-	      right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-	      right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);
+	      right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+	      right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+	      right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);
 	      
-	      __m128d uX1_k0_sse = _mm_load_pd( &uX1[j * 4] );
-	      __m128d uX1_k2_sse = _mm_load_pd( &uX1[j * 4 + 2] );
+	      __m128d uX1_k0_sse = vec_load2dpaligned( &uX1[j * 4] );
+	      __m128d uX1_k2_sse = vec_load2dpaligned( &uX1[j * 4 + 2] );
 	      
-	      __m128d x1px2_k0 = _mm_mul_pd( uX1_k0_sse, right_k0_0 );
-	      __m128d x1px2_k2 = _mm_mul_pd( uX1_k2_sse, right_k2_0 );
+	      __m128d x1px2_k0 = vec_multiply2dp( uX1_k0_sse, right_k0_0 );
+	      __m128d x1px2_k2 = vec_multiply2dp( uX1_k2_sse, right_k2_0 );
 	      
 	      __m128d EV_t_l0_k0 = EVV[0];
 	      __m128d EV_t_l0_k2 = EVV[1];
@@ -3917,35 +3917,35 @@
 	      __m128d EV_t_l3_k0 = EVV[6]; 
 	      __m128d EV_t_l3_k2 = EVV[7];
 	      
-	      EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	      EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	      EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	      EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	      EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	      EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 	      
-	      EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	      EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	      EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	      EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 	      
-	      EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	      EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	      EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	      EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 	      
-	      EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	      EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	      EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	      EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	      EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	      EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 	      
-	      EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-	      EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-	      EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	      EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+	      EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+	      EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 	      
-	      EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+	      EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 	      
 	      values[j * 2]     = EV_t_l0_k0;
 	      values[j * 2 + 1] = EV_t_l2_k0;		   		   
 	      
-	      maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l0_k0, absMask.m));
-	      maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l2_k0, absMask.m));		   	     		   
+	      maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l0_k0, absMask.m));
+	      maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l2_k0, absMask.m));		   	     		   
 	    }
 
 	  
-	  _mm_store_pd(maxima, maxv);
+	  vec_store2dpto2dp(maxima, maxv);
 		 
 	  max = MAX(maxima[0], maxima[1]);
 	  
@@ -3953,27 +3953,27 @@
 	    {
 	      scaleGap = 1;
 	      
-	      __m128d sv = _mm_set1_pd(twotothe256);
+	      __m128d sv = vec_splat2dp(twotothe256);
 	      
-	      _mm_store_pd(&x3[0], _mm_mul_pd(values[0], sv));	   
-	      _mm_store_pd(&x3[2], _mm_mul_pd(values[1], sv));
-	      _mm_store_pd(&x3[4], _mm_mul_pd(values[2], sv));
-	      _mm_store_pd(&x3[6], _mm_mul_pd(values[3], sv));
-	      _mm_store_pd(&x3[8], _mm_mul_pd(values[4], sv));	   
-	      _mm_store_pd(&x3[10], _mm_mul_pd(values[5], sv));
-	      _mm_store_pd(&x3[12], _mm_mul_pd(values[6], sv));
-	      _mm_store_pd(&x3[14], _mm_mul_pd(values[7], sv));	     	      	     
+	      vec_store2dpto2dp(&x3[0], vec_multiply2dp(values[0], sv));	   
+	      vec_store2dpto2dp(&x3[2], vec_multiply2dp(values[1], sv));
+	      vec_store2dpto2dp(&x3[4], vec_multiply2dp(values[2], sv));
+	      vec_store2dpto2dp(&x3[6], vec_multiply2dp(values[3], sv));
+	      vec_store2dpto2dp(&x3[8], vec_multiply2dp(values[4], sv));	   
+	      vec_store2dpto2dp(&x3[10], vec_multiply2dp(values[5], sv));
+	      vec_store2dpto2dp(&x3[12], vec_multiply2dp(values[6], sv));
+	      vec_store2dpto2dp(&x3[14], vec_multiply2dp(values[7], sv));	     	      	     
 	    }
 	  else
 	    {
-	      _mm_store_pd(&x3[0], values[0]);	   
-	      _mm_store_pd(&x3[2], values[1]);
-	      _mm_store_pd(&x3[4], values[2]);
-	      _mm_store_pd(&x3[6], values[3]);
-	      _mm_store_pd(&x3[8], values[4]);	   
-	      _mm_store_pd(&x3[10], values[5]);
-	      _mm_store_pd(&x3[12], values[6]);
-	      _mm_store_pd(&x3[14], values[7]);
+	      vec_store2dpto2dp(&x3[0], values[0]);	   
+	      vec_store2dpto2dp(&x3[2], values[1]);
+	      vec_store2dpto2dp(&x3[4], values[2]);
+	      vec_store2dpto2dp(&x3[6], values[3]);
+	      vec_store2dpto2dp(&x3[8], values[4]);	   
+	      vec_store2dpto2dp(&x3[10], values[5]);
+	      vec_store2dpto2dp(&x3[12], values[6]);
+	      vec_store2dpto2dp(&x3[14], values[7]);
 	    }
 	}		       	
       	
@@ -3993,7 +3993,7 @@
 	       }
 	     else
 	       {				 
-		 __m128d maxv =_mm_setzero_pd();		 
+		 __m128d maxv =vec_zero2dp();		 
 		 
 		 if(x2_gap[i / 32] & mask32[i % 32])
 		   x2 = x2_gapColumn;
@@ -4013,56 +4013,56 @@
 		     double *right_k1_p = &right[j*16 + 1*4];
 		     double *right_k2_p = &right[j*16 + 2*4];
 		     double *right_k3_p = &right[j*16 + 3*4];
-		     __m128d x2_0 = _mm_load_pd( &x2_p[0] );
-		     __m128d x2_2 = _mm_load_pd( &x2_p[2] );
+		     __m128d x2_0 = vec_load2dpaligned( &x2_p[0] );
+		     __m128d x2_2 = vec_load2dpaligned( &x2_p[2] );
 		     
-		     __m128d right_k0_0 = _mm_load_pd( &right_k0_p[0] );
-		     __m128d right_k0_2 = _mm_load_pd( &right_k0_p[2] );
-		     __m128d right_k1_0 = _mm_load_pd( &right_k1_p[0] );
-		     __m128d right_k1_2 = _mm_load_pd( &right_k1_p[2] );
-		     __m128d right_k2_0 = _mm_load_pd( &right_k2_p[0] );
-		     __m128d right_k2_2 = _mm_load_pd( &right_k2_p[2] );
-		     __m128d right_k3_0 = _mm_load_pd( &right_k3_p[0] );
-		     __m128d right_k3_2 = _mm_load_pd( &right_k3_p[2] );
+		     __m128d right_k0_0 = vec_load2dpaligned( &right_k0_p[0] );
+		     __m128d right_k0_2 = vec_load2dpaligned( &right_k0_p[2] );
+		     __m128d right_k1_0 = vec_load2dpaligned( &right_k1_p[0] );
+		     __m128d right_k1_2 = vec_load2dpaligned( &right_k1_p[2] );
+		     __m128d right_k2_0 = vec_load2dpaligned( &right_k2_p[0] );
+		     __m128d right_k2_2 = vec_load2dpaligned( &right_k2_p[2] );
+		     __m128d right_k3_0 = vec_load2dpaligned( &right_k3_p[0] );
+		     __m128d right_k3_2 = vec_load2dpaligned( &right_k3_p[2] );
 		     
 		     		     
-		     right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-		     right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+		     right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+		     right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 		     
-		     right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-		     right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+		     right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+		     right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 		     
-		     right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-		     right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-		     right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+		     right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+		     right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+		     right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 		     
 		     
-		     right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-		     right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+		     right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+		     right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 		     
 		     
-		     right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-		     right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+		     right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+		     right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 		     
-		     right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-		     right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-		     right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);
+		     right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+		     right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+		     right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);
 		     
 		     {
 		       //
 		       // load left side from tip vector
 		       //
 		       
-		       __m128d uX1_k0_sse = _mm_load_pd( &uX1[j * 4] );
-		       __m128d uX1_k2_sse = _mm_load_pd( &uX1[j * 4 + 2] );
+		       __m128d uX1_k0_sse = vec_load2dpaligned( &uX1[j * 4] );
+		       __m128d uX1_k2_sse = vec_load2dpaligned( &uX1[j * 4 + 2] );
 		       
 		       
 		       //
 		       // multiply left * right
 		       //
 		       
-		       __m128d x1px2_k0 = _mm_mul_pd( uX1_k0_sse, right_k0_0 );
-		       __m128d x1px2_k2 = _mm_mul_pd( uX1_k2_sse, right_k2_0 );
+		       __m128d x1px2_k0 = vec_multiply2dp( uX1_k0_sse, right_k0_0 );
+		       __m128d x1px2_k2 = vec_multiply2dp( uX1_k2_sse, right_k2_0 );
 		       
 		       
 		       //
@@ -4079,51 +4079,51 @@
 		       __m128d EV_t_l3_k2 = EVV[7];
 		       
 		       
-		       EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-		       EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-		       EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+		       EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+		       EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+		       EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 		       
-		       EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-		       EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+		       EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+		       EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 		       
-		       EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-		       EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+		       EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+		       EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 		       
-		       EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-		       EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-		       EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+		       EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+		       EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+		       EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 		       
-		       EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-		       EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-		       EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+		       EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+		       EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+		       EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 		       
-		       EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+		       EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 		       
 		       values[j * 2]     = EV_t_l0_k0;
 		       values[j * 2 + 1] = EV_t_l2_k0;		   		   
 			   
-		       maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l0_k0, absMask.m));
-		       maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l2_k0, absMask.m));		   
+		       maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l0_k0, absMask.m));
+		       maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l2_k0, absMask.m));		   
 		     }		   
 		   }
 
 	     
-		 _mm_store_pd(maxima, maxv);
+		 vec_store2dpto2dp(maxima, maxv);
 		 
 		 max = MAX(maxima[0], maxima[1]);
 		 
 		 if(max < minlikelihood)
 		   {
-		     __m128d sv = _mm_set1_pd(twotothe256);
+		     __m128d sv = vec_splat2dp(twotothe256);
 		     
-		     _mm_store_pd(&x3[0], _mm_mul_pd(values[0], sv));	   
-		     _mm_store_pd(&x3[2], _mm_mul_pd(values[1], sv));
-		     _mm_store_pd(&x3[4], _mm_mul_pd(values[2], sv));
-		     _mm_store_pd(&x3[6], _mm_mul_pd(values[3], sv));
-		     _mm_store_pd(&x3[8], _mm_mul_pd(values[4], sv));	   
-		     _mm_store_pd(&x3[10], _mm_mul_pd(values[5], sv));
-		     _mm_store_pd(&x3[12], _mm_mul_pd(values[6], sv));
-		     _mm_store_pd(&x3[14], _mm_mul_pd(values[7], sv));	     
+		     vec_store2dpto2dp(&x3[0], vec_multiply2dp(values[0], sv));	   
+		     vec_store2dpto2dp(&x3[2], vec_multiply2dp(values[1], sv));
+		     vec_store2dpto2dp(&x3[4], vec_multiply2dp(values[2], sv));
+		     vec_store2dpto2dp(&x3[6], vec_multiply2dp(values[3], sv));
+		     vec_store2dpto2dp(&x3[8], vec_multiply2dp(values[4], sv));	   
+		     vec_store2dpto2dp(&x3[10], vec_multiply2dp(values[5], sv));
+		     vec_store2dpto2dp(&x3[12], vec_multiply2dp(values[6], sv));
+		     vec_store2dpto2dp(&x3[14], vec_multiply2dp(values[7], sv));	     
 		     
 		     if(useFastScaling)
 		       addScale += wgt[i];
@@ -4132,14 +4132,14 @@
 		   }
 		 else
 		   {
-		     _mm_store_pd(&x3[0], values[0]);	   
-		     _mm_store_pd(&x3[2], values[1]);
-		     _mm_store_pd(&x3[4], values[2]);
-		     _mm_store_pd(&x3[6], values[3]);
-		     _mm_store_pd(&x3[8], values[4]);	   
-		     _mm_store_pd(&x3[10], values[5]);
-		     _mm_store_pd(&x3[12], values[6]);
-		     _mm_store_pd(&x3[14], values[7]);
+		     vec_store2dpto2dp(&x3[0], values[0]);	   
+		     vec_store2dpto2dp(&x3[2], values[1]);
+		     vec_store2dpto2dp(&x3[4], values[2]);
+		     vec_store2dpto2dp(&x3[6], values[3]);
+		     vec_store2dpto2dp(&x3[8], values[4]);	   
+		     vec_store2dpto2dp(&x3[10], values[5]);
+		     vec_store2dpto2dp(&x3[12], values[6]);
+		     vec_store2dpto2dp(&x3[14], values[7]);
 		   }		 
 		 
 		 x3 += 16;
@@ -4149,7 +4149,7 @@
       break;
     case INNER_INNER:         
       {
-	__m128d maxv =_mm_setzero_pd();
+	__m128d maxv =vec_zero2dp();
 	
 	scaleGap = 0;
 	
@@ -4166,37 +4166,37 @@
 	    double *left_k2_p = &left[j*16 + 2*4];
 	    double *left_k3_p = &left[j*16 + 3*4];
 	    
-	    __m128d x1_0 = _mm_load_pd( &x1_p[0] );
-	    __m128d x1_2 = _mm_load_pd( &x1_p[2] );
+	    __m128d x1_0 = vec_load2dpaligned( &x1_p[0] );
+	    __m128d x1_2 = vec_load2dpaligned( &x1_p[2] );
 	    
-	    __m128d left_k0_0 = _mm_load_pd( &left_k0_p[0] );
-	    __m128d left_k0_2 = _mm_load_pd( &left_k0_p[2] );
-	    __m128d left_k1_0 = _mm_load_pd( &left_k1_p[0] );
-	    __m128d left_k1_2 = _mm_load_pd( &left_k1_p[2] );
-	    __m128d left_k2_0 = _mm_load_pd( &left_k2_p[0] );
-	    __m128d left_k2_2 = _mm_load_pd( &left_k2_p[2] );
-	    __m128d left_k3_0 = _mm_load_pd( &left_k3_p[0] );
-	    __m128d left_k3_2 = _mm_load_pd( &left_k3_p[2] );
+	    __m128d left_k0_0 = vec_load2dpaligned( &left_k0_p[0] );
+	    __m128d left_k0_2 = vec_load2dpaligned( &left_k0_p[2] );
+	    __m128d left_k1_0 = vec_load2dpaligned( &left_k1_p[0] );
+	    __m128d left_k1_2 = vec_load2dpaligned( &left_k1_p[2] );
+	    __m128d left_k2_0 = vec_load2dpaligned( &left_k2_p[0] );
+	    __m128d left_k2_2 = vec_load2dpaligned( &left_k2_p[2] );
+	    __m128d left_k3_0 = vec_load2dpaligned( &left_k3_p[0] );
+	    __m128d left_k3_2 = vec_load2dpaligned( &left_k3_p[2] );
 	    
-	    left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-	    left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+	    left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+	    left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 	    
-	    left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-	    left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+	    left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+	    left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 	    
-	    left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-	    left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-	    left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+	    left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+	    left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+	    left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 	    
-	    left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-	    left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+	    left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+	    left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 	    
-	    left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-	    left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+	    left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+	    left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 	    
-	    left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-	    left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-	    left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+	    left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+	    left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+	    left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 	    
 	    
 	    double *x2_p = &x2[j*4];
@@ -4204,40 +4204,40 @@
 	    double *right_k1_p = &right[j*16 + 1*4];
 	    double *right_k2_p = &right[j*16 + 2*4];
 	    double *right_k3_p = &right[j*16 + 3*4];
-	    __m128d x2_0 = _mm_load_pd( &x2_p[0] );
-	    __m128d x2_2 = _mm_load_pd( &x2_p[2] );
+	    __m128d x2_0 = vec_load2dpaligned( &x2_p[0] );
+	    __m128d x2_2 = vec_load2dpaligned( &x2_p[2] );
 	    
-	    __m128d right_k0_0 = _mm_load_pd( &right_k0_p[0] );
-	    __m128d right_k0_2 = _mm_load_pd( &right_k0_p[2] );
-	    __m128d right_k1_0 = _mm_load_pd( &right_k1_p[0] );
-	    __m128d right_k1_2 = _mm_load_pd( &right_k1_p[2] );
-	    __m128d right_k2_0 = _mm_load_pd( &right_k2_p[0] );
-	    __m128d right_k2_2 = _mm_load_pd( &right_k2_p[2] );
-	    __m128d right_k3_0 = _mm_load_pd( &right_k3_p[0] );
-	    __m128d right_k3_2 = _mm_load_pd( &right_k3_p[2] );
+	    __m128d right_k0_0 = vec_load2dpaligned( &right_k0_p[0] );
+	    __m128d right_k0_2 = vec_load2dpaligned( &right_k0_p[2] );
+	    __m128d right_k1_0 = vec_load2dpaligned( &right_k1_p[0] );
+	    __m128d right_k1_2 = vec_load2dpaligned( &right_k1_p[2] );
+	    __m128d right_k2_0 = vec_load2dpaligned( &right_k2_p[0] );
+	    __m128d right_k2_2 = vec_load2dpaligned( &right_k2_p[2] );
+	    __m128d right_k3_0 = vec_load2dpaligned( &right_k3_p[0] );
+	    __m128d right_k3_2 = vec_load2dpaligned( &right_k3_p[2] );
 	    
-	    right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-	    right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+	    right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+	    right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 	    
-	    right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-	    right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+	    right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+	    right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 	    
-	    right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-	    right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-	    right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+	    right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+	    right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+	    right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 	    
-	    right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-	    right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+	    right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+	    right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 	    	    
-	    right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-	    right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+	    right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+	    right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 	    
-	    right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-	    right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-	    right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   		 		
+	    right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+	    right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+	    right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   		 		
 	    
-	    __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-	    __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );		 		 	   
+	    __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+	    __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );		 		 	   
 	    
 	    __m128d EV_t_l0_k0 = EVV[0];
 	    __m128d EV_t_l0_k2 = EVV[1];
@@ -4248,63 +4248,63 @@
 	    __m128d EV_t_l3_k0 = EVV[6]; 
 	    __m128d EV_t_l3_k2 = EVV[7];
 	    
-	    EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-	    EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-	    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+	    EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+	    EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+	    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 	    
-	    EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-	    EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+	    EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+	    EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 	    
-	    EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-	    EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+	    EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+	    EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 	    
-	    EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-	    EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-	    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+	    EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+	    EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+	    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 	    
-	    EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-	    EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-	    EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+	    EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+	    EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+	    EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 	    
-	    EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+	    EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 	    
 	    
 	    values[j * 2] = EV_t_l0_k0;
 	    values[j * 2 + 1] = EV_t_l2_k0;            	   	    
 	    
-	    maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l0_k0, absMask.m));
-	    maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l2_k0, absMask.m));
+	    maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l0_k0, absMask.m));
+	    maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l2_k0, absMask.m));
 	  }
 		     
-	_mm_store_pd(maxima, maxv);
+	vec_store2dpto2dp(maxima, maxv);
 	
 	max = MAX(maxima[0], maxima[1]);
 	
 	if(max < minlikelihood)
 	  {
-	    __m128d sv = _mm_set1_pd(twotothe256);
+	    __m128d sv = vec_splat2dp(twotothe256);
 	    
 	    scaleGap = 1;
 	    
-	    _mm_store_pd(&x3[0], _mm_mul_pd(values[0], sv));	   
-	    _mm_store_pd(&x3[2], _mm_mul_pd(values[1], sv));
-	    _mm_store_pd(&x3[4], _mm_mul_pd(values[2], sv));
-	    _mm_store_pd(&x3[6], _mm_mul_pd(values[3], sv));
-	    _mm_store_pd(&x3[8], _mm_mul_pd(values[4], sv));	   
-	    _mm_store_pd(&x3[10], _mm_mul_pd(values[5], sv));
-	    _mm_store_pd(&x3[12], _mm_mul_pd(values[6], sv));
-	    _mm_store_pd(&x3[14], _mm_mul_pd(values[7], sv));	     	    	 
+	    vec_store2dpto2dp(&x3[0], vec_multiply2dp(values[0], sv));	   
+	    vec_store2dpto2dp(&x3[2], vec_multiply2dp(values[1], sv));
+	    vec_store2dpto2dp(&x3[4], vec_multiply2dp(values[2], sv));
+	    vec_store2dpto2dp(&x3[6], vec_multiply2dp(values[3], sv));
+	    vec_store2dpto2dp(&x3[8], vec_multiply2dp(values[4], sv));	   
+	    vec_store2dpto2dp(&x3[10], vec_multiply2dp(values[5], sv));
+	    vec_store2dpto2dp(&x3[12], vec_multiply2dp(values[6], sv));
+	    vec_store2dpto2dp(&x3[14], vec_multiply2dp(values[7], sv));	     	    	 
 	  }
 	else
 	  {
-	    _mm_store_pd(&x3[0], values[0]);	   
-	    _mm_store_pd(&x3[2], values[1]);
-	    _mm_store_pd(&x3[4], values[2]);
-	    _mm_store_pd(&x3[6], values[3]);
-	    _mm_store_pd(&x3[8], values[4]);	   
-	    _mm_store_pd(&x3[10], values[5]);
-	    _mm_store_pd(&x3[12], values[6]);
-	    _mm_store_pd(&x3[14], values[7]);
+	    vec_store2dpto2dp(&x3[0], values[0]);	   
+	    vec_store2dpto2dp(&x3[2], values[1]);
+	    vec_store2dpto2dp(&x3[4], values[2]);
+	    vec_store2dpto2dp(&x3[6], values[3]);
+	    vec_store2dpto2dp(&x3[8], values[4]);	   
+	    vec_store2dpto2dp(&x3[10], values[5]);
+	    vec_store2dpto2dp(&x3[12], values[6]);
+	    vec_store2dpto2dp(&x3[14], values[7]);
 	  }
       }
 
@@ -4325,7 +4325,7 @@
 	   }
 	 else
 	   {
-	     __m128d maxv =_mm_setzero_pd();	     	    
+	     __m128d maxv =vec_zero2dp();	     	    
 	     
 	     if(x1_gap[i / 32] & mask32[i % 32])
 	       x1 = x1_gapColumn;
@@ -4353,37 +4353,37 @@
 		 double *left_k2_p = &left[j*16 + 2*4];
 		 double *left_k3_p = &left[j*16 + 3*4];
 		 
-		 __m128d x1_0 = _mm_load_pd( &x1_p[0] );
-		 __m128d x1_2 = _mm_load_pd( &x1_p[2] );
+		 __m128d x1_0 = vec_load2dpaligned( &x1_p[0] );
+		 __m128d x1_2 = vec_load2dpaligned( &x1_p[2] );
 		 
-		 __m128d left_k0_0 = _mm_load_pd( &left_k0_p[0] );
-		 __m128d left_k0_2 = _mm_load_pd( &left_k0_p[2] );
-		 __m128d left_k1_0 = _mm_load_pd( &left_k1_p[0] );
-		 __m128d left_k1_2 = _mm_load_pd( &left_k1_p[2] );
-		 __m128d left_k2_0 = _mm_load_pd( &left_k2_p[0] );
-		 __m128d left_k2_2 = _mm_load_pd( &left_k2_p[2] );
-		 __m128d left_k3_0 = _mm_load_pd( &left_k3_p[0] );
-		 __m128d left_k3_2 = _mm_load_pd( &left_k3_p[2] );
+		 __m128d left_k0_0 = vec_load2dpaligned( &left_k0_p[0] );
+		 __m128d left_k0_2 = vec_load2dpaligned( &left_k0_p[2] );
+		 __m128d left_k1_0 = vec_load2dpaligned( &left_k1_p[0] );
+		 __m128d left_k1_2 = vec_load2dpaligned( &left_k1_p[2] );
+		 __m128d left_k2_0 = vec_load2dpaligned( &left_k2_p[0] );
+		 __m128d left_k2_2 = vec_load2dpaligned( &left_k2_p[2] );
+		 __m128d left_k3_0 = vec_load2dpaligned( &left_k3_p[0] );
+		 __m128d left_k3_2 = vec_load2dpaligned( &left_k3_p[2] );
 		 
-		 left_k0_0 = _mm_mul_pd(x1_0, left_k0_0);
-		 left_k0_2 = _mm_mul_pd(x1_2, left_k0_2);
+		 left_k0_0 = vec_multiply2dp(x1_0, left_k0_0);
+		 left_k0_2 = vec_multiply2dp(x1_2, left_k0_2);
 		 
-		 left_k1_0 = _mm_mul_pd(x1_0, left_k1_0);
-		 left_k1_2 = _mm_mul_pd(x1_2, left_k1_2);
+		 left_k1_0 = vec_multiply2dp(x1_0, left_k1_0);
+		 left_k1_2 = vec_multiply2dp(x1_2, left_k1_2);
 		 
-		 left_k0_0 = _mm_hadd_pd( left_k0_0, left_k0_2 );
-		 left_k1_0 = _mm_hadd_pd( left_k1_0, left_k1_2);
-		 left_k0_0 = _mm_hadd_pd( left_k0_0, left_k1_0);
+		 left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k0_2 );
+		 left_k1_0 = vec_horizontaladd2dp( left_k1_0, left_k1_2);
+		 left_k0_0 = vec_horizontaladd2dp( left_k0_0, left_k1_0);
 		 
-		 left_k2_0 = _mm_mul_pd(x1_0, left_k2_0);
-		 left_k2_2 = _mm_mul_pd(x1_2, left_k2_2);
+		 left_k2_0 = vec_multiply2dp(x1_0, left_k2_0);
+		 left_k2_2 = vec_multiply2dp(x1_2, left_k2_2);
 		 
-		 left_k3_0 = _mm_mul_pd(x1_0, left_k3_0);
-		 left_k3_2 = _mm_mul_pd(x1_2, left_k3_2);
+		 left_k3_0 = vec_multiply2dp(x1_0, left_k3_0);
+		 left_k3_2 = vec_multiply2dp(x1_2, left_k3_2);
 		 
-		 left_k2_0 = _mm_hadd_pd( left_k2_0, left_k2_2);
-		 left_k3_0 = _mm_hadd_pd( left_k3_0, left_k3_2);
-		 left_k2_0 = _mm_hadd_pd( left_k2_0, left_k3_0);
+		 left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k2_2);
+		 left_k3_0 = vec_horizontaladd2dp( left_k3_0, left_k3_2);
+		 left_k2_0 = vec_horizontaladd2dp( left_k2_0, left_k3_0);
 		 
 		 
 		 //
@@ -4394,45 +4394,45 @@
 		 double *right_k1_p = &right[j*16 + 1*4];
 		 double *right_k2_p = &right[j*16 + 2*4];
 		 double *right_k3_p = &right[j*16 + 3*4];
-		 __m128d x2_0 = _mm_load_pd( &x2_p[0] );
-		 __m128d x2_2 = _mm_load_pd( &x2_p[2] );
+		 __m128d x2_0 = vec_load2dpaligned( &x2_p[0] );
+		 __m128d x2_2 = vec_load2dpaligned( &x2_p[2] );
 		 
-		 __m128d right_k0_0 = _mm_load_pd( &right_k0_p[0] );
-		 __m128d right_k0_2 = _mm_load_pd( &right_k0_p[2] );
-		 __m128d right_k1_0 = _mm_load_pd( &right_k1_p[0] );
-		 __m128d right_k1_2 = _mm_load_pd( &right_k1_p[2] );
-		 __m128d right_k2_0 = _mm_load_pd( &right_k2_p[0] );
-		 __m128d right_k2_2 = _mm_load_pd( &right_k2_p[2] );
-		 __m128d right_k3_0 = _mm_load_pd( &right_k3_p[0] );
-		 __m128d right_k3_2 = _mm_load_pd( &right_k3_p[2] );
+		 __m128d right_k0_0 = vec_load2dpaligned( &right_k0_p[0] );
+		 __m128d right_k0_2 = vec_load2dpaligned( &right_k0_p[2] );
+		 __m128d right_k1_0 = vec_load2dpaligned( &right_k1_p[0] );
+		 __m128d right_k1_2 = vec_load2dpaligned( &right_k1_p[2] );
+		 __m128d right_k2_0 = vec_load2dpaligned( &right_k2_p[0] );
+		 __m128d right_k2_2 = vec_load2dpaligned( &right_k2_p[2] );
+		 __m128d right_k3_0 = vec_load2dpaligned( &right_k3_p[0] );
+		 __m128d right_k3_2 = vec_load2dpaligned( &right_k3_p[2] );
 		 
-		 right_k0_0 = _mm_mul_pd( x2_0, right_k0_0);
-		 right_k0_2 = _mm_mul_pd( x2_2, right_k0_2);
+		 right_k0_0 = vec_multiply2dp( x2_0, right_k0_0);
+		 right_k0_2 = vec_multiply2dp( x2_2, right_k0_2);
 		 
-		 right_k1_0 = _mm_mul_pd( x2_0, right_k1_0);
-		 right_k1_2 = _mm_mul_pd( x2_2, right_k1_2);
+		 right_k1_0 = vec_multiply2dp( x2_0, right_k1_0);
+		 right_k1_2 = vec_multiply2dp( x2_2, right_k1_2);
 		 
-		 right_k0_0 = _mm_hadd_pd( right_k0_0, right_k0_2);
-		 right_k1_0 = _mm_hadd_pd( right_k1_0, right_k1_2);
-		 right_k0_0 = _mm_hadd_pd( right_k0_0, right_k1_0);
+		 right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k0_2);
+		 right_k1_0 = vec_horizontaladd2dp( right_k1_0, right_k1_2);
+		 right_k0_0 = vec_horizontaladd2dp( right_k0_0, right_k1_0);
 		 
-		 right_k2_0 = _mm_mul_pd( x2_0, right_k2_0);
-		 right_k2_2 = _mm_mul_pd( x2_2, right_k2_2);
+		 right_k2_0 = vec_multiply2dp( x2_0, right_k2_0);
+		 right_k2_2 = vec_multiply2dp( x2_2, right_k2_2);
 		 
 		 
-		 right_k3_0 = _mm_mul_pd( x2_0, right_k3_0);
-		 right_k3_2 = _mm_mul_pd( x2_2, right_k3_2);
+		 right_k3_0 = vec_multiply2dp( x2_0, right_k3_0);
+		 right_k3_2 = vec_multiply2dp( x2_2, right_k3_2);
 		 
-		 right_k2_0 = _mm_hadd_pd( right_k2_0, right_k2_2);
-		 right_k3_0 = _mm_hadd_pd( right_k3_0, right_k3_2);
-		 right_k2_0 = _mm_hadd_pd( right_k2_0, right_k3_0);	   
+		 right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k2_2);
+		 right_k3_0 = vec_horizontaladd2dp( right_k3_0, right_k3_2);
+		 right_k2_0 = vec_horizontaladd2dp( right_k2_0, right_k3_0);	   
 		 
 		 //
 		 // multiply left * right
 		 //
 		 
-		 __m128d x1px2_k0 = _mm_mul_pd( left_k0_0, right_k0_0 );
-		 __m128d x1px2_k2 = _mm_mul_pd( left_k2_0, right_k2_0 );
+		 __m128d x1px2_k0 = vec_multiply2dp( left_k0_0, right_k0_0 );
+		 __m128d x1px2_k2 = vec_multiply2dp( left_k2_0, right_k2_0 );
 		 
 		 
 		 //
@@ -4449,52 +4449,52 @@
 		 __m128d EV_t_l3_k2 = EVV[7];
 		 
 		 
-		 EV_t_l0_k0 = _mm_mul_pd( x1px2_k0, EV_t_l0_k0 );
-		 EV_t_l0_k2 = _mm_mul_pd( x1px2_k2, EV_t_l0_k2 );
-		 EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l0_k2 );
+		 EV_t_l0_k0 = vec_multiply2dp( x1px2_k0, EV_t_l0_k0 );
+		 EV_t_l0_k2 = vec_multiply2dp( x1px2_k2, EV_t_l0_k2 );
+		 EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l0_k2 );
 		 
-		 EV_t_l1_k0 = _mm_mul_pd( x1px2_k0, EV_t_l1_k0 );
-		 EV_t_l1_k2 = _mm_mul_pd( x1px2_k2, EV_t_l1_k2 );
+		 EV_t_l1_k0 = vec_multiply2dp( x1px2_k0, EV_t_l1_k0 );
+		 EV_t_l1_k2 = vec_multiply2dp( x1px2_k2, EV_t_l1_k2 );
 		 
-		 EV_t_l1_k0 = _mm_hadd_pd( EV_t_l1_k0, EV_t_l1_k2 );
-		 EV_t_l0_k0 = _mm_hadd_pd( EV_t_l0_k0, EV_t_l1_k0 );
+		 EV_t_l1_k0 = vec_horizontaladd2dp( EV_t_l1_k0, EV_t_l1_k2 );
+		 EV_t_l0_k0 = vec_horizontaladd2dp( EV_t_l0_k0, EV_t_l1_k0 );
 		 
-		 EV_t_l2_k0 = _mm_mul_pd( x1px2_k0, EV_t_l2_k0 );
-		 EV_t_l2_k2 = _mm_mul_pd( x1px2_k2, EV_t_l2_k2 );
-		 EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l2_k2 );
+		 EV_t_l2_k0 = vec_multiply2dp( x1px2_k0, EV_t_l2_k0 );
+		 EV_t_l2_k2 = vec_multiply2dp( x1px2_k2, EV_t_l2_k2 );
+		 EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l2_k2 );
 		 
 		 
-		 EV_t_l3_k0 = _mm_mul_pd( x1px2_k0, EV_t_l3_k0 );
-		 EV_t_l3_k2 = _mm_mul_pd( x1px2_k2, EV_t_l3_k2 );
-		 EV_t_l3_k0 = _mm_hadd_pd( EV_t_l3_k0, EV_t_l3_k2 );
+		 EV_t_l3_k0 = vec_multiply2dp( x1px2_k0, EV_t_l3_k0 );
+		 EV_t_l3_k2 = vec_multiply2dp( x1px2_k2, EV_t_l3_k2 );
+		 EV_t_l3_k0 = vec_horizontaladd2dp( EV_t_l3_k0, EV_t_l3_k2 );
 		 
-		 EV_t_l2_k0 = _mm_hadd_pd( EV_t_l2_k0, EV_t_l3_k0 );
+		 EV_t_l2_k0 = vec_horizontaladd2dp( EV_t_l2_k0, EV_t_l3_k0 );
 		 
 		 
 		 values[j * 2] = EV_t_l0_k0;
 		 values[j * 2 + 1] = EV_t_l2_k0;            	   	    
 		 
-		 maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l0_k0, absMask.m));
-		 maxv = _mm_max_pd(maxv, _mm_and_pd(EV_t_l2_k0, absMask.m));
+		 maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l0_k0, absMask.m));
+		 maxv = vec_max2dp(maxv, vec_bitand2dp(EV_t_l2_k0, absMask.m));
 	       }
 	     
 	     
-	     _mm_store_pd(maxima, maxv);
+	     vec_store2dpto2dp(maxima, maxv);
 	     
 	     max = MAX(maxima[0], maxima[1]);
 	     
 	     if(max < minlikelihood)
 	       {
-		 __m128d sv = _mm_set1_pd(twotothe256);
+		 __m128d sv = vec_splat2dp(twotothe256);
 		 
-		 _mm_store_pd(&x3[0], _mm_mul_pd(values[0], sv));	   
-		 _mm_store_pd(&x3[2], _mm_mul_pd(values[1], sv));
-		 _mm_store_pd(&x3[4], _mm_mul_pd(values[2], sv));
-		 _mm_store_pd(&x3[6], _mm_mul_pd(values[3], sv));
-		 _mm_store_pd(&x3[8], _mm_mul_pd(values[4], sv));	   
-		 _mm_store_pd(&x3[10], _mm_mul_pd(values[5], sv));
-		 _mm_store_pd(&x3[12], _mm_mul_pd(values[6], sv));
-		 _mm_store_pd(&x3[14], _mm_mul_pd(values[7], sv));	     
+		 vec_store2dpto2dp(&x3[0], vec_multiply2dp(values[0], sv));	   
+		 vec_store2dpto2dp(&x3[2], vec_multiply2dp(values[1], sv));
+		 vec_store2dpto2dp(&x3[4], vec_multiply2dp(values[2], sv));
+		 vec_store2dpto2dp(&x3[6], vec_multiply2dp(values[3], sv));
+		 vec_store2dpto2dp(&x3[8], vec_multiply2dp(values[4], sv));	   
+		 vec_store2dpto2dp(&x3[10], vec_multiply2dp(values[5], sv));
+		 vec_store2dpto2dp(&x3[12], vec_multiply2dp(values[6], sv));
+		 vec_store2dpto2dp(&x3[14], vec_multiply2dp(values[7], sv));	     
 		 
 		 if(useFastScaling)
 		   addScale += wgt[i];
@@ -4503,14 +4503,14 @@
 	       }
 	     else
 	       {
-		 _mm_store_pd(&x3[0], values[0]);	   
-		 _mm_store_pd(&x3[2], values[1]);
-		 _mm_store_pd(&x3[4], values[2]);
-		 _mm_store_pd(&x3[6], values[3]);
-		 _mm_store_pd(&x3[8], values[4]);	   
-		 _mm_store_pd(&x3[10], values[5]);
-		 _mm_store_pd(&x3[12], values[6]);
-		 _mm_store_pd(&x3[14], values[7]);
+		 vec_store2dpto2dp(&x3[0], values[0]);	   
+		 vec_store2dpto2dp(&x3[2], values[1]);
+		 vec_store2dpto2dp(&x3[4], values[2]);
+		 vec_store2dpto2dp(&x3[6], values[3]);
+		 vec_store2dpto2dp(&x3[8], values[4]);	   
+		 vec_store2dpto2dp(&x3[10], values[5]);
+		 vec_store2dpto2dp(&x3[12], values[6]);
+		 vec_store2dpto2dp(&x3[14], values[7]);
 	       }	 
 
 	    
@@ -4757,7 +4757,7 @@
 #endif
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 static void newviewGTRCATPROT_SAVE(int tipCase, double *extEV,
     int *cptr,
     double *x1, double *x2, double *x3, double *tipVector,
@@ -4793,12 +4793,12 @@
     ri = &right[maxCats * 400];	  
 
     for(l = 0; l < 20; l+=2)
-      _mm_store_pd(&v[l], _mm_setzero_pd());	      		
+      vec_store2dpto2dp(&v[l], vec_zero2dp());	      		
 
     for(l = 0; l < 20; l++)
     {
-      __m128d x1v = _mm_setzero_pd();
-      __m128d x2v = _mm_setzero_pd();
+      __m128d x1v = vec_zero2dp();
+      __m128d x2v = vec_zero2dp();
       double 
         *ev = &extEV[l * 20],
         *lv = &le[l * 20],
@@ -4807,45 +4807,45 @@
 
       for(j = 0; j < 20; j+=2)
       {
-        x1v = _mm_add_pd(x1v, _mm_mul_pd(_mm_load_pd(&vl[j]), _mm_load_pd(&lv[j])));		    
-        x2v = _mm_add_pd(x2v, _mm_mul_pd(_mm_load_pd(&vr[j]), _mm_load_pd(&rv[j])));
+        x1v = vec_add2dp(x1v, vec_multiply2dp(vec_load2dpaligned(&vl[j]), vec_load2dpaligned(&lv[j])));		    
+        x2v = vec_add2dp(x2v, vec_multiply2dp(vec_load2dpaligned(&vr[j]), vec_load2dpaligned(&rv[j])));
       }
 
-      x1v = _mm_hadd_pd(x1v, x1v);
-      x2v = _mm_hadd_pd(x2v, x2v);
+      x1v = vec_horizontaladd2dp(x1v, x1v);
+      x2v = vec_horizontaladd2dp(x2v, x2v);
 
-      x1v = _mm_mul_pd(x1v, x2v);
+      x1v = vec_multiply2dp(x1v, x2v);
 
       for(j = 0; j < 20; j+=2)
       {
-        __m128d vv = _mm_load_pd(&v[j]);
-        vv = _mm_add_pd(vv, _mm_mul_pd(x1v, _mm_load_pd(&ev[j])));
-        _mm_store_pd(&v[j], vv);
+        __m128d vv = vec_load2dpaligned(&v[j]);
+        vv = vec_add2dp(vv, vec_multiply2dp(x1v, vec_load2dpaligned(&ev[j])));
+        vec_store2dpto2dp(&v[j], vv);
       }		    	
     }
 
     if(tipCase != TIP_TIP)
     { 	    
-      __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+      __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 
       scale = 1;
       for(l = 0; scale && (l < 20); l += 2)
       {
-        __m128d vv = _mm_load_pd(&v[l]);
-        __m128d v1 = _mm_and_pd(vv, absMask.m);
-        v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-        if(_mm_movemask_pd( v1 ) != 3)
+        __m128d vv = vec_load2dpaligned(&v[l]);
+        __m128d v1 = vec_bitand2dp(vv, absMask.m);
+        v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+        if(vec_extractupperbit2dp( v1 ) != 3)
           scale = 0;
       }	    	        
 
       if(scale)
       {
-        __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+        __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
         for(l = 0; l < 20; l+=2)
         {
-          __m128d ex3v = _mm_load_pd(&v[l]);		  
-          _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+          __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+          vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
         }		   		  
 
         scaleGap = TRUE;	   
@@ -4876,12 +4876,12 @@
               ri =  &right[cptr[i] * 400];
 
             for(l = 0; l < 20; l+=2)
-              _mm_store_pd(&v[l], _mm_setzero_pd());	      		
+              vec_store2dpto2dp(&v[l], vec_zero2dp());	      		
 
             for(l = 0; l < 20; l++)
             {
-              __m128d x1v = _mm_setzero_pd();
-              __m128d x2v = _mm_setzero_pd();	 
+              __m128d x1v = vec_zero2dp();
+              __m128d x2v = vec_zero2dp();	 
               double 
                 *ev = &extEV[l * 20],
                 *lv = &le[l * 20],
@@ -4889,20 +4889,20 @@
 
               for(j = 0; j < 20; j+=2)
               {
-                x1v = _mm_add_pd(x1v, _mm_mul_pd(_mm_load_pd(&vl[j]), _mm_load_pd(&lv[j])));		    
-                x2v = _mm_add_pd(x2v, _mm_mul_pd(_mm_load_pd(&vr[j]), _mm_load_pd(&rv[j])));
+                x1v = vec_add2dp(x1v, vec_multiply2dp(vec_load2dpaligned(&vl[j]), vec_load2dpaligned(&lv[j])));		    
+                x2v = vec_add2dp(x2v, vec_multiply2dp(vec_load2dpaligned(&vr[j]), vec_load2dpaligned(&rv[j])));
               }
 
-              x1v = _mm_hadd_pd(x1v, x1v);
-              x2v = _mm_hadd_pd(x2v, x2v);
+              x1v = vec_horizontaladd2dp(x1v, x1v);
+              x2v = vec_horizontaladd2dp(x2v, x2v);
 
-              x1v = _mm_mul_pd(x1v, x2v);
+              x1v = vec_multiply2dp(x1v, x2v);
 
               for(j = 0; j < 20; j+=2)
               {
-                __m128d vv = _mm_load_pd(&v[j]);
-                vv = _mm_add_pd(vv, _mm_mul_pd(x1v, _mm_load_pd(&ev[j])));
-                _mm_store_pd(&v[j], vv);
+                __m128d vv = vec_load2dpaligned(&v[j]);
+                vv = vec_add2dp(vv, vec_multiply2dp(x1v, vec_load2dpaligned(&ev[j])));
+                vec_store2dpto2dp(&v[j], vv);
               }		   
             }
 
@@ -4951,12 +4951,12 @@
             }	  	  	  	  		  
 
             for(l = 0; l < 20; l+=2)
-              _mm_store_pd(&v[l], _mm_setzero_pd());	      			   
+              vec_store2dpto2dp(&v[l], vec_zero2dp());	      			   
 
             for(l = 0; l < 20; l++)
             {
-              __m128d x1v = _mm_setzero_pd();
-              __m128d x2v = _mm_setzero_pd();	
+              __m128d x1v = vec_zero2dp();
+              __m128d x2v = vec_zero2dp();	
               double 
                 *ev = &extEV[l * 20],
                 *lv = &le[l * 20],
@@ -4964,33 +4964,33 @@
 
               for(j = 0; j < 20; j+=2)
               {
-                x1v = _mm_add_pd(x1v, _mm_mul_pd(_mm_load_pd(&vl[j]), _mm_load_pd(&lv[j])));		    
-                x2v = _mm_add_pd(x2v, _mm_mul_pd(_mm_load_pd(&vr[j]), _mm_load_pd(&rv[j])));
+                x1v = vec_add2dp(x1v, vec_multiply2dp(vec_load2dpaligned(&vl[j]), vec_load2dpaligned(&lv[j])));		    
+                x2v = vec_add2dp(x2v, vec_multiply2dp(vec_load2dpaligned(&vr[j]), vec_load2dpaligned(&rv[j])));
               }
 
-              x1v = _mm_hadd_pd(x1v, x1v);
-              x2v = _mm_hadd_pd(x2v, x2v);
+              x1v = vec_horizontaladd2dp(x1v, x1v);
+              x2v = vec_horizontaladd2dp(x2v, x2v);
 
-              x1v = _mm_mul_pd(x1v, x2v);
+              x1v = vec_multiply2dp(x1v, x2v);
 
               for(j = 0; j < 20; j+=2)
               {
-                __m128d vv = _mm_load_pd(&v[j]);
-                vv = _mm_add_pd(vv, _mm_mul_pd(x1v, _mm_load_pd(&ev[j])));
-                _mm_store_pd(&v[j], vv);
+                __m128d vv = vec_load2dpaligned(&v[j]);
+                vv = vec_add2dp(vv, vec_multiply2dp(x1v, vec_load2dpaligned(&ev[j])));
+                vec_store2dpto2dp(&v[j], vv);
               }		    
             }
 
             { 	    
-              __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+              __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 
               scale = 1;
               for(l = 0; scale && (l < 20); l += 2)
               {
-                __m128d vv = _mm_load_pd(&v[l]);
-                __m128d v1 = _mm_and_pd(vv, absMask.m);
-                v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-                if(_mm_movemask_pd( v1 ) != 3)
+                __m128d vv = vec_load2dpaligned(&v[l]);
+                __m128d v1 = vec_bitand2dp(vv, absMask.m);
+                v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+                if(vec_extractupperbit2dp( v1 ) != 3)
                   scale = 0;
               }	    	  
             }
@@ -4998,12 +4998,12 @@
 
             if(scale)
 	      {
-		__m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+		__m128d twoto = vec_set2dp(twotothe256, twotothe256);
 		
 		for(l = 0; l < 20; l+=2)
 		  {
-		    __m128d ex3v = _mm_load_pd(&v[l]);
-		    _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));		    
+		    __m128d ex3v = vec_load2dpaligned(&v[l]);
+		    vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));		    
 		  }
 		
 		if(useFastScaling)
@@ -5060,12 +5060,12 @@
           }	 	  	  	  
 
           for(l = 0; l < 20; l+=2)
-            _mm_store_pd(&v[l], _mm_setzero_pd());	      		
+            vec_store2dpto2dp(&v[l], vec_zero2dp());	      		
 
           for(l = 0; l < 20; l++)
           {
-            __m128d x1v = _mm_setzero_pd();
-            __m128d x2v = _mm_setzero_pd();
+            __m128d x1v = vec_zero2dp();
+            __m128d x2v = vec_zero2dp();
             double 
               *ev = &extEV[l * 20],
               *lv = &le[l * 20],
@@ -5073,46 +5073,46 @@
 
             for(j = 0; j < 20; j+=2)
             {
-              x1v = _mm_add_pd(x1v, _mm_mul_pd(_mm_load_pd(&vl[j]), _mm_load_pd(&lv[j])));		    
-              x2v = _mm_add_pd(x2v, _mm_mul_pd(_mm_load_pd(&vr[j]), _mm_load_pd(&rv[j])));
+              x1v = vec_add2dp(x1v, vec_multiply2dp(vec_load2dpaligned(&vl[j]), vec_load2dpaligned(&lv[j])));		    
+              x2v = vec_add2dp(x2v, vec_multiply2dp(vec_load2dpaligned(&vr[j]), vec_load2dpaligned(&rv[j])));
             }
 
-            x1v = _mm_hadd_pd(x1v, x1v);
-            x2v = _mm_hadd_pd(x2v, x2v);
+            x1v = vec_horizontaladd2dp(x1v, x1v);
+            x2v = vec_horizontaladd2dp(x2v, x2v);
 
-            x1v = _mm_mul_pd(x1v, x2v);
+            x1v = vec_multiply2dp(x1v, x2v);
 
             for(j = 0; j < 20; j+=2)
             {
-              __m128d vv = _mm_load_pd(&v[j]);
-              vv = _mm_add_pd(vv, _mm_mul_pd(x1v, _mm_load_pd(&ev[j])));
-              _mm_store_pd(&v[j], vv);
+              __m128d vv = vec_load2dpaligned(&v[j]);
+              vv = vec_add2dp(vv, vec_multiply2dp(x1v, vec_load2dpaligned(&ev[j])));
+              vec_store2dpto2dp(&v[j], vv);
             }		    
 
           }
 
           { 	    
-            __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+            __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 
             scale = 1;
             for(l = 0; scale && (l < 20); l += 2)
             {
-              __m128d vv = _mm_load_pd(&v[l]);
-              __m128d v1 = _mm_and_pd(vv, absMask.m);
-              v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-              if(_mm_movemask_pd( v1 ) != 3)
+              __m128d vv = vec_load2dpaligned(&v[l]);
+              __m128d v1 = vec_bitand2dp(vv, absMask.m);
+              v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+              if(vec_extractupperbit2dp( v1 ) != 3)
                 scale = 0;
             }	    	  
           }
 
           if(scale)
           {
-            __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+            __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
             for(l = 0; l < 20; l+=2)
             {
-              __m128d ex3v = _mm_load_pd(&v[l]);		  
-              _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+              __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+              vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
             }		   		  
 	    
 	    if(useFastScaling)
@@ -5144,7 +5144,7 @@
 {
   double
     *le, *ri, *v, *vl, *vr;
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
   double
     ump_x1, ump_x2, x1px2;
 #endif
@@ -5162,9 +5162,9 @@
 	    vl = &(tipVector[20 * tipX1[i]]);
 	    vr = &(tipVector[20 * tipX2[i]]);
 	    v  = &x3[20 * i];
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	    for(l = 0; l < 20; l+=2)
-	      _mm_store_pd(&v[l], _mm_setzero_pd());	      		
+	      vec_store2dpto2dp(&v[l], vec_zero2dp());	      		
 #else
 	    for(l = 0; l < 20; l++)
 	      v[l] = 0.0;
@@ -5172,9 +5172,9 @@
 
 	    for(l = 0; l < 20; l++)
 	      {
-#ifdef __SIM_SSE3
-		__m128d x1v = _mm_setzero_pd();
-		__m128d x2v = _mm_setzero_pd();	 
+#ifdef __SIM_VECLIB
+		__m128d x1v = vec_zero2dp();
+		__m128d x2v = vec_zero2dp();	 
 		double 
 		  *ev = &extEV[l * 20],
 		  *lv = &le[l * 20],
@@ -5182,20 +5182,20 @@
 
 		for(j = 0; j < 20; j+=2)
 		  {
-		    x1v = _mm_add_pd(x1v, _mm_mul_pd(_mm_load_pd(&vl[j]), _mm_load_pd(&lv[j])));		    
-		    x2v = _mm_add_pd(x2v, _mm_mul_pd(_mm_load_pd(&vr[j]), _mm_load_pd(&rv[j])));
+		    x1v = vec_add2dp(x1v, vec_multiply2dp(vec_load2dpaligned(&vl[j]), vec_load2dpaligned(&lv[j])));		    
+		    x2v = vec_add2dp(x2v, vec_multiply2dp(vec_load2dpaligned(&vr[j]), vec_load2dpaligned(&rv[j])));
 		  }
 
-		x1v = _mm_hadd_pd(x1v, x1v);
-		x2v = _mm_hadd_pd(x2v, x2v);
+		x1v = vec_horizontaladd2dp(x1v, x1v);
+		x2v = vec_horizontaladd2dp(x2v, x2v);
 
-		x1v = _mm_mul_pd(x1v, x2v);
+		x1v = vec_multiply2dp(x1v, x2v);
 		
 		for(j = 0; j < 20; j+=2)
 		  {
-		    __m128d vv = _mm_load_pd(&v[j]);
-		    vv = _mm_add_pd(vv, _mm_mul_pd(x1v, _mm_load_pd(&ev[j])));
-		    _mm_store_pd(&v[j], vv);
+		    __m128d vv = vec_load2dpaligned(&v[j]);
+		    vv = vec_add2dp(vv, vec_multiply2dp(x1v, vec_load2dpaligned(&ev[j])));
+		    vec_store2dpto2dp(&v[j], vv);
 		  }		    
 #else
 		ump_x1 = 0.0;
@@ -5227,9 +5227,9 @@
 	    vr = &x2[20 * i];
 	    v  = &x3[20 * i];
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	    for(l = 0; l < 20; l+=2)
-	      _mm_store_pd(&v[l], _mm_setzero_pd());	      		
+	      vec_store2dpto2dp(&v[l], vec_zero2dp());	      		
 #else
 	    for(l = 0; l < 20; l++)
 	      v[l] = 0.0;
@@ -5238,10 +5238,10 @@
 
 	    for(l = 0; l < 20; l++)
 	      {
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
-		__m128d x1v = _mm_setzero_pd();
-		__m128d x2v = _mm_setzero_pd();	
+		__m128d x1v = vec_zero2dp();
+		__m128d x2v = vec_zero2dp();	
 		double 
 		  *ev = &extEV[l * 20],
 		  *lv = &le[l * 20],
@@ -5249,20 +5249,20 @@
 
 		for(j = 0; j < 20; j+=2)
 		  {
-		    x1v = _mm_add_pd(x1v, _mm_mul_pd(_mm_load_pd(&vl[j]), _mm_load_pd(&lv[j])));		    
-		    x2v = _mm_add_pd(x2v, _mm_mul_pd(_mm_load_pd(&vr[j]), _mm_load_pd(&rv[j])));
+		    x1v = vec_add2dp(x1v, vec_multiply2dp(vec_load2dpaligned(&vl[j]), vec_load2dpaligned(&lv[j])));		    
+		    x2v = vec_add2dp(x2v, vec_multiply2dp(vec_load2dpaligned(&vr[j]), vec_load2dpaligned(&rv[j])));
 		  }
 
-		x1v = _mm_hadd_pd(x1v, x1v);
-		x2v = _mm_hadd_pd(x2v, x2v);
+		x1v = vec_horizontaladd2dp(x1v, x1v);
+		x2v = vec_horizontaladd2dp(x2v, x2v);
 
-		x1v = _mm_mul_pd(x1v, x2v);
+		x1v = vec_multiply2dp(x1v, x2v);
 		
 		for(j = 0; j < 20; j+=2)
 		  {
-		    __m128d vv = _mm_load_pd(&v[j]);
-		    vv = _mm_add_pd(vv, _mm_mul_pd(x1v, _mm_load_pd(&ev[j])));
-		    _mm_store_pd(&v[j], vv);
+		    __m128d vv = vec_load2dpaligned(&v[j]);
+		    vv = vec_add2dp(vv, vec_multiply2dp(x1v, vec_load2dpaligned(&ev[j])));
+		    vec_store2dpto2dp(&v[j], vv);
 		  }		    
 #else
 		ump_x1 = 0.0;
@@ -5280,17 +5280,17 @@
 		  v[j] += x1px2 * extEV[l * 20 + j];
 #endif
 	      }
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	    { 	    
-	      __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	      __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	      
 	      scale = 1;
 	      for(l = 0; scale && (l < 20); l += 2)
 		{
-		  __m128d vv = _mm_load_pd(&v[l]);
-		  __m128d v1 = _mm_and_pd(vv, absMask.m);
-		  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-		  if(_mm_movemask_pd( v1 ) != 3)
+		  __m128d vv = vec_load2dpaligned(&v[l]);
+		  __m128d v1 = vec_bitand2dp(vv, absMask.m);
+		  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+		  if(vec_extractupperbit2dp( v1 ) != 3)
 		    scale = 0;
 		}	    	  
 	    }
@@ -5302,13 +5302,13 @@
 
 	    if(scale)
 	      {
-#ifdef __SIM_SSE3
-		__m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+#ifdef __SIM_VECLIB
+		__m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
 		for(l = 0; l < 20; l+=2)
 		  {
-		    __m128d ex3v = _mm_load_pd(&v[l]);
-		    _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));		    
+		    __m128d ex3v = vec_load2dpaligned(&v[l]);
+		    vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));		    
 		  }
 #else
 		for(l = 0; l < 20; l++)
@@ -5333,9 +5333,9 @@
 	  vr = &x2[20 * i];
 	  v = &x3[20 * i];
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	    for(l = 0; l < 20; l+=2)
-	      _mm_store_pd(&v[l], _mm_setzero_pd());	      		
+	      vec_store2dpto2dp(&v[l], vec_zero2dp());	      		
 #else
 	    for(l = 0; l < 20; l++)
 	      v[l] = 0.0;
@@ -5343,9 +5343,9 @@
 	 
 	  for(l = 0; l < 20; l++)
 	    {
-#ifdef __SIM_SSE3
-		__m128d x1v = _mm_setzero_pd();
-		__m128d x2v = _mm_setzero_pd();
+#ifdef __SIM_VECLIB
+		__m128d x1v = vec_zero2dp();
+		__m128d x2v = vec_zero2dp();
 		double 
 		  *ev = &extEV[l * 20],
 		  *lv = &le[l * 20],
@@ -5354,20 +5354,20 @@
 
 		for(j = 0; j < 20; j+=2)
 		  {
-		    x1v = _mm_add_pd(x1v, _mm_mul_pd(_mm_load_pd(&vl[j]), _mm_load_pd(&lv[j])));		    
-		    x2v = _mm_add_pd(x2v, _mm_mul_pd(_mm_load_pd(&vr[j]), _mm_load_pd(&rv[j])));
+		    x1v = vec_add2dp(x1v, vec_multiply2dp(vec_load2dpaligned(&vl[j]), vec_load2dpaligned(&lv[j])));		    
+		    x2v = vec_add2dp(x2v, vec_multiply2dp(vec_load2dpaligned(&vr[j]), vec_load2dpaligned(&rv[j])));
 		  }
 
-		x1v = _mm_hadd_pd(x1v, x1v);
-		x2v = _mm_hadd_pd(x2v, x2v);
+		x1v = vec_horizontaladd2dp(x1v, x1v);
+		x2v = vec_horizontaladd2dp(x2v, x2v);
 
-		x1v = _mm_mul_pd(x1v, x2v);
+		x1v = vec_multiply2dp(x1v, x2v);
 		
 		for(j = 0; j < 20; j+=2)
 		  {
-		    __m128d vv = _mm_load_pd(&v[j]);
-		    vv = _mm_add_pd(vv, _mm_mul_pd(x1v, _mm_load_pd(&ev[j])));
-		    _mm_store_pd(&v[j], vv);
+		    __m128d vv = vec_load2dpaligned(&v[j]);
+		    vv = vec_add2dp(vv, vec_multiply2dp(x1v, vec_load2dpaligned(&ev[j])));
+		    vec_store2dpto2dp(&v[j], vv);
 		  }		    
 #else
 	      ump_x1 = 0.0;
@@ -5385,17 +5385,17 @@
 		v[j] += x1px2 * extEV[l * 20 + j];
 #endif
 	    }
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	    { 	    
-	      __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	      __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	      
 	      scale = 1;
 	      for(l = 0; scale && (l < 20); l += 2)
 		{
-		  __m128d vv = _mm_load_pd(&v[l]);
-		  __m128d v1 = _mm_and_pd(vv, absMask.m);
-		  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-		  if(_mm_movemask_pd( v1 ) != 3)
+		  __m128d vv = vec_load2dpaligned(&v[l]);
+		  __m128d v1 = vec_bitand2dp(vv, absMask.m);
+		  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+		  if(vec_extractupperbit2dp( v1 ) != 3)
 		    scale = 0;
 		}	    	  
 	    }
@@ -5407,13 +5407,13 @@
 
 	   if(scale)
 	     {
-#ifdef __SIM_SSE3
-	       __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+#ifdef __SIM_VECLIB
+	       __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	       
 	       for(l = 0; l < 20; l+=2)
 		 {
-		   __m128d ex3v = _mm_load_pd(&v[l]);		  
-		   _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+		   __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+		   vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
 		 }		   		  
 #else
 	       for(l = 0; l < 20; l++)
@@ -5904,7 +5904,7 @@
   double x1px2;
   int  i, j, l, k, scale, addScale = 0;
   double *vl, *vr;
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
   double al, ar;
 #endif
 
@@ -5922,25 +5922,25 @@
 
 	    for(k = 0; k < 80; k++)
 	      {
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		double *ll =  &left[k * 20];
 		double *rr =  &right[k * 20];
 		
-		__m128d umpX1v = _mm_setzero_pd();
-		__m128d umpX2v = _mm_setzero_pd();
+		__m128d umpX1v = vec_zero2dp();
+		__m128d umpX2v = vec_zero2dp();
 
 		for(l = 0; l < 20; l+=2)
 		  {
-		    __m128d vv = _mm_load_pd(&v[l]);
-		    umpX1v = _mm_add_pd(umpX1v, _mm_mul_pd(vv, _mm_load_pd(&ll[l])));
-		    umpX2v = _mm_add_pd(umpX2v, _mm_mul_pd(vv, _mm_load_pd(&rr[l])));					
+		    __m128d vv = vec_load2dpaligned(&v[l]);
+		    umpX1v = vec_add2dp(umpX1v, vec_multiply2dp(vv, vec_load2dpaligned(&ll[l])));
+		    umpX2v = vec_add2dp(umpX2v, vec_multiply2dp(vv, vec_load2dpaligned(&rr[l])));					
 		  }
 		
-		umpX1v = _mm_hadd_pd(umpX1v, umpX1v);
-		umpX2v = _mm_hadd_pd(umpX2v, umpX2v);
+		umpX1v = vec_horizontaladd2dp(umpX1v, umpX1v);
+		umpX2v = vec_horizontaladd2dp(umpX2v, umpX2v);
 		
-		_mm_storel_pd(&umpX1[80 * i + k], umpX1v);
-		_mm_storel_pd(&umpX2[80 * i + k], umpX2v);
+		vec_storelower1dpof2dp(&umpX1[80 * i + k], umpX1v);
+		vec_storelower1dpof2dp(&umpX2[80 * i + k], umpX2v);
 #else
 		umpX1[80 * i + k] = 0.0;
 		umpX2[80 * i + k] = 0.0;
@@ -5963,25 +5963,25 @@
 	      {
 		v = &x3[i * 80 + j * 20];
 
-#ifdef __SIM_SSE3
-		__m128d zero =  _mm_setzero_pd();
+#ifdef __SIM_VECLIB
+		__m128d zero =  vec_zero2dp();
 		for(k = 0; k < 20; k+=2)		  		    
-		  _mm_store_pd(&v[k], zero);
+		  vec_store2dpto2dp(&v[k], zero);
 
 		for(k = 0; k < 20; k++)
 		  { 
 		    double *eev = &extEV[k * 20];
 		    x1px2 = uX1[j * 20 + k] * uX2[j * 20 + k];
-		    __m128d x1px2v = _mm_set1_pd(x1px2);
+		    __m128d x1px2v = vec_splat2dp(x1px2);
 
 		    for(l = 0; l < 20; l+=2)
 		      {
-		      	__m128d vv = _mm_load_pd(&v[l]);
-			__m128d ee = _mm_load_pd(&eev[l]);
+		      	__m128d vv = vec_load2dpaligned(&v[l]);
+			__m128d ee = vec_load2dpaligned(&eev[l]);
 
-			vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+			vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 			
-			_mm_store_pd(&v[l], vv);
+			vec_store2dpto2dp(&v[l], vv);
 		      }
 		  }
 
@@ -6013,19 +6013,19 @@
 
 	    for(k = 0; k < 80; k++)
 	      {
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		double *ll =  &left[k * 20];
 				
-		__m128d umpX1v = _mm_setzero_pd();
+		__m128d umpX1v = vec_zero2dp();
 		
 		for(l = 0; l < 20; l+=2)
 		  {
-		    __m128d vv = _mm_load_pd(&v[l]);
-		    umpX1v = _mm_add_pd(umpX1v, _mm_mul_pd(vv, _mm_load_pd(&ll[l])));		    					
+		    __m128d vv = vec_load2dpaligned(&v[l]);
+		    umpX1v = vec_add2dp(umpX1v, vec_multiply2dp(vv, vec_load2dpaligned(&ll[l])));		    					
 		  }
 		
-		umpX1v = _mm_hadd_pd(umpX1v, umpX1v);				
-		_mm_storel_pd(&umpX1[80 * i + k], umpX1v);		
+		umpX1v = vec_horizontaladd2dp(umpX1v, umpX1v);				
+		vec_storelower1dpof2dp(&umpX1[80 * i + k], umpX1v);		
 #else	    
 		umpX1[80 * i + k] = 0.0;
 
@@ -6043,44 +6043,44 @@
 	    for(k = 0; k < 4; k++)
 	      {
 		v = &(x2[80 * i + k * 20]);
-#ifdef __SIM_SSE3	       
+#ifdef __SIM_VECLIB	       
 		for(l = 0; l < 20; l++)
 		  {		   
 		    double *r =  &right[k * 400 + l * 20];
-		    __m128d ump_x2v = _mm_setzero_pd();	    
+		    __m128d ump_x2v = vec_zero2dp();	    
 		    
 		    for(j = 0; j < 20; j+= 2)
 		      {
-			__m128d vv = _mm_load_pd(&v[j]);
-			__m128d rr = _mm_load_pd(&r[j]);
-			ump_x2v = _mm_add_pd(ump_x2v, _mm_mul_pd(vv, rr));
+			__m128d vv = vec_load2dpaligned(&v[j]);
+			__m128d rr = vec_load2dpaligned(&r[j]);
+			ump_x2v = vec_add2dp(ump_x2v, vec_multiply2dp(vv, rr));
 		      }
 		     
-		    ump_x2v = _mm_hadd_pd(ump_x2v, ump_x2v);
+		    ump_x2v = vec_horizontaladd2dp(ump_x2v, ump_x2v);
 		    
-		    _mm_storel_pd(&ump_x2[l], ump_x2v);		   		     
+		    vec_storelower1dpof2dp(&ump_x2[l], ump_x2v);		   		     
 		  }
 
 		v = &(x3[80 * i + 20 * k]);
 
-		__m128d zero =  _mm_setzero_pd();
+		__m128d zero =  vec_zero2dp();
 		for(l = 0; l < 20; l+=2)		  		    
-		  _mm_store_pd(&v[l], zero);
+		  vec_store2dpto2dp(&v[l], zero);
 		  
 		for(l = 0; l < 20; l++)
 		  {
 		    double *eev = &extEV[l * 20];
 		    x1px2 = uX1[k * 20 + l]  * ump_x2[l];
-		    __m128d x1px2v = _mm_set1_pd(x1px2);
+		    __m128d x1px2v = vec_splat2dp(x1px2);
 		  
 		    for(j = 0; j < 20; j+=2)
 		      {
-			__m128d vv = _mm_load_pd(&v[j]);
-			__m128d ee = _mm_load_pd(&eev[j]);
+			__m128d vv = vec_load2dpaligned(&v[j]);
+			__m128d ee = vec_load2dpaligned(&eev[j]);
 			
-			vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+			vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 			
-			_mm_store_pd(&v[j], vv);
+			vec_store2dpto2dp(&v[j], vv);
 		      }		     		    
 		  }			
 #else
@@ -6106,18 +6106,18 @@
 #endif
 	      }
 	   
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	    { 
 	      v = &(x3[80 * i]);
-	      __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	      __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	      
 	      scale = 1;
 	      for(l = 0; scale && (l < 80); l += 2)
 		{
-		  __m128d vv = _mm_load_pd(&v[l]);
-		  __m128d v1 = _mm_and_pd(vv, absMask.m);
-		  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-		  if(_mm_movemask_pd( v1 ) != 3)
+		  __m128d vv = vec_load2dpaligned(&v[l]);
+		  __m128d v1 = vec_bitand2dp(vv, absMask.m);
+		  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+		  if(vec_extractupperbit2dp( v1 ) != 3)
 		    scale = 0;
 		}	    	  
 	    }
@@ -6130,13 +6130,13 @@
 
 	    if (scale)
 	      {
-#ifdef __SIM_SSE3
-	       __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+#ifdef __SIM_VECLIB
+	       __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	       
 	       for(l = 0; l < 80; l+=2)
 		 {
-		   __m128d ex3v = _mm_load_pd(&v[l]);		  
-		   _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+		   __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+		   vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
 		 }		   		  
 #else
 		for(l = 0; l < 80; l++)
@@ -6160,10 +6160,10 @@
 	     vr = &(x2[80 * i + 20 * k]);
 	     v =  &(x3[80 * i + 20 * k]);
 
-#ifdef __SIM_SSE3
-	     __m128d zero =  _mm_setzero_pd();
+#ifdef __SIM_VECLIB
+	     __m128d zero =  vec_zero2dp();
 	     for(l = 0; l < 20; l+=2)		  		    
-	       _mm_store_pd(&v[l], zero);
+	       vec_store2dpto2dp(&v[l], zero);
 #else
 	     for(l = 0; l < 20; l++)
 	       v[l] = 0;
@@ -6171,10 +6171,10 @@
 
 	     for(l = 0; l < 20; l++)
 	       {		 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		 {
-		   __m128d al = _mm_setzero_pd();
-		   __m128d ar = _mm_setzero_pd();
+		   __m128d al = vec_zero2dp();
+		   __m128d ar = vec_zero2dp();
 
 		   double *ll   = &left[k * 400 + l * 20];
 		   double *rr   = &right[k * 400 + l * 20];
@@ -6182,28 +6182,28 @@
 		   
 		   for(j = 0; j < 20; j+=2)
 		     {
-		       __m128d lv  = _mm_load_pd(&ll[j]);
-		       __m128d rv  = _mm_load_pd(&rr[j]);
-		       __m128d vll = _mm_load_pd(&vl[j]);
-		       __m128d vrr = _mm_load_pd(&vr[j]);
+		       __m128d lv  = vec_load2dpaligned(&ll[j]);
+		       __m128d rv  = vec_load2dpaligned(&rr[j]);
+		       __m128d vll = vec_load2dpaligned(&vl[j]);
+		       __m128d vrr = vec_load2dpaligned(&vr[j]);
 		       
-		       al = _mm_add_pd(al, _mm_mul_pd(vll, lv));
-		       ar = _mm_add_pd(ar, _mm_mul_pd(vrr, rv));
+		       al = vec_add2dp(al, vec_multiply2dp(vll, lv));
+		       ar = vec_add2dp(ar, vec_multiply2dp(vrr, rv));
 		     }  		 
 		       
-		   al = _mm_hadd_pd(al, al);
-		   ar = _mm_hadd_pd(ar, ar);
+		   al = vec_horizontaladd2dp(al, al);
+		   ar = vec_horizontaladd2dp(ar, ar);
 		   
-		   al = _mm_mul_pd(al, ar);
+		   al = vec_multiply2dp(al, ar);
 
 		   for(j = 0; j < 20; j+=2)
 		     {
-		       __m128d vv  = _mm_load_pd(&v[j]);
-		       __m128d EVV = _mm_load_pd(&EVEV[j]);
+		       __m128d vv  = vec_load2dpaligned(&v[j]);
+		       __m128d EVV = vec_load2dpaligned(&EVEV[j]);
 
-		       vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+		       vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 
-		       _mm_store_pd(&v[j], vv);
+		       vec_store2dpto2dp(&v[j], vv);
 		     }		  		   		  
 		 }		 
 #else
@@ -6225,18 +6225,18 @@
 	   }
 	 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	 { 
 	   v = &(x3[80 * i]);
-	   __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	   __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	   
 	   scale = 1;
 	   for(l = 0; scale && (l < 80); l += 2)
 	     {
-	       __m128d vv = _mm_load_pd(&v[l]);
-	       __m128d v1 = _mm_and_pd(vv, absMask.m);
-	       v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	       if(_mm_movemask_pd( v1 ) != 3)
+	       __m128d vv = vec_load2dpaligned(&v[l]);
+	       __m128d v1 = vec_bitand2dp(vv, absMask.m);
+	       v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	       if(vec_extractupperbit2dp( v1 ) != 3)
 		 scale = 0;
 	     }	    	  
 	 }
@@ -6249,13 +6249,13 @@
 
 	 if (scale)
 	   {
-#ifdef __SIM_SSE3
-	       __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+#ifdef __SIM_VECLIB
+	       __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	       
 	       for(l = 0; l < 80; l+=2)
 		 {
-		   __m128d ex3v = _mm_load_pd(&v[l]);		  
-		   _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+		   __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+		   vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
 		 }		   		  
 #else	     
 	     for(l = 0; l < 80; l++)
@@ -6287,7 +6287,7 @@
   double x1px2;
   int  i, j, l, k, scale, addScale = 0;
   double *vl, *vr;
-#ifndef __SIM_SSE3
+#ifndef __SIM_VECLIB
   double al, ar;
 #endif
 
@@ -6307,25 +6307,25 @@
 	      {
 		
 		v = &(tipVector[k / 20][20 * i]);
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		double *ll =  &left[k * 20];
 		double *rr =  &right[k * 20];
 		
-		__m128d umpX1v = _mm_setzero_pd();
-		__m128d umpX2v = _mm_setzero_pd();
+		__m128d umpX1v = vec_zero2dp();
+		__m128d umpX2v = vec_zero2dp();
 
 		for(l = 0; l < 20; l+=2)
 		  {
-		    __m128d vv = _mm_load_pd(&v[l]);
-		    umpX1v = _mm_add_pd(umpX1v, _mm_mul_pd(vv, _mm_load_pd(&ll[l])));
-		    umpX2v = _mm_add_pd(umpX2v, _mm_mul_pd(vv, _mm_load_pd(&rr[l])));					
+		    __m128d vv = vec_load2dpaligned(&v[l]);
+		    umpX1v = vec_add2dp(umpX1v, vec_multiply2dp(vv, vec_load2dpaligned(&ll[l])));
+		    umpX2v = vec_add2dp(umpX2v, vec_multiply2dp(vv, vec_load2dpaligned(&rr[l])));					
 		  }
 		
-		umpX1v = _mm_hadd_pd(umpX1v, umpX1v);
-		umpX2v = _mm_hadd_pd(umpX2v, umpX2v);
+		umpX1v = vec_horizontaladd2dp(umpX1v, umpX1v);
+		umpX2v = vec_horizontaladd2dp(umpX2v, umpX2v);
 		
-		_mm_storel_pd(&umpX1[80 * i + k], umpX1v);
-		_mm_storel_pd(&umpX2[80 * i + k], umpX2v);
+		vec_storelower1dpof2dp(&umpX1[80 * i + k], umpX1v);
+		vec_storelower1dpof2dp(&umpX2[80 * i + k], umpX2v);
 #else
 		umpX1[80 * i + k] = 0.0;
 		umpX2[80 * i + k] = 0.0;
@@ -6348,25 +6348,25 @@
 	      {
 		v = &x3[i * 80 + j * 20];
 
-#ifdef __SIM_SSE3
-		__m128d zero =  _mm_setzero_pd();
+#ifdef __SIM_VECLIB
+		__m128d zero =  vec_zero2dp();
 		for(k = 0; k < 20; k+=2)		  		    
-		  _mm_store_pd(&v[k], zero);
+		  vec_store2dpto2dp(&v[k], zero);
 
 		for(k = 0; k < 20; k++)
 		  { 
 		    double *eev = &extEV[j][k * 20];
 		    x1px2 = uX1[j * 20 + k] * uX2[j * 20 + k];
-		    __m128d x1px2v = _mm_set1_pd(x1px2);
+		    __m128d x1px2v = vec_splat2dp(x1px2);
 
 		    for(l = 0; l < 20; l+=2)
 		      {
-		      	__m128d vv = _mm_load_pd(&v[l]);
-			__m128d ee = _mm_load_pd(&eev[l]);
+		      	__m128d vv = vec_load2dpaligned(&v[l]);
+			__m128d ee = vec_load2dpaligned(&eev[l]);
 
-			vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+			vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 			
-			_mm_store_pd(&v[l], vv);
+			vec_store2dpto2dp(&v[l], vv);
 		      }
 		  }
 
@@ -6399,19 +6399,19 @@
 	    for(k = 0; k < 80; k++)
 	      { 
 		v = &(tipVector[k / 20][20 * i]);
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		double *ll =  &left[k * 20];
 				
-		__m128d umpX1v = _mm_setzero_pd();
+		__m128d umpX1v = vec_zero2dp();
 		
 		for(l = 0; l < 20; l+=2)
 		  {
-		    __m128d vv = _mm_load_pd(&v[l]);
-		    umpX1v = _mm_add_pd(umpX1v, _mm_mul_pd(vv, _mm_load_pd(&ll[l])));		    					
+		    __m128d vv = vec_load2dpaligned(&v[l]);
+		    umpX1v = vec_add2dp(umpX1v, vec_multiply2dp(vv, vec_load2dpaligned(&ll[l])));		    					
 		  }
 		
-		umpX1v = _mm_hadd_pd(umpX1v, umpX1v);				
-		_mm_storel_pd(&umpX1[80 * i + k], umpX1v);		
+		umpX1v = vec_horizontaladd2dp(umpX1v, umpX1v);				
+		vec_storelower1dpof2dp(&umpX1[80 * i + k], umpX1v);		
 #else	    
 		umpX1[80 * i + k] = 0.0;
 
@@ -6429,44 +6429,44 @@
 	    for(k = 0; k < 4; k++)
 	      {
 		v = &(x2[80 * i + k * 20]);
-#ifdef __SIM_SSE3	       
+#ifdef __SIM_VECLIB	       
 		for(l = 0; l < 20; l++)
 		  {		   
 		    double *r =  &right[k * 400 + l * 20];
-		    __m128d ump_x2v = _mm_setzero_pd();	    
+		    __m128d ump_x2v = vec_zero2dp();	    
 		    
 		    for(j = 0; j < 20; j+= 2)
 		      {
-			__m128d vv = _mm_load_pd(&v[j]);
-			__m128d rr = _mm_load_pd(&r[j]);
-			ump_x2v = _mm_add_pd(ump_x2v, _mm_mul_pd(vv, rr));
+			__m128d vv = vec_load2dpaligned(&v[j]);
+			__m128d rr = vec_load2dpaligned(&r[j]);
+			ump_x2v = vec_add2dp(ump_x2v, vec_multiply2dp(vv, rr));
 		      }
 		     
-		    ump_x2v = _mm_hadd_pd(ump_x2v, ump_x2v);
+		    ump_x2v = vec_horizontaladd2dp(ump_x2v, ump_x2v);
 		    
-		    _mm_storel_pd(&ump_x2[l], ump_x2v);		   		     
+		    vec_storelower1dpof2dp(&ump_x2[l], ump_x2v);		   		     
 		  }
 
 		v = &(x3[80 * i + 20 * k]);
 
-		__m128d zero =  _mm_setzero_pd();
+		__m128d zero =  vec_zero2dp();
 		for(l = 0; l < 20; l+=2)		  		    
-		  _mm_store_pd(&v[l], zero);
+		  vec_store2dpto2dp(&v[l], zero);
 		  
 		for(l = 0; l < 20; l++)
 		  {
 		    double *eev = &extEV[k][l * 20];
 		    x1px2 = uX1[k * 20 + l]  * ump_x2[l];
-		    __m128d x1px2v = _mm_set1_pd(x1px2);
+		    __m128d x1px2v = vec_splat2dp(x1px2);
 		  
 		    for(j = 0; j < 20; j+=2)
 		      {
-			__m128d vv = _mm_load_pd(&v[j]);
-			__m128d ee = _mm_load_pd(&eev[j]);
+			__m128d vv = vec_load2dpaligned(&v[j]);
+			__m128d ee = vec_load2dpaligned(&eev[j]);
 			
-			vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+			vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 			
-			_mm_store_pd(&v[j], vv);
+			vec_store2dpto2dp(&v[j], vv);
 		      }		     		    
 		  }			
 #else
@@ -6492,18 +6492,18 @@
 #endif
 	      }
 	   
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	    { 
 	      v = &(x3[80 * i]);
-	      __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	      __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	      
 	      scale = 1;
 	      for(l = 0; scale && (l < 80); l += 2)
 		{
-		  __m128d vv = _mm_load_pd(&v[l]);
-		  __m128d v1 = _mm_and_pd(vv, absMask.m);
-		  v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-		  if(_mm_movemask_pd( v1 ) != 3)
+		  __m128d vv = vec_load2dpaligned(&v[l]);
+		  __m128d v1 = vec_bitand2dp(vv, absMask.m);
+		  v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+		  if(vec_extractupperbit2dp( v1 ) != 3)
 		    scale = 0;
 		}	    	  
 	    }
@@ -6516,13 +6516,13 @@
 
 	    if (scale)
 	      {
-#ifdef __SIM_SSE3
-	       __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+#ifdef __SIM_VECLIB
+	       __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	       
 	       for(l = 0; l < 80; l+=2)
 		 {
-		   __m128d ex3v = _mm_load_pd(&v[l]);		  
-		   _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+		   __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+		   vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
 		 }		   		  
 #else
 		for(l = 0; l < 80; l++)
@@ -6546,10 +6546,10 @@
 	     vr = &(x2[80 * i + 20 * k]);
 	     v =  &(x3[80 * i + 20 * k]);
 
-#ifdef __SIM_SSE3
-	     __m128d zero =  _mm_setzero_pd();
+#ifdef __SIM_VECLIB
+	     __m128d zero =  vec_zero2dp();
 	     for(l = 0; l < 20; l+=2)		  		    
-	       _mm_store_pd(&v[l], zero);
+	       vec_store2dpto2dp(&v[l], zero);
 #else
 	     for(l = 0; l < 20; l++)
 	       v[l] = 0;
@@ -6557,10 +6557,10 @@
 
 	     for(l = 0; l < 20; l++)
 	       {		 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 		 {
-		   __m128d al = _mm_setzero_pd();
-		   __m128d ar = _mm_setzero_pd();
+		   __m128d al = vec_zero2dp();
+		   __m128d ar = vec_zero2dp();
 
 		   double *ll   = &left[k * 400 + l * 20];
 		   double *rr   = &right[k * 400 + l * 20];
@@ -6568,28 +6568,28 @@
 		   
 		   for(j = 0; j < 20; j+=2)
 		     {
-		       __m128d lv  = _mm_load_pd(&ll[j]);
-		       __m128d rv  = _mm_load_pd(&rr[j]);
-		       __m128d vll = _mm_load_pd(&vl[j]);
-		       __m128d vrr = _mm_load_pd(&vr[j]);
+		       __m128d lv  = vec_load2dpaligned(&ll[j]);
+		       __m128d rv  = vec_load2dpaligned(&rr[j]);
+		       __m128d vll = vec_load2dpaligned(&vl[j]);
+		       __m128d vrr = vec_load2dpaligned(&vr[j]);
 		       
-		       al = _mm_add_pd(al, _mm_mul_pd(vll, lv));
-		       ar = _mm_add_pd(ar, _mm_mul_pd(vrr, rv));
+		       al = vec_add2dp(al, vec_multiply2dp(vll, lv));
+		       ar = vec_add2dp(ar, vec_multiply2dp(vrr, rv));
 		     }  		 
 		       
-		   al = _mm_hadd_pd(al, al);
-		   ar = _mm_hadd_pd(ar, ar);
+		   al = vec_horizontaladd2dp(al, al);
+		   ar = vec_horizontaladd2dp(ar, ar);
 		   
-		   al = _mm_mul_pd(al, ar);
+		   al = vec_multiply2dp(al, ar);
 
 		   for(j = 0; j < 20; j+=2)
 		     {
-		       __m128d vv  = _mm_load_pd(&v[j]);
-		       __m128d EVV = _mm_load_pd(&EVEV[j]);
+		       __m128d vv  = vec_load2dpaligned(&v[j]);
+		       __m128d EVV = vec_load2dpaligned(&EVEV[j]);
 
-		       vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+		       vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 
-		       _mm_store_pd(&v[j], vv);
+		       vec_store2dpto2dp(&v[j], vv);
 		     }		  		   		  
 		 }		 
 #else
@@ -6611,18 +6611,18 @@
 	   }
 	 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 	 { 
 	   v = &(x3[80 * i]);
-	   __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+	   __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 	   
 	   scale = 1;
 	   for(l = 0; scale && (l < 80); l += 2)
 	     {
-	       __m128d vv = _mm_load_pd(&v[l]);
-	       __m128d v1 = _mm_and_pd(vv, absMask.m);
-	       v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-	       if(_mm_movemask_pd( v1 ) != 3)
+	       __m128d vv = vec_load2dpaligned(&v[l]);
+	       __m128d v1 = vec_bitand2dp(vv, absMask.m);
+	       v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+	       if(vec_extractupperbit2dp( v1 ) != 3)
 		 scale = 0;
 	     }	    	  
 	 }
@@ -6635,13 +6635,13 @@
 
 	 if (scale)
 	   {
-#ifdef __SIM_SSE3
-	       __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+#ifdef __SIM_VECLIB
+	       __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 	       
 	       for(l = 0; l < 80; l+=2)
 		 {
-		   __m128d ex3v = _mm_load_pd(&v[l]);		  
-		   _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+		   __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+		   vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
 		 }		   		  
 #else	     
 	     for(l = 0; l < 80; l++)
@@ -6665,7 +6665,7 @@
 }
 
 
-#ifdef __SIM_SSE3
+#ifdef __SIM_VECLIB
 
 
 static void newviewGTRGAMMAPROT_GAPPED_SAVE(int tipCase,
@@ -6703,21 +6703,21 @@
             double *ll =  &left[k * 20];
             double *rr =  &right[k * 20];
 
-            __m128d umpX1v = _mm_setzero_pd();
-            __m128d umpX2v = _mm_setzero_pd();
+            __m128d umpX1v = vec_zero2dp();
+            __m128d umpX2v = vec_zero2dp();
 
             for(l = 0; l < 20; l+=2)
             {
-              __m128d vv = _mm_load_pd(&v[l]);
-              umpX1v = _mm_add_pd(umpX1v, _mm_mul_pd(vv, _mm_load_pd(&ll[l])));
-              umpX2v = _mm_add_pd(umpX2v, _mm_mul_pd(vv, _mm_load_pd(&rr[l])));					
+              __m128d vv = vec_load2dpaligned(&v[l]);
+              umpX1v = vec_add2dp(umpX1v, vec_multiply2dp(vv, vec_load2dpaligned(&ll[l])));
+              umpX2v = vec_add2dp(umpX2v, vec_multiply2dp(vv, vec_load2dpaligned(&rr[l])));					
             }
 
-            umpX1v = _mm_hadd_pd(umpX1v, umpX1v);
-            umpX2v = _mm_hadd_pd(umpX2v, umpX2v);
+            umpX1v = vec_horizontaladd2dp(umpX1v, umpX1v);
+            umpX2v = vec_horizontaladd2dp(umpX2v, umpX2v);
 
-            _mm_storel_pd(&umpX1[80 * i + k], umpX1v);
-            _mm_storel_pd(&umpX2[80 * i + k], umpX2v);
+            vec_storelower1dpof2dp(&umpX1[80 * i + k], umpX1v);
+            vec_storelower1dpof2dp(&umpX2[80 * i + k], umpX2v);
           }
         }
 
@@ -6729,24 +6729,24 @@
           {
             v = &x3_gapColumn[j * 20];
 
-            __m128d zero =  _mm_setzero_pd();
+            __m128d zero =  vec_zero2dp();
             for(k = 0; k < 20; k+=2)		  		    
-              _mm_store_pd(&v[k], zero);
+              vec_store2dpto2dp(&v[k], zero);
 
             for(k = 0; k < 20; k++)
             { 
               double *eev = &extEV[k * 20];
               x1px2 = uX1[j * 20 + k] * uX2[j * 20 + k];
-              __m128d x1px2v = _mm_set1_pd(x1px2);
+              __m128d x1px2v = vec_splat2dp(x1px2);
 
               for(l = 0; l < 20; l+=2)
               {
-                __m128d vv = _mm_load_pd(&v[l]);
-                __m128d ee = _mm_load_pd(&eev[l]);
+                __m128d vv = vec_load2dpaligned(&v[l]);
+                __m128d ee = vec_load2dpaligned(&eev[l]);
 
-                vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+                vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 
-                _mm_store_pd(&v[l], vv);
+                vec_store2dpto2dp(&v[l], vv);
               }
             }
           }	   
@@ -6764,24 +6764,24 @@
               v = &x3_ptr[j * 20];
 
 
-              __m128d zero =  _mm_setzero_pd();
+              __m128d zero =  vec_zero2dp();
               for(k = 0; k < 20; k+=2)		  		    
-                _mm_store_pd(&v[k], zero);
+                vec_store2dpto2dp(&v[k], zero);
 
               for(k = 0; k < 20; k++)
               { 
                 double *eev = &extEV[k * 20];
                 x1px2 = uX1[j * 20 + k] * uX2[j * 20 + k];
-                __m128d x1px2v = _mm_set1_pd(x1px2);
+                __m128d x1px2v = vec_splat2dp(x1px2);
 
                 for(l = 0; l < 20; l+=2)
                 {
-                  __m128d vv = _mm_load_pd(&v[l]);
-                  __m128d ee = _mm_load_pd(&eev[l]);
+                  __m128d vv = vec_load2dpaligned(&v[l]);
+                  __m128d ee = vec_load2dpaligned(&eev[l]);
 
-                  vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+                  vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 
-                  _mm_store_pd(&v[l], vv);
+                  vec_store2dpto2dp(&v[l], vv);
                 }
               }
             }	   
@@ -6803,16 +6803,16 @@
           {
             double *ll =  &left[k * 20];
 
-            __m128d umpX1v = _mm_setzero_pd();
+            __m128d umpX1v = vec_zero2dp();
 
             for(l = 0; l < 20; l+=2)
             {
-              __m128d vv = _mm_load_pd(&v[l]);
-              umpX1v = _mm_add_pd(umpX1v, _mm_mul_pd(vv, _mm_load_pd(&ll[l])));		    					
+              __m128d vv = vec_load2dpaligned(&v[l]);
+              umpX1v = vec_add2dp(umpX1v, vec_multiply2dp(vv, vec_load2dpaligned(&ll[l])));		    					
             }
 
-            umpX1v = _mm_hadd_pd(umpX1v, umpX1v);				
-            _mm_storel_pd(&umpX1[80 * i + k], umpX1v);		
+            umpX1v = vec_horizontaladd2dp(umpX1v, umpX1v);				
+            vec_storelower1dpof2dp(&umpX1[80 * i + k], umpX1v);		
 
           }
         }
@@ -6827,40 +6827,40 @@
             for(l = 0; l < 20; l++)
             {		   
               double *r =  &right[k * 400 + l * 20];
-              __m128d ump_x2v = _mm_setzero_pd();	    
+              __m128d ump_x2v = vec_zero2dp();	    
 
               for(j = 0; j < 20; j+= 2)
               {
-                __m128d vv = _mm_load_pd(&v[j]);
-                __m128d rr = _mm_load_pd(&r[j]);
-                ump_x2v = _mm_add_pd(ump_x2v, _mm_mul_pd(vv, rr));
+                __m128d vv = vec_load2dpaligned(&v[j]);
+                __m128d rr = vec_load2dpaligned(&r[j]);
+                ump_x2v = vec_add2dp(ump_x2v, vec_multiply2dp(vv, rr));
               }
 
-              ump_x2v = _mm_hadd_pd(ump_x2v, ump_x2v);
+              ump_x2v = vec_horizontaladd2dp(ump_x2v, ump_x2v);
 
-              _mm_storel_pd(&ump_x2[l], ump_x2v);		   		     
+              vec_storelower1dpof2dp(&ump_x2[l], ump_x2v);		   		     
             }
 
             v = &(x3_gapColumn[20 * k]);
 
-            __m128d zero =  _mm_setzero_pd();
+            __m128d zero =  vec_zero2dp();
             for(l = 0; l < 20; l+=2)		  		    
-              _mm_store_pd(&v[l], zero);
+              vec_store2dpto2dp(&v[l], zero);
 
             for(l = 0; l < 20; l++)
             {
               double *eev = &extEV[l * 20];
               x1px2 = uX1[k * 20 + l]  * ump_x2[l];
-              __m128d x1px2v = _mm_set1_pd(x1px2);
+              __m128d x1px2v = vec_splat2dp(x1px2);
 
               for(j = 0; j < 20; j+=2)
               {
-                __m128d vv = _mm_load_pd(&v[j]);
-                __m128d ee = _mm_load_pd(&eev[j]);
+                __m128d vv = vec_load2dpaligned(&v[j]);
+                __m128d ee = vec_load2dpaligned(&eev[j]);
 
-                vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+                vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 
-                _mm_store_pd(&v[j], vv);
+                vec_store2dpto2dp(&v[j], vv);
               }		     		    
             }			
 
@@ -6868,15 +6868,15 @@
 
           { 
             v = x3_gapColumn;
-            __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+            __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 
             scale = 1;
             for(l = 0; scale && (l < 80); l += 2)
             {
-              __m128d vv = _mm_load_pd(&v[l]);
-              __m128d v1 = _mm_and_pd(vv, absMask.m);
-              v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-              if(_mm_movemask_pd( v1 ) != 3)
+              __m128d vv = vec_load2dpaligned(&v[l]);
+              __m128d v1 = vec_bitand2dp(vv, absMask.m);
+              v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+              if(vec_extractupperbit2dp( v1 ) != 3)
                 scale = 0;
             }	    	  
           }
@@ -6885,12 +6885,12 @@
           if (scale)
           {
             gapScaling = 1;
-            __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+            __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
             for(l = 0; l < 80; l+=2)
             {
-              __m128d ex3v = _mm_load_pd(&v[l]);		  
-              _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+              __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+              vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
             }		   		  	      	    	       
           }
         }
@@ -6926,40 +6926,40 @@
               for(l = 0; l < 20; l++)
               {		   
                 double *r =  &right[k * 400 + l * 20];
-                __m128d ump_x2v = _mm_setzero_pd();	    
+                __m128d ump_x2v = vec_zero2dp();	    
 
                 for(j = 0; j < 20; j+= 2)
                 {
-                  __m128d vv = _mm_load_pd(&v[j]);
-                  __m128d rr = _mm_load_pd(&r[j]);
-                  ump_x2v = _mm_add_pd(ump_x2v, _mm_mul_pd(vv, rr));
+                  __m128d vv = vec_load2dpaligned(&v[j]);
+                  __m128d rr = vec_load2dpaligned(&r[j]);
+                  ump_x2v = vec_add2dp(ump_x2v, vec_multiply2dp(vv, rr));
                 }
 
-                ump_x2v = _mm_hadd_pd(ump_x2v, ump_x2v);
+                ump_x2v = vec_horizontaladd2dp(ump_x2v, ump_x2v);
 
-                _mm_storel_pd(&ump_x2[l], ump_x2v);		   		     
+                vec_storelower1dpof2dp(&ump_x2[l], ump_x2v);		   		     
               }
 
               v = &x3_ptr[20 * k];
 
-              __m128d zero =  _mm_setzero_pd();
+              __m128d zero =  vec_zero2dp();
               for(l = 0; l < 20; l+=2)		  		    
-                _mm_store_pd(&v[l], zero);
+                vec_store2dpto2dp(&v[l], zero);
 
               for(l = 0; l < 20; l++)
               {
                 double *eev = &extEV[l * 20];
                 x1px2 = uX1[k * 20 + l]  * ump_x2[l];
-                __m128d x1px2v = _mm_set1_pd(x1px2);
+                __m128d x1px2v = vec_splat2dp(x1px2);
 
                 for(j = 0; j < 20; j+=2)
                 {
-                  __m128d vv = _mm_load_pd(&v[j]);
-                  __m128d ee = _mm_load_pd(&eev[j]);
+                  __m128d vv = vec_load2dpaligned(&v[j]);
+                  __m128d ee = vec_load2dpaligned(&eev[j]);
 
-                  vv = _mm_add_pd(vv, _mm_mul_pd(x1px2v,ee));
+                  vv = vec_add2dp(vv, vec_multiply2dp(x1px2v,ee));
 
-                  _mm_store_pd(&v[j], vv);
+                  vec_store2dpto2dp(&v[j], vv);
                 }		     		    
               }			
 
@@ -6968,15 +6968,15 @@
 
             { 
               v = x3_ptr;
-              __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+              __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 
               scale = 1;
               for(l = 0; scale && (l < 80); l += 2)
               {
-                __m128d vv = _mm_load_pd(&v[l]);
-                __m128d v1 = _mm_and_pd(vv, absMask.m);
-                v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-                if(_mm_movemask_pd( v1 ) != 3)
+                __m128d vv = vec_load2dpaligned(&v[l]);
+                __m128d v1 = vec_bitand2dp(vv, absMask.m);
+                v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+                if(vec_extractupperbit2dp( v1 ) != 3)
                   scale = 0;
               }	    	  
             }
@@ -6984,12 +6984,12 @@
 
             if (scale)
             {
-              __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+              __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
               for(l = 0; l < 80; l+=2)
               {
-                __m128d ex3v = _mm_load_pd(&v[l]);		  
-                _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+                __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+                vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
               }		   		  
 
               if(useFastScaling)
@@ -7011,15 +7011,15 @@
           vr = &(x2_gapColumn[20 * k]);
           v =  &(x3_gapColumn[20 * k]);
 
-          __m128d zero =  _mm_setzero_pd();
+          __m128d zero =  vec_zero2dp();
           for(l = 0; l < 20; l+=2)		  		    
-            _mm_store_pd(&v[l], zero);
+            vec_store2dpto2dp(&v[l], zero);
 
           for(l = 0; l < 20; l++)
           {		 
             {
-              __m128d al = _mm_setzero_pd();
-              __m128d ar = _mm_setzero_pd();
+              __m128d al = vec_zero2dp();
+              __m128d ar = vec_zero2dp();
 
               double *ll   = &left[k * 400 + l * 20];
               double *rr   = &right[k * 400 + l * 20];
@@ -7027,28 +7027,28 @@
 
               for(j = 0; j < 20; j+=2)
               {
-                __m128d lv  = _mm_load_pd(&ll[j]);
-                __m128d rv  = _mm_load_pd(&rr[j]);
-                __m128d vll = _mm_load_pd(&vl[j]);
-                __m128d vrr = _mm_load_pd(&vr[j]);
+                __m128d lv  = vec_load2dpaligned(&ll[j]);
+                __m128d rv  = vec_load2dpaligned(&rr[j]);
+                __m128d vll = vec_load2dpaligned(&vl[j]);
+                __m128d vrr = vec_load2dpaligned(&vr[j]);
 
-                al = _mm_add_pd(al, _mm_mul_pd(vll, lv));
-                ar = _mm_add_pd(ar, _mm_mul_pd(vrr, rv));
+                al = vec_add2dp(al, vec_multiply2dp(vll, lv));
+                ar = vec_add2dp(ar, vec_multiply2dp(vrr, rv));
               }  		 
 
-              al = _mm_hadd_pd(al, al);
-              ar = _mm_hadd_pd(ar, ar);
+              al = vec_horizontaladd2dp(al, al);
+              ar = vec_horizontaladd2dp(ar, ar);
 
-              al = _mm_mul_pd(al, ar);
+              al = vec_multiply2dp(al, ar);
 
               for(j = 0; j < 20; j+=2)
               {
-                __m128d vv  = _mm_load_pd(&v[j]);
-                __m128d EVV = _mm_load_pd(&EVEV[j]);
+                __m128d vv  = vec_load2dpaligned(&v[j]);
+                __m128d EVV = vec_load2dpaligned(&EVEV[j]);
 
-                vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+                vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 
-                _mm_store_pd(&v[j], vv);
+                vec_store2dpto2dp(&v[j], vv);
               }		  		   		  
             }		 
 
@@ -7058,15 +7058,15 @@
 
         { 
           v = x3_gapColumn;
-          __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+          __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 
           scale = 1;
           for(l = 0; scale && (l < 80); l += 2)
           {
-            __m128d vv = _mm_load_pd(&v[l]);
-            __m128d v1 = _mm_and_pd(vv, absMask.m);
-            v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-            if(_mm_movemask_pd( v1 ) != 3)
+            __m128d vv = vec_load2dpaligned(&v[l]);
+            __m128d v1 = vec_bitand2dp(vv, absMask.m);
+            v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+            if(vec_extractupperbit2dp( v1 ) != 3)
               scale = 0;
           }	    	  
         }
@@ -7074,12 +7074,12 @@
         if (scale)
         {
           gapScaling = 1;
-          __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+          __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
           for(l = 0; l < 80; l+=2)
           {
-            __m128d ex3v = _mm_load_pd(&v[l]);		  
-            _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+            __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+            vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
           }		   		  
 
 
@@ -7122,15 +7122,15 @@
             vr = &(x2v[20 * k]);
             v =  &x3_ptr[20 * k];
 
-            __m128d zero =  _mm_setzero_pd();
+            __m128d zero =  vec_zero2dp();
             for(l = 0; l < 20; l+=2)		  		    
-              _mm_store_pd(&v[l], zero);
+              vec_store2dpto2dp(&v[l], zero);
 
             for(l = 0; l < 20; l++)
             {		 
               {
-                __m128d al = _mm_setzero_pd();
-                __m128d ar = _mm_setzero_pd();
+                __m128d al = vec_zero2dp();
+                __m128d ar = vec_zero2dp();
 
                 double *ll   = &left[k * 400 + l * 20];
                 double *rr   = &right[k * 400 + l * 20];
@@ -7138,28 +7138,28 @@
 
                 for(j = 0; j < 20; j+=2)
                 {
-                  __m128d lv  = _mm_load_pd(&ll[j]);
-                  __m128d rv  = _mm_load_pd(&rr[j]);
-                  __m128d vll = _mm_load_pd(&vl[j]);
-                  __m128d vrr = _mm_load_pd(&vr[j]);
+                  __m128d lv  = vec_load2dpaligned(&ll[j]);
+                  __m128d rv  = vec_load2dpaligned(&rr[j]);
+                  __m128d vll = vec_load2dpaligned(&vl[j]);
+                  __m128d vrr = vec_load2dpaligned(&vr[j]);
 
-                  al = _mm_add_pd(al, _mm_mul_pd(vll, lv));
-                  ar = _mm_add_pd(ar, _mm_mul_pd(vrr, rv));
+                  al = vec_add2dp(al, vec_multiply2dp(vll, lv));
+                  ar = vec_add2dp(ar, vec_multiply2dp(vrr, rv));
                 }  		 
 
-                al = _mm_hadd_pd(al, al);
-                ar = _mm_hadd_pd(ar, ar);
+                al = vec_horizontaladd2dp(al, al);
+                ar = vec_horizontaladd2dp(ar, ar);
 
-                al = _mm_mul_pd(al, ar);
+                al = vec_multiply2dp(al, ar);
 
                 for(j = 0; j < 20; j+=2)
                 {
-                  __m128d vv  = _mm_load_pd(&v[j]);
-                  __m128d EVV = _mm_load_pd(&EVEV[j]);
+                  __m128d vv  = vec_load2dpaligned(&v[j]);
+                  __m128d EVV = vec_load2dpaligned(&EVEV[j]);
 
-                  vv = _mm_add_pd(vv, _mm_mul_pd(al, EVV));
+                  vv = vec_add2dp(vv, vec_multiply2dp(al, EVV));
 
-                  _mm_store_pd(&v[j], vv);
+                  vec_store2dpto2dp(&v[j], vv);
                 }		  		   		  
               }		 
 
@@ -7170,15 +7170,15 @@
 
           { 
             v = x3_ptr;
-            __m128d minlikelihood_sse = _mm_set1_pd( minlikelihood );
+            __m128d minlikelihood_sse = vec_splat2dp( minlikelihood );
 
             scale = 1;
             for(l = 0; scale && (l < 80); l += 2)
             {
-              __m128d vv = _mm_load_pd(&v[l]);
-              __m128d v1 = _mm_and_pd(vv, absMask.m);
-              v1 = _mm_cmplt_pd(v1,  minlikelihood_sse);
-              if(_mm_movemask_pd( v1 ) != 3)
+              __m128d vv = vec_load2dpaligned(&v[l]);
+              __m128d v1 = vec_bitand2dp(vv, absMask.m);
+              v1 = vec_comparelt2dp(v1,  minlikelihood_sse);
+              if(vec_extractupperbit2dp( v1 ) != 3)
                 scale = 0;
             }	    	  
           }
@@ -7186,12 +7186,12 @@
 
           if (scale)
           {
-            __m128d twoto = _mm_set_pd(twotothe256, twotothe256);
+            __m128d twoto = vec_set2dp(twotothe256, twotothe256);
 
             for(l = 0; l < 80; l+=2)
             {
-              __m128d ex3v = _mm_load_pd(&v[l]);		  
-              _mm_store_pd(&v[l], _mm_mul_pd(ex3v,twoto));	
+              __m128d ex3v = vec_load2dpaligned(&v[l]);		  
+              vec_store2dpto2dp(&v[l], vec_multiply2dp(ex3v,twoto));	
             }		   		  
 
             if(useFastScaling)
@@ -8197,7 +8197,7 @@
 				tr->partitionData[model].EIGN, tr->partitionData[model].numberOfCategories,
 				left, right, DNA_DATA, tr->saveMemory, tr->maxCategories);
 
-#if (defined(__SIM_SSE3) || defined(__AVX))
+#if (defined(__SIM_VECLIB) || defined(__AVX))
 			  if(tr->saveMemory)
 			    {			   		      
 #ifdef __AVX
@@ -8292,7 +8292,7 @@
 			      4, left, right, DNA_DATA, tr->saveMemory, tr->maxCategories);
 			
 			
-#if (defined(__SIM_SSE3) || defined(__AVX))
+#if (defined(__SIM_VECLIB) || defined(__AVX))
 			if(tr->saveMemory)
 			  {
 #ifdef __AVX					     
@@ -8348,7 +8348,7 @@
 				tr->partitionData[model].EIGN,
 				tr->partitionData[model].numberOfCategories, left, right, AA_DATA, tr->saveMemory, tr->maxCategories);
 
-#if  (defined(__SIM_SSE3) || defined(__AVX))
+#if  (defined(__SIM_VECLIB) || defined(__AVX))
 			  if(tr->saveMemory)
 #ifdef __AVX
 			    newviewGTRCATPROT_AVX_GAPPED_SAVE(tInfo->tipCase,  tr->partitionData[model].EV, tr->partitionData[model].rateCategory,
@@ -8407,7 +8407,7 @@
 				tr->partitionData[model].EI,
 				tr->partitionData[model].EIGN,
 				4, left, right, AA_DATA, tr->saveMemory, tr->maxCategories);
-#if (defined(__SIM_SSE3) || defined(__AVX))
+#if (defined(__SIM_VECLIB) || defined(__AVX))
 			  if(tr->saveMemory)
 #ifdef __AVX
 			    newviewGTRGAMMAPROT_AVX_GAPPED_SAVE(tInfo->tipCase,
