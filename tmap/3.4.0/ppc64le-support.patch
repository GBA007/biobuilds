--- src/index/tmap_bwt_aux.c
+++ src/index/tmap_bwt_aux.c
@@ -71,9 +71,9 @@ tmap_bwt_aux_set_mask()
   tmap_bwt_aux_n_mask_64[6] = _mm_cvtsi64x_si64(0x0f0f0f0f0f0f0f0ful);
   tmap_bwt_aux_n_mask_64[7] = _mm_cvtsi64x_si64(0x1555555555555555ul);
   tmap_bwt_aux_n_mask_64[8] = _mm_cvtsi64x_si64(0x1111111111111111ul);
-  tmap_bwt_aux_n_mask_128[0] = _mm_set1_epi64(tmap_bwt_aux_n_mask_64[2]);
-  tmap_bwt_aux_n_mask_128[1] = _mm_set1_epi64(tmap_bwt_aux_n_mask_64[5]);
-  tmap_bwt_aux_n_mask_128[2] = _mm_set1_epi64(tmap_bwt_aux_n_mask_64[6]);
+  tmap_bwt_aux_n_mask_128[0] = vec_splat2sd(tmap_bwt_aux_n_mask_64[2]);
+  tmap_bwt_aux_n_mask_128[1] = vec_splat2sd(tmap_bwt_aux_n_mask_64[5]);
+  tmap_bwt_aux_n_mask_128[2] = vec_splat2sd(tmap_bwt_aux_n_mask_64[6]);
 }
 
 #define occ_mask2(n) (0x5555555555555555ul - (0x1555555555555555ul >> ((n&31)<<1)))
--- src/index/tmap_bwt_aux.h
+++ src/index/tmap_bwt_aux.h
@@ -8,7 +8,7 @@
 
 #include <stdlib.h>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <unistd.h>
 
 // TODO
--- src/index/tmap_sa_aux.c
+++ src/index/tmap_sa_aux.c
@@ -33,11 +33,7 @@
 #include <string.h>
 #include <assert.h>
 #include <stdint.h>
-#include <emmintrin.h>
-#if TMAP_BWT_RUN_TYPE != 0 // Just not the original
-#include <mmintrin.h>
-#include <smmintrin.h>
-#endif
+#include <vec128int.h>
 #include "../util/tmap_error.h"
 #include "../util/tmap_alloc.h"
 #include "../util/tmap_progress.h"
@@ -91,43 +87,43 @@ extern __m64 tmap_bwt_aux_n_mask_64[9];
 #define _mm_ctmap_bwt_aux_n_mask_epi128(t2, t, m, shft) _mm_ctmap_bwt_aux_n_mask_xxx(t2, t, m, shft, si128, epi64)
 
 #define _mm_sum2si128_si64(t)                   \
-  _mm_add_si64(_mm_movepi64_pi64(t), _mm_cvtsi64x_si64(_mm_extract_epi64(t, 1)))
+  _mm_add_si64(_mm_movepi64_pi64(t), _mm_cvtsi64x_si64(vec_extract1sdfrom2sd(t, 1)))
 
 
 static inline uint64_t 
 tmap_bwt_aux_occ(tmap_bwt_int_t k, __m64 x, const uint32_t *const p)
 {
   __m128i t, t2;
-  t2 = t = _mm_set1_epi64(x);
+  t2 = t = vec_splat2sd(x);
   switch (k&0x60) {
     case 0x60:
       x = _mm_set_pi32(p[-6], p[-5]);
     case 0x40:
-      t = _mm_set_epi64(x, _mm_set_pi32(p[-4], p[-3]));
+      t = vec_set2sd(x, _mm_set_pi32(p[-4], p[-3]));
     case 0x20: x = _mm_set_pi32(p[-2], p[-1]);
   }
   t = _mm_nc_combmask_epi128(t, t2, tmap_bwt_aux_n_mask_128[0]);
 
-  t2 = _mm_xor_si128(_mm_set_epi64(x, _mm_set_pi32(p[0], p[1])), t2);
-  t2 = _mm_and_si128(t2, _mm_srli_epi64(t2, 1));
+  t2 = vec_bitxor1q(vec_set2sd(x, _mm_set_pi32(p[0], p[1])), t2);
+  t2 = vec_bitand1q(t2, _mm_srli_epi64(t2, 1));
 
   x = _mm_srli_si64(tmap_bwt_aux_n_mask_64[7], ((k&31)<<1));
   x = _mm_sub_si64(tmap_bwt_aux_n_mask_64[2], x);
-  t2 = _mm_and_si128(t2, _mm_set_epi64(tmap_bwt_aux_n_mask_64[2], x));
+  t2 = vec_bitand1q(t2, vec_set2sd(tmap_bwt_aux_n_mask_64[2], x));
 
-  t = _mm_add_epi64(t, t2);
+  t = vec_add2sd(t, t2);
 
   t2 = _mm_srli_epi64(t, 2);
-  t2 = _mm_and_si128(t2, tmap_bwt_aux_n_mask_128[1]);
-  t = _mm_and_si128(t, tmap_bwt_aux_n_mask_128[1]);
-  t = _mm_add_epi64(t, t2);
+  t2 = vec_bitand1q(t2, tmap_bwt_aux_n_mask_128[1]);
+  t = vec_bitand1q(t, tmap_bwt_aux_n_mask_128[1]);
+  t = vec_add2sd(t, t2);
 
   t2 = _mm_srli_epi64(t, 4);
-  t2 = _mm_and_si128(t2, tmap_bwt_aux_n_mask_128[2]);
-  t = _mm_and_si128(t, tmap_bwt_aux_n_mask_128[2]);
-  t = _mm_add_epi64(t, t2);
+  t2 = vec_bitand1q(t2, tmap_bwt_aux_n_mask_128[2]);
+  t = vec_bitand1q(t, tmap_bwt_aux_n_mask_128[2]);
+  t = vec_add2sd(t, t2);
 
-  return _mm_extract_epi64(t, 1) + _mm_extract_epi64(t, 0);
+  return vec_extract1sdfrom2sd(t, 1) + vec_extract1sdfrom2sd(t, 0);
 }
 
 static inline tmap_bwt_int_t 
--- src/sw/lib/AffineSWOptimizationHash.cpp
+++ src/sw/lib/AffineSWOptimizationHash.cpp
@@ -3,7 +3,7 @@
 #include <stdio.h>
 #include <iostream>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include "AffineSWOptimizationHash.h"
 
 using namespace std;
@@ -33,28 +33,28 @@ static uint64_t hashDNA(const string &s) {
         //return h;
     }
 
-    __m128i mhash = _mm_set1_epi16(1);
-    __m128i mmul = _mm_set1_epi16(13);
-    __m128i mzero = _mm_setzero_si128();
+    __m128i mhash = vec_splat8sh(1);
+    __m128i mmul = vec_splat8sh(13);
+    __m128i mzero = vec_zero1q();
     __m128i m0, m1a, m1b;
     int i = 0;
     while (i + 16 < len) {
-        m0 = _mm_loadu_si128((__m128i*)&s[i]);
-        m1a = _mm_unpacklo_epi8(m0, mzero);
-        m1b = _mm_unpackhi_epi8(m0, mzero);
-        mhash = _mm_mullo_epi16(mhash, mmul);
-        mhash = _mm_add_epi16(mhash, m1a);
-        mhash = _mm_mullo_epi16(mhash, mmul);
-        mhash = _mm_add_epi16(mhash, m1b);
+        m0 = vec_load1qu((__m128i*)&s[i]);
+        m1a = vec_unpacklow88sb(m0, mzero);
+        m1b = vec_unpackhigh88sb(m0, mzero);
+        mhash = vec_multiply8sh(mhash, mmul);
+        mhash = vec_add8sh(mhash, m1a);
+        mhash = vec_multiply8sh(mhash, mmul);
+        mhash = vec_add8sh(mhash, m1b);
         i += 16;
     }
-    m0 = _mm_loadu_si128((__m128i*)&s[len - 16]);
-    m1a = _mm_unpacklo_epi8(m0, mzero);
-    m1b = _mm_unpackhi_epi8(m0, mzero);
-    mhash = _mm_mullo_epi16(mhash, mmul);
-    mhash = _mm_add_epi16(mhash, m1a);
-    mhash = _mm_mullo_epi16(mhash, mmul);
-    mhash = _mm_add_epi16(mhash, m1b);
+    m0 = vec_load1qu((__m128i*)&s[len - 16]);
+    m1a = vec_unpacklow88sb(m0, mzero);
+    m1b = vec_unpackhigh88sb(m0, mzero);
+    mhash = vec_multiply8sh(mhash, mmul);
+    mhash = vec_add8sh(mhash, m1a);
+    mhash = vec_multiply8sh(mhash, mmul);
+    mhash = vec_add8sh(mhash, m1b);
 
     /*
     uint32_t* p = (uint32_t*)&mhash;
@@ -63,14 +63,14 @@ static uint64_t hashDNA(const string &s) {
     h = h * 1337 + p[2];
     h = h * 1337 + p[3];
     */
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 0);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 1);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 2);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 3);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 4);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 5);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 6);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 7);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 0);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 1);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 2);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 3);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 4);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 5);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 6);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 7);
     //return h;
     return h >> 8;
 }
--- src/sw/lib/Solution10.cpp
+++ src/sw/lib/Solution10.cpp
@@ -27,7 +27,7 @@ it was released under GPL v2. He passed suddenly in late 2010.  RIP. */
 
 #include <stdlib.h>
 #include <stdio.h>
-#include <emmintrin.h> // use SSE2
+#include <vec128int.h>
 #include <cstring>
 #include <sstream>
 #include "Solution10.h"
@@ -282,20 +282,20 @@ int swStripedByte3(int              queryLength,
   __m128i vGapExtend;
 
   __m128i vTemp;
-  __m128i vZero = _mm_setzero_si128();
+  __m128i vZero = vec_zero1q();
 
   __m128i *pvScore;
 
-  vBias = _mm_set1_epi8(bias);
-  vGapOpen = _mm_set1_epi8(gapOpen);
-  vGapExtend = _mm_set1_epi8(gapExtend);
+  vBias = vec_splat16sb(bias);
+  vGapOpen = vec_splat16sb(gapOpen);
+  vGapExtend = vec_splat16sb(gapExtend);
   vMaxScore = vZero;
 
   /* Zero out the storage vector */
   for (i = 0; i < iter; ++i)
     {
-      _mm_store_si128 (pvE + i, vZero);
-      _mm_store_si128 (pvHStore + i, vZero);
+      vec_store1q (pvE + i, vZero);
+      vec_store1q (pvHStore + i, vZero);
     }
 
   for (i = 0; i < dbLength; ++i)
@@ -307,8 +307,8 @@ int swStripedByte3(int              queryLength,
       vF = vZero;
 
       /* load the next h value */
-      vH = _mm_load_si128 (pvHStore + iter - 1);
-      vH = _mm_slli_si128 (vH, 1);
+      vH = vec_load1q (pvHStore + iter - 1);
+      vH = vec_shiftleftbytes1q (vH, 1);
 
       pv = pvHLoad;
       pvHLoad = pvHStore;
@@ -317,43 +317,43 @@ int swStripedByte3(int              queryLength,
       for (j = 0; j < iter; ++j)
         {
           /* load values of vF and vH from previous row (one unit up) */
-          vE = _mm_load_si128 (pvE + j);
+          vE = vec_load1q (pvE + j);
 
           /* add score to vH */
-          vH = _mm_adds_epu8 (vH, *(pvScore + j));
-          vH = _mm_subs_epu8 (vH, vBias);
+          vH = vec_addsaturating16ub (vH, *(pvScore + j));
+          vH = vec_subtractsaturating16ub (vH, vBias);
 
           /* Update highest score encountered this far */               
-          vTemp = _mm_cmpeq_epi8 (vH, vMaxScore);
-          equalMask = _mm_movemask_epi8 (vTemp);
+          vTemp = vec_compareeq16sb (vH, vMaxScore);
+          equalMask = vec_extractupperbit16sb (vTemp);
 
-          vTemp = _mm_subs_epu8 (vH, vMaxScore);
-          vTemp = _mm_cmpgt_epi8 (vTemp, vZero);            
-          greaterMask = _mm_movemask_epi8(vTemp);
+          vTemp = vec_subtractsaturating16ub (vH, vMaxScore);
+          vTemp = vec_comparegt16sb (vTemp, vZero);            
+          greaterMask = vec_extractupperbit16sb(vTemp);
 
           if (greaterMask != 0x0000)
             {                
               vMaxScore = vH;
               /* find largest score in the vMaxScore vector */
-              vTemp = _mm_srli_si128 (vMaxScore, 8);
-              vMaxScore = _mm_max_epu8 (vMaxScore, vTemp);
-              vTemp = _mm_srli_si128 (vMaxScore, 4);
-              vMaxScore = _mm_max_epu8 (vMaxScore, vTemp);
-              vTemp = _mm_srli_si128 (vMaxScore, 2);
-              vMaxScore = _mm_max_epu8 (vMaxScore, vTemp);
-              vTemp = _mm_srli_si128 (vMaxScore, 1);
-              vMaxScore = _mm_max_epu8 (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 8);
+              vMaxScore = vec_max16ub (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 4);
+              vMaxScore = vec_max16ub (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 2);
+              vMaxScore = vec_max16ub (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 1);
+              vMaxScore = vec_max16ub (vMaxScore, vTemp);
 
               /* store in temporary variable */
-              score = _mm_extract_epi16 (vMaxScore, 0);
+              score = vec_extract8sh (vMaxScore, 0);
               score = score & 0x00ff;                        
               if (score + bias >= 255)
                 return 255;
 
-              vMaxScore = _mm_set1_epi8(score);
+              vMaxScore = vec_splat16sb(score);
 
-              vTemp = _mm_cmpeq_epi8 (vH, vMaxScore);
-              equalMask = _mm_movemask_epi8 (vTemp);
+              vTemp = vec_compareeq16sb (vH, vMaxScore);
+              equalMask = vec_extractupperbit16sb (vTemp);
 
               target_end = i;
               n_best = 0;
@@ -389,71 +389,71 @@ int swStripedByte3(int              queryLength,
                 }
             }                                  
           /* get max from vH, vE and vF */
-          vH = _mm_max_epu8 (vH, vE);
-          vH = _mm_max_epu8 (vH, vF);
+          vH = vec_max16ub (vH, vE);
+          vH = vec_max16ub (vH, vF);
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);
+          vec_store1q (pvHStore + j, vH);
 
           /* update vE value */
-          vH = _mm_subs_epu8 (vH, vGapOpen);
-          vE = _mm_subs_epu8 (vE, vGapExtend);
-          vE = _mm_max_epu8 (vE, vH);
+          vH = vec_subtractsaturating16ub (vH, vGapOpen);
+          vE = vec_subtractsaturating16ub (vE, vGapExtend);
+          vE = vec_max16ub (vE, vH);
 
           /* update vF value */
-          vF = _mm_subs_epu8 (vF, vGapExtend);
-          vF = _mm_max_epu8 (vF, vH);
+          vF = vec_subtractsaturating16ub (vF, vGapExtend);
+          vF = vec_max16ub (vF, vH);
 
           /* save vE values */
-          _mm_store_si128 (pvE + j, vE);
+          vec_store1q (pvE + j, vE);
 
           /* load the next h value */
-          vH = _mm_load_si128 (pvHLoad + j);
+          vH = vec_load1q (pvHLoad + j);
         }
 
       /* reset pointers to the start of the saved data */
       j = 0;
-      vH = _mm_load_si128 (pvHStore + j);
+      vH = vec_load1q (pvHStore + j);
 
       /*  the computed vF value is for the given column.  since */
       /*  we are at the end, we need to shift the vF value over */
       /*  to the next column. */
-      vF = _mm_slli_si128 (vF, 1);
-      vTemp = _mm_subs_epu8 (vH, vGapOpen);
-      vTemp = _mm_subs_epu8 (vF, vTemp);
-      vTemp = _mm_cmpeq_epi8 (vTemp, vZero);
-      cmp  = _mm_movemask_epi8 (vTemp);
+      vF = vec_shiftleftbytes1q (vF, 1);
+      vTemp = vec_subtractsaturating16ub (vH, vGapOpen);
+      vTemp = vec_subtractsaturating16ub (vF, vTemp);
+      vTemp = vec_compareeq16sb (vTemp, vZero);
+      cmp  = vec_extractupperbit16sb (vTemp);
 
       while (cmp != 0xffff) 
         {
-          vE = _mm_load_si128 (pvE + j);
+          vE = vec_load1q (pvE + j);
 
-          vH = _mm_max_epu8 (vH, vF);
+          vH = vec_max16ub (vH, vF);
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);
+          vec_store1q (pvHStore + j, vH);
 
           /*  update vE incase the new vH value would change it */
-          vH = _mm_subs_epu8 (vH, vGapOpen);
-          vE = _mm_max_epu8 (vE, vH);
-          _mm_store_si128 (pvE + j, vE);
+          vH = vec_subtractsaturating16ub (vH, vGapOpen);
+          vE = vec_max16ub (vE, vH);
+          vec_store1q (pvE + j, vE);
 
           /* update vF value */
-          vF = _mm_subs_epu8 (vF, vGapExtend);
+          vF = vec_subtractsaturating16ub (vF, vGapExtend);
 
           ++j;
           if (j >= iter)
             {
               j = 0;
-              vF = _mm_slli_si128 (vF, 1);
+              vF = vec_shiftleftbytes1q (vF, 1);
             }
 
-          vH = _mm_load_si128 (pvHStore + j);
+          vH = vec_load1q (pvHStore + j);
 
-          vTemp = _mm_subs_epu8 (vH, vGapOpen);
-          vTemp = _mm_subs_epu8 (vF, vTemp);
-          vTemp = _mm_cmpeq_epi8 (vTemp, vZero);
-          cmp  = _mm_movemask_epi8 (vTemp);
+          vTemp = vec_subtractsaturating16ub (vH, vGapOpen);
+          vTemp = vec_subtractsaturating16ub (vF, vTemp);
+          vTemp = vec_compareeq16sb (vTemp, vZero);
+          cmp  = vec_extractupperbit16sb (vTemp);
         }
     }        
 
@@ -493,30 +493,30 @@ void swStripedWord3(int              queryLength,
   __m128i vMin;
   __m128i vTemp;
   __m128i *pvScore;
-  __m128i vZero = _mm_setzero_si128();
-  __m128i vInf = _mm_set1_epi16(0x8000);
+  __m128i vZero = vec_zero1q();
+  __m128i vInf = vec_splat8sh(0x8000);
 
   int equalMask, greaterMask;
   int n_best = 0, query_end = -1, target_end = -1;
 
   /* Load gap opening penalty to all elements of a constant */
-  vGapOpen = _mm_set1_epi16 (gapOpen);
+  vGapOpen = vec_splat8sh (gapOpen);
 
   /* Load gap extension penalty to all elements of a constant */
-  vGapExtend = _mm_set1_epi16 (gapExtend);
+  vGapExtend = vec_splat8sh (gapExtend);
 
   /*  load vMaxScore with the vZero.  since we are using signed */
   /*  math, we will bias the maxscore to -32768 so we have the */
   /*  full range of the short. */
 
-  vMin = _mm_insert_epi16 (vZero, 0x8000, 0);
+  vMin = vec_insert8sh (vZero, 0x8000, 0);
   vMaxScore = vInf;
 
   /* Zero out the storage vector */
   for (i = 0; i < iter; ++i)
     {
-      _mm_store_si128 (pvE + i, vMaxScore);
-      _mm_store_si128 (pvHStore + i, vMaxScore);
+      vec_store1q (pvE + i, vMaxScore);
+      vec_store1q (pvHStore + i, vMaxScore);
     }
 
   for (i = 0; i < dbLength; ++i)
@@ -528,9 +528,9 @@ void swStripedWord3(int              queryLength,
       vF = vInf;
 
       /* load the next h value */
-      vH = _mm_load_si128 (pvHStore + iter - 1);
-      vH = _mm_slli_si128 (vH, 2);
-      vH = _mm_or_si128 (vH, vMin);
+      vH = vec_load1q (pvHStore + iter - 1);
+      vH = vec_shiftleftbytes1q (vH, 2);
+      vH = vec_bitor1q (vH, vMin);
 
       pv = pvHLoad;
       pvHLoad = pvHStore;
@@ -539,34 +539,34 @@ void swStripedWord3(int              queryLength,
       for (j = 0; j < iter; ++j)
         {
           /* load values of vF and vH from previous row (one unit up) */
-          vE = _mm_load_si128 (pvE + j);
+          vE = vec_load1q (pvE + j);
 
           /* add score to vH */
-          vH = _mm_adds_epi16 (vH, *pvScore++);
+          vH = vec_addsaturating8sh (vH, *pvScore++);
 
           /* Update highest score encountered this far */           
-          vTemp = _mm_cmpeq_epi16(vH, vMaxScore);
-          equalMask = _mm_movemask_epi8(vTemp);
-          vTemp = _mm_cmpgt_epi16(vH, vMaxScore);
-          greaterMask = _mm_movemask_epi8(vTemp);
+          vTemp = vec_compareeq8sh(vH, vMaxScore);
+          equalMask = vec_extractupperbit16sb(vTemp);
+          vTemp = vec_comparegt8sh(vH, vMaxScore);
+          greaterMask = vec_extractupperbit16sb(vTemp);
 
           if (greaterMask != 0x0000)
             {
               vMaxScore = vH;
               // find largest score in the vMaxScore vector 
-              vTemp = _mm_srli_si128 (vMaxScore, 8);
-              vMaxScore = _mm_max_epi16 (vMaxScore, vTemp);
-              vTemp = _mm_srli_si128 (vMaxScore, 4);
-              vMaxScore = _mm_max_epi16 (vMaxScore, vTemp);
-              vTemp = _mm_srli_si128 (vMaxScore, 2);
-              vMaxScore = _mm_max_epi16 (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 8);
+              vMaxScore = vec_max8sh (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 4);
+              vMaxScore = vec_max8sh (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 2);
+              vMaxScore = vec_max8sh (vMaxScore, vTemp);
 
               // store in temporary variable 
-              score = (short)_mm_extract_epi16 (vMaxScore, 0);
-              vMaxScore = _mm_set1_epi16(score);
+              score = (short)vec_extract8sh (vMaxScore, 0);
+              vMaxScore = vec_splat8sh(score);
 
-              vTemp = _mm_cmpeq_epi16(vH, vMaxScore);
-              equalMask = _mm_movemask_epi8(vTemp);
+              vTemp = vec_compareeq8sh(vH, vMaxScore);
+              equalMask = vec_extractupperbit16sb(vTemp);
 
               target_end = i;
               n_best = 0;
@@ -603,70 +603,70 @@ void swStripedWord3(int              queryLength,
             }                                                                                
 
           /* get max from vH, vE and vF */ 
-          vH = _mm_max_epi16 (vH, vE);
-          vH = _mm_max_epi16 (vH, vF);                    
+          vH = vec_max8sh (vH, vE);
+          vH = vec_max8sh (vH, vF);                    
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);        
+          vec_store1q (pvHStore + j, vH);        
 
           /* update vE value */
-          vH = _mm_subs_epi16 (vH, vGapOpen);
-          vE = _mm_subs_epi16 (vE, vGapExtend);
-          vE = _mm_max_epi16 (vE, vH);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
+          vE = vec_subtractsaturating8sh (vE, vGapExtend);
+          vE = vec_max8sh (vE, vH);
 
           /* update vF value */
-          vF = _mm_subs_epi16 (vF, vGapExtend);
-          vF = _mm_max_epi16 (vF, vH);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
+          vF = vec_max8sh (vF, vH);
 
           /* save vE values */
-          _mm_store_si128 (pvE + j, vE);
+          vec_store1q (pvE + j, vE);
 
           /* load the next h value */
-          vH = _mm_load_si128 (pvHLoad + j);
+          vH = vec_load1q (pvHLoad + j);
         }
 
       /* reset pointers to the start of the saved data */
       j = 0;
-      vH = _mm_load_si128 (pvHStore + j);
+      vH = vec_load1q (pvHStore + j);
 
       /*  the computed vF value is for the given column.  since */
       /*  we are at the end, we need to shift the vF value over */
       /*  to the next column. */
-      vF = _mm_slli_si128 (vF, 2);
-      vF = _mm_or_si128 (vF, vMin);
-      vTemp = _mm_subs_epi16 (vH, vGapOpen);
-      vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-      cmp  = _mm_movemask_epi8 (vTemp);
+      vF = vec_shiftleftbytes1q (vF, 2);
+      vF = vec_bitor1q (vF, vMin);
+      vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+      vTemp = vec_comparegt8sh (vF, vTemp);
+      cmp  = vec_extractupperbit16sb (vTemp);
       while (cmp != 0x0000) 
         {                               
-          vH = _mm_max_epi16 (vH, vF);
+          vH = vec_max8sh (vH, vF);
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);
+          vec_store1q (pvHStore + j, vH);
 
           /*  update vE incase the new vH value would change it */
-          vH = _mm_subs_epi16 (vH, vGapOpen);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
 
-          vE = _mm_load_si128 (pvE + j);     
-          vE = _mm_max_epi16 (vE, vH);
-          _mm_store_si128 (pvE + j, vE);
+          vE = vec_load1q (pvE + j);     
+          vE = vec_max8sh (vE, vH);
+          vec_store1q (pvE + j, vE);
 
           /* update vF value */
-          vF = _mm_subs_epi16 (vF, vGapExtend);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
 
           ++j;
           if (j >= iter)
             {
               j = 0;
-              vF = _mm_slli_si128 (vF, 2);
-              vF = _mm_or_si128 (vF, vMin);
+              vF = vec_shiftleftbytes1q (vF, 2);
+              vF = vec_bitor1q (vF, vMin);
             }
 
-          vH = _mm_load_si128 (pvHStore + j);
+          vH = vec_load1q (pvHStore + j);
 
-          vTemp = _mm_subs_epi16 (vH, vGapOpen);
-          vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-          cmp  = _mm_movemask_epi8 (vTemp);
+          vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+          vTemp = vec_comparegt8sh (vF, vTemp);
+          cmp  = vec_extractupperbit16sb (vTemp);
         }
     }   
   (*_opt) = score+0x8000;
@@ -748,29 +748,29 @@ void swStripedWord2(int              queryLength,
 
   __m128i vMin, vTemp;
   __m128i *pvScore;
-  __m128i vZero = _mm_setzero_si128();
-  __m128i vInf = _mm_set1_epi16(0x8000);
+  __m128i vZero = vec_zero1q();
+  __m128i vInf = vec_splat8sh(0x8000);
 
   int n_best = 0, query_end = -1;
 
   /* Load gap opening penalty to all elements of a constant */
-  vGapOpen = _mm_set1_epi16 (gapOpen);
+  vGapOpen = vec_splat8sh (gapOpen);
 
   /* Load gap extension penalty to all elements of a constant */
-  vGapExtend = _mm_set1_epi16 (gapExtend);
+  vGapExtend = vec_splat8sh (gapExtend);
 
   /*  load vMaxScore with the vZero.  since we are using signed */
   /*  math, we will bias the maxscore to -32768 so we have the */
   /*  full range of the short. */
 
-  vMin = _mm_insert_epi16 (vZero, 0x8000, 0);
+  vMin = vec_insert8sh (vZero, 0x8000, 0);
   vMaxScore = vInf;    
 
   /* Zero out the storage vector */
   for (i = 0; i < iter; ++i)
     {
-      _mm_store_si128 (pvE + i, vInf);
-      _mm_store_si128 (pvHStore + i, vInf);
+      vec_store1q (pvE + i, vInf);
+      vec_store1q (pvHStore + i, vInf);
     }
 
   for (i = 0; i < dbLength; ++i)
@@ -782,9 +782,9 @@ void swStripedWord2(int              queryLength,
       vF = vInf;
 
       /* load the next h value */
-      vH = _mm_load_si128 (pvHStore + iter - 1);
-      vH = _mm_slli_si128 (vH, 2);
-      vH = _mm_or_si128 (vH, vMin);
+      vH = vec_load1q (pvHStore + iter - 1);
+      vH = vec_shiftleftbytes1q (vH, 2);
+      vH = vec_bitor1q (vH, vMin);
 
       pv = pvHLoad;
       pvHLoad = pvHStore;
@@ -793,76 +793,76 @@ void swStripedWord2(int              queryLength,
       for (j = 0; j < iter; ++j)
         {
           /* load values of vF and vH from previous row (one unit up) */
-          vE = _mm_load_si128 (pvE + j);
+          vE = vec_load1q (pvE + j);
 
           /* add score to vH */
-          vH = _mm_adds_epi16 (vH, *pvScore++);           
+          vH = vec_addsaturating8sh (vH, *pvScore++);           
 
           /* get max from vH, vE and vF */ 
-          vH = _mm_max_epi16 (vH, vE);
-          vH = _mm_max_epi16 (vH, vF);                 
+          vH = vec_max8sh (vH, vE);
+          vH = vec_max8sh (vH, vF);                 
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);        
+          vec_store1q (pvHStore + j, vH);        
 
           /* update vE value */
-          vH = _mm_subs_epi16 (vH, vGapOpen);
-          vE = _mm_subs_epi16 (vE, vGapExtend);
-          vE = _mm_max_epi16 (vE, vH);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
+          vE = vec_subtractsaturating8sh (vE, vGapExtend);
+          vE = vec_max8sh (vE, vH);
 
           /* update vF value */
-          vF = _mm_subs_epi16 (vF, vGapExtend);
-          vF = _mm_max_epi16 (vF, vH);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
+          vF = vec_max8sh (vF, vH);
 
           /* save vE values */
-          _mm_store_si128 (pvE + j, vE);
+          vec_store1q (pvE + j, vE);
 
           /* load the next h value */
-          vH = _mm_load_si128 (pvHLoad + j);
+          vH = vec_load1q (pvHLoad + j);
         }
 
       /* reset pointers to the start of the saved data */
       j = 0;
-      vH = _mm_load_si128 (pvHStore + j);
+      vH = vec_load1q (pvHStore + j);
 
       /*  the computed vF value is for the given column.  since */
       /*  we are at the end, we need to shift the vF value over */
       /*  to the next column. */
-      vF = _mm_slli_si128 (vF, 2);
-      vF = _mm_or_si128 (vF, vMin);
-      vTemp = _mm_subs_epi16 (vH, vGapOpen);
-      vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-      cmp  = _mm_movemask_epi8 (vTemp);
+      vF = vec_shiftleftbytes1q (vF, 2);
+      vF = vec_bitor1q (vF, vMin);
+      vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+      vTemp = vec_comparegt8sh (vF, vTemp);
+      cmp  = vec_extractupperbit16sb (vTemp);
       while (cmp != 0x0000) 
         {                               
-          vH = _mm_max_epi16 (vH, vF);
+          vH = vec_max8sh (vH, vF);
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);
+          vec_store1q (pvHStore + j, vH);
 
           /*  update vE incase the new vH value would change it */
-          vH = _mm_subs_epi16 (vH, vGapOpen);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
 
-          vE = _mm_load_si128 (pvE + j);     
-          vE = _mm_max_epi16 (vE, vH);
-          _mm_store_si128 (pvE + j, vE);
+          vE = vec_load1q (pvE + j);     
+          vE = vec_max8sh (vE, vH);
+          vec_store1q (pvE + j, vE);
 
           /* update vF value */
-          vF = _mm_subs_epi16 (vF, vGapExtend);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
 
           ++j;
           if (j >= iter)
             {
               j = 0;
-              vF = _mm_slli_si128 (vF, 2);
-              vF = _mm_or_si128 (vF, vMin);
+              vF = vec_shiftleftbytes1q (vF, 2);
+              vF = vec_bitor1q (vF, vMin);
             }
 
-          vH = _mm_load_si128 (pvHStore + j);
+          vH = vec_load1q (pvHStore + j);
 
-          vTemp = _mm_subs_epi16 (vH, vGapOpen);
-          vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-          cmp  = _mm_movemask_epi8 (vTemp);
+          vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+          vTemp = vec_comparegt8sh (vF, vTemp);
+          cmp  = vec_extractupperbit16sb (vTemp);
         }
     }   
 
@@ -930,29 +930,29 @@ void swStripedWord1(int              queryLength,
   __m128i vMin;
   __m128i vTemp;
   __m128i *pvScore;
-  __m128i vZero = _mm_setzero_si128();
-  __m128i vInf = _mm_set1_epi16(0x8000);
+  __m128i vZero = vec_zero1q();
+  __m128i vInf = vec_splat8sh(0x8000);
 
   int equalMask, greaterMask;
   int n_best = 0, query_end = -1, target_end = -1;
 
   /* Load gap opening penalty to all elements of a constant */
-  vGapOpen = _mm_set1_epi16 (gapOpen);
+  vGapOpen = vec_splat8sh (gapOpen);
 
   /* Load gap extension penalty to all elements of a constant */
-  vGapExtend = _mm_set1_epi16 (gapExtend);
+  vGapExtend = vec_splat8sh (gapExtend);
 
   /*  load vMaxScore with the vZero.  since we are using signed */
   /*  math, we will bias the maxscore to -32768 so we have the */
   /*  full range of the short. */
 
-  vMin = _mm_insert_epi16 (vZero, 0x8000, 0);
+  vMin = vec_insert8sh (vZero, 0x8000, 0);
   vMaxScore = vInf;    
 
   /* Zero out the storage vector */
   for (i = 0; i < iter; ++i) {        
-      _mm_store_si128 (pvHStore + i, vZero);
-      _mm_store_si128 (pvE + i, vInf);
+      vec_store1q (pvHStore + i, vZero);
+      vec_store1q (pvE + i, vInf);
   }    
 
   for (i = 0; i < dbLength; ++i)
@@ -964,10 +964,10 @@ void swStripedWord1(int              queryLength,
       vF = vInf;            
 
       /* load the next h value */
-      vH = _mm_load_si128 (pvHStore + iter - 1);
-      vH = _mm_slli_si128 (vH, 2);    
+      vH = vec_load1q (pvHStore + iter - 1);
+      vH = vec_shiftleftbytes1q (vH, 2);    
       if (i > 0)
-        vH = _mm_insert_epi16 (vH, -gapOpen - (i-1) * gapExtend, 0);
+        vH = vec_insert8sh (vH, -gapOpen - (i-1) * gapExtend, 0);
 
       pv = pvHLoad;
       pvHLoad = pvHStore;
@@ -976,46 +976,46 @@ void swStripedWord1(int              queryLength,
       for (j = 0; j < iter; ++j)
         {
           /* load values of vF and vH from previous row (one unit up) */
-          vE = _mm_load_si128 (pvE + j);
+          vE = vec_load1q (pvE + j);
 
           if (i == 0)                 
-            vE = _mm_subs_epi16(vZero, vGapOpen);
+            vE = vec_subtractsaturating8sh(vZero, vGapOpen);
           else
-            vE = _mm_load_si128 (pvE + j);
+            vE = vec_load1q (pvE + j);
 
           /* add score to vH */
-          vH = _mm_adds_epi16 (vH, *pvScore++);  
+          vH = vec_addsaturating8sh (vH, *pvScore++);  
 
           /* get max from vH, vE and vF */ 
-          vH = _mm_max_epi16 (vH, vE);
-          vH = _mm_max_epi16 (vH, vF);                    
+          vH = vec_max8sh (vH, vE);
+          vH = vec_max8sh (vH, vF);                    
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);        
+          vec_store1q (pvHStore + j, vH);        
 
           /* Update  highest score encountered this far */           
-          vTemp = _mm_cmpeq_epi16(vH, vMaxScore);
-          equalMask = _mm_movemask_epi8(vTemp);
-          vTemp = _mm_cmpgt_epi16(vH, vMaxScore);
-          greaterMask = _mm_movemask_epi8(vTemp);
+          vTemp = vec_compareeq8sh(vH, vMaxScore);
+          equalMask = vec_extractupperbit16sb(vTemp);
+          vTemp = vec_comparegt8sh(vH, vMaxScore);
+          greaterMask = vec_extractupperbit16sb(vTemp);
 
           if (greaterMask != 0x0000)
             {
               vMaxScore = vH;
               // find largest score in the vMaxScore vector 
-              vTemp = _mm_srli_si128 (vMaxScore, 8);
-              vMaxScore = _mm_max_epi16 (vMaxScore, vTemp);
-              vTemp = _mm_srli_si128 (vMaxScore, 4);
-              vMaxScore = _mm_max_epi16 (vMaxScore, vTemp);
-              vTemp = _mm_srli_si128 (vMaxScore, 2);
-              vMaxScore = _mm_max_epi16 (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 8);
+              vMaxScore = vec_max8sh (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 4);
+              vMaxScore = vec_max8sh (vMaxScore, vTemp);
+              vTemp = vec_shiftrightbytes1q (vMaxScore, 2);
+              vMaxScore = vec_max8sh (vMaxScore, vTemp);
 
               // store in temporary variable 
-              score = (short)_mm_extract_epi16 (vMaxScore, 0);
-              vMaxScore = _mm_set1_epi16(score);
+              score = (short)vec_extract8sh (vMaxScore, 0);
+              vMaxScore = vec_splat8sh(score);
 
-              vTemp = _mm_cmpeq_epi16(vH, vMaxScore);
-              equalMask = _mm_movemask_epi8(vTemp);
+              vTemp = vec_compareeq8sh(vH, vMaxScore);
+              equalMask = vec_extractupperbit16sb(vTemp);
 
               target_end = i;
               n_best = 0;
@@ -1052,65 +1052,65 @@ void swStripedWord1(int              queryLength,
             }                                                                                            
 
           /* update vE value */
-          vH = _mm_subs_epi16 (vH, vGapOpen);
-          vE = _mm_subs_epi16 (vE, vGapExtend);
-          vE = _mm_max_epi16 (vE, vH);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
+          vE = vec_subtractsaturating8sh (vE, vGapExtend);
+          vE = vec_max8sh (vE, vH);
 
           /* update vF value */
-          vF = _mm_subs_epi16 (vF, vGapExtend);
-          vF = _mm_max_epi16 (vF, vH);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
+          vF = vec_max8sh (vF, vH);
 
           /* save vE values */
-          _mm_store_si128 (pvE + j, vE);
+          vec_store1q (pvE + j, vE);
 
           /* load the next h value */
-          vH = _mm_load_si128 (pvHLoad + j);
+          vH = vec_load1q (pvHLoad + j);
         }
 
       /* reset pointers to the start of the saved data */
       j = 0;
-      vH = _mm_load_si128 (pvHStore + j);
+      vH = vec_load1q (pvHStore + j);
 
       /*  the computed vF value is for the given column.  since */
       /*  we are at the end, we need to shift the vF value over */
       /*  to the next column. */        
 
-      vF = _mm_slli_si128 (vF, 2);
-      vF = _mm_or_si128 (vF, vMin);
+      vF = vec_shiftleftbytes1q (vF, 2);
+      vF = vec_bitor1q (vF, vMin);
 
-      vTemp = _mm_subs_epi16 (vH, vGapOpen);
-      vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-      cmp  = _mm_movemask_epi8 (vTemp);
+      vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+      vTemp = vec_comparegt8sh (vF, vTemp);
+      cmp  = vec_extractupperbit16sb (vTemp);
       while (cmp != 0x0000) 
         {                                                           
-          vH = _mm_max_epi16 (vH, vF);
+          vH = vec_max8sh (vH, vF);
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);
+          vec_store1q (pvHStore + j, vH);
 
           /*  update vE incase the new vH value would change it */
-          vH = _mm_subs_epi16 (vH, vGapOpen);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
 
-          vE = _mm_load_si128 (pvE + j);     
-          vE = _mm_max_epi16 (vE, vH);
-          _mm_store_si128 (pvE + j, vE);
+          vE = vec_load1q (pvE + j);     
+          vE = vec_max8sh (vE, vH);
+          vec_store1q (pvE + j, vE);
 
           /* update vF value */
-          vF = _mm_subs_epi16 (vF, vGapExtend);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
 
           ++j;
           if (j >= iter)
             {
               j = 0;            
-              vF = _mm_slli_si128 (vF, 2);
-              vF = _mm_or_si128 (vF, vMin);
+              vF = vec_shiftleftbytes1q (vF, 2);
+              vF = vec_bitor1q (vF, vMin);
             }            
 
-          vH = _mm_load_si128 (pvHStore + j);
+          vH = vec_load1q (pvHStore + j);
 
-          vTemp = _mm_subs_epi16 (vH, vGapOpen);
-          vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-          cmp  = _mm_movemask_epi8 (vTemp);
+          vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+          vTemp = vec_comparegt8sh (vF, vTemp);
+          cmp  = vec_extractupperbit16sb (vTemp);
         }
     }   
 
@@ -1146,28 +1146,28 @@ void swStripedWord0(int              queryLength,
 
   __m128i vMin, vTemp;
   __m128i *pvScore;
-  __m128i vZero = _mm_setzero_si128();
-  __m128i vInf = _mm_set1_epi16(0x8000);
+  __m128i vZero = vec_zero1q();
+  __m128i vInf = vec_splat8sh(0x8000);
 
   int n_best = 0, query_end = -1;
 
   /* Load gap opening penalty to all elements of a constant */
-  vGapOpen = _mm_set1_epi16 (gapOpen);
+  vGapOpen = vec_splat8sh (gapOpen);
 
   /* Load gap extension penalty to all elements of a constant */
-  vGapExtend = _mm_set1_epi16 (gapExtend);
+  vGapExtend = vec_splat8sh (gapExtend);
 
   /*  load vMaxScore with the vZero.  since we are using signed */
   /*  math, we will bias the maxscore to -32768 so we have the */
   /*  full range of the short. */
 
-  vMin = _mm_insert_epi16 (vZero, 0x8000, 0);
+  vMin = vec_insert8sh (vZero, 0x8000, 0);
   vMaxScore = vInf;    
 
   /* Zero out the storage vector */        
   for (i = 0; i < iter; ++i) {                
-      _mm_store_si128 (pvHStore + i, vZero);
-      _mm_store_si128 (pvE + i, vInf);
+      vec_store1q (pvHStore + i, vZero);
+      vec_store1q (pvE + i, vInf);
   }    
 
   for (i = 0; i < dbLength; ++i)
@@ -1179,10 +1179,10 @@ void swStripedWord0(int              queryLength,
       vF = vInf;    
 
       /* load the next h value */
-      vH = _mm_load_si128 (pvHStore + iter - 1);
-      vH = _mm_slli_si128 (vH, 2);    
+      vH = vec_load1q (pvHStore + iter - 1);
+      vH = vec_shiftleftbytes1q (vH, 2);    
       if (i > 0)
-        vH = _mm_insert_epi16 (vH, -gapOpen - (i-1) * gapExtend, 0);
+        vH = vec_insert8sh (vH, -gapOpen - (i-1) * gapExtend, 0);
 
       pv = pvHLoad;
       pvHLoad = pvHStore;
@@ -1192,78 +1192,78 @@ void swStripedWord0(int              queryLength,
         {
           /* load values of vF and vH from previous row (one unit up) */                        
           if (i == 0)                 
-            vE = _mm_subs_epi16(vZero, vGapOpen);
+            vE = vec_subtractsaturating8sh(vZero, vGapOpen);
           else
-            vE = _mm_load_si128 (pvE + j);
+            vE = vec_load1q (pvE + j);
 
           /* add score to vH */
-          vH = _mm_adds_epi16 (vH, *pvScore++);           
+          vH = vec_addsaturating8sh (vH, *pvScore++);           
 
           /* get max from vH, vE and vF */ 
-          vH = _mm_max_epi16 (vH, vE);
-          vH = _mm_max_epi16 (vH, vF);                 
+          vH = vec_max8sh (vH, vE);
+          vH = vec_max8sh (vH, vF);                 
 
           /* save vH values */
-          _mm_store_si128 (pvHStore + j, vH);        
+          vec_store1q (pvHStore + j, vH);        
 
           /* update vE value */
-          vH = _mm_subs_epi16 (vH, vGapOpen);
-          vE = _mm_subs_epi16 (vE, vGapExtend);
-          vE = _mm_max_epi16 (vE, vH);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
+          vE = vec_subtractsaturating8sh (vE, vGapExtend);
+          vE = vec_max8sh (vE, vH);
 
           /* update vF value */
-          vF = _mm_subs_epi16 (vF, vGapExtend);
-          vF = _mm_max_epi16 (vF, vH);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
+          vF = vec_max8sh (vF, vH);
 
           /* save vE values */
-          _mm_store_si128 (pvE + j, vE);
+          vec_store1q (pvE + j, vE);
 
           /* load the next h value */
-          vH = _mm_load_si128 (pvHLoad + j);
+          vH = vec_load1q (pvHLoad + j);
         }
 
       // reset pointers to the start of the saved data 
       j = 0;
-      vH = _mm_load_si128 (pvHStore + j);
+      vH = vec_load1q (pvHStore + j);
 
       //  the computed vF value is for the given column.  since 
       //  we are at the end, we need to shift the vF value over 
       //  to the next column. 
-      vF = _mm_slli_si128 (vF, 2);
-      vF = _mm_or_si128 (vF, vMin);
-      vTemp = _mm_subs_epi16 (vH, vGapOpen);
-      vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-      cmp  = _mm_movemask_epi8 (vTemp);
+      vF = vec_shiftleftbytes1q (vF, 2);
+      vF = vec_bitor1q (vF, vMin);
+      vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+      vTemp = vec_comparegt8sh (vF, vTemp);
+      cmp  = vec_extractupperbit16sb (vTemp);
       while (cmp != 0x0000) 
         {                               
-          vH = _mm_max_epi16 (vH, vF);
+          vH = vec_max8sh (vH, vF);
 
           // save vH values 
-          _mm_store_si128 (pvHStore + j, vH);
+          vec_store1q (pvHStore + j, vH);
 
           //  update vE incase the new vH value would change it 
-          vH = _mm_subs_epi16 (vH, vGapOpen);
+          vH = vec_subtractsaturating8sh (vH, vGapOpen);
 
-          vE = _mm_load_si128 (pvE + j);     
-          vE = _mm_max_epi16 (vE, vH);
-          _mm_store_si128 (pvE + j, vE);
+          vE = vec_load1q (pvE + j);     
+          vE = vec_max8sh (vE, vH);
+          vec_store1q (pvE + j, vE);
 
           // update vF value 
-          vF = _mm_subs_epi16 (vF, vGapExtend);
+          vF = vec_subtractsaturating8sh (vF, vGapExtend);
 
           ++j;
           if (j >= iter)
             {
               j = 0;
-              vF = _mm_slli_si128 (vF, 2);
-              vF = _mm_or_si128 (vF, vMin);            
+              vF = vec_shiftleftbytes1q (vF, 2);
+              vF = vec_bitor1q (vF, vMin);            
             }
 
-          vH = _mm_load_si128 (pvHStore + j);
+          vH = vec_load1q (pvHStore + j);
 
-          vTemp = _mm_subs_epi16 (vH, vGapOpen);
-          vTemp = _mm_cmpgt_epi16 (vF, vTemp);
-          cmp  = _mm_movemask_epi8 (vTemp);
+          vTemp = vec_subtractsaturating8sh (vH, vGapOpen);
+          vTemp = vec_comparegt8sh (vF, vTemp);
+          cmp  = vec_extractupperbit16sb (vTemp);
         }        
     }   
 
--- src/sw/lib/Solution4.cpp
+++ src/sw/lib/Solution4.cpp
@@ -25,7 +25,7 @@
 #include <iostream>
 #include <sstream>
 #include <set>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <map>
 #include <stdint.h>
 #include <inttypes.h>
@@ -78,11 +78,11 @@ const int INF = (1<<14);
 const int MIN_VAL = -(1<<15);
 
 INLINE int16_t Solution4::findMax16(const __m128i &mMax) {
-    __m128i mshift = _mm_srli_si128(mMax, 2);
-    __m128i m = _mm_max_epi16(mshift, mMax);
-    mshift = _mm_srli_si128(m, 4);
-    m = _mm_max_epi16(mshift, m);
-    return max(_mm_extract_epi16(m, 0), _mm_extract_epi16(m, 4));
+    __m128i mshift = vec_shiftrightbytes1q(mMax, 2);
+    __m128i m = vec_max8sh(mshift, mMax);
+    mshift = vec_shiftrightbytes1q(m, 4);
+    m = vec_max8sh(mshift, m);
+    return max(vec_extract8sh(m, 0), vec_extract8sh(m, 4));
     /*
     int16_t* p = (int16_t*)&m;
     return max(p[0], p[4]);
@@ -91,10 +91,10 @@ INLINE int16_t Solution4::findMax16(const __m128i &mMax) {
 
 INLINE int16_t Solution4::findMax16Simple(const __m128i &mMax) {
     __m128i m = mMax;
-    m = _mm_max_epi16(m, _mm_srli_si128(m, 8)); 
-    m = _mm_max_epi16(m, _mm_srli_si128(m, 4)); 
-    m = _mm_max_epi16(m, _mm_srli_si128(m, 2)); 
-    return (int16_t)(_mm_extract_epi16(m, 0) & 0xffff); 
+    m = vec_max8sh(m, vec_shiftrightbytes1q(m, 8)); 
+    m = vec_max8sh(m, vec_shiftrightbytes1q(m, 4)); 
+    m = vec_max8sh(m, vec_shiftrightbytes1q(m, 2)); 
+    return (int16_t)(vec_extract8sh(m, 0) & 0xffff); 
     /*
     int16_t* p = (int16_t*)&mMax;
     int16_t mx = p[0];
@@ -111,18 +111,18 @@ INLINE int16_t Solution4::findMax16Simple(const __m128i &mMax) {
 
 /*
    INLINE uint8_t findMax8(const __m128i &mMax) {
-   __m128i mshift = _mm_srli_si128(mMax, 1);
-   __m128i m = _mm_max_epu8(mshift, mMax);
+   __m128i mshift = vec_shiftrightbytes1q(mMax, 1);
+   __m128i m = vec_max16ub(mshift, mMax);
    uint8_t* p = (uint8_t*)&m;
    return max(max(max(p[0], p[2]), max(p[4], p[6])), max(max(p[8], p[10]), max(p[12], p[14])));    
    }
    */
 
 INLINE uint8_t Solution4::findMax8(const __m128i &mMax) {
-    __m128i mshift = _mm_srli_si128(mMax, 1);
-    __m128i m = _mm_max_epu8(mshift, mMax);
-    mshift = _mm_srli_si128(m, 2);
-    m = _mm_max_epu8(mshift, m);
+    __m128i mshift = vec_shiftrightbytes1q(mMax, 1);
+    __m128i m = vec_max16ub(mshift, mMax);
+    mshift = vec_shiftrightbytes1q(m, 2);
+    m = vec_max16ub(mshift, m);
     uint8_t* p = (uint8_t*)&m;
     uint8_t mx = p[0];
     mx = max(mx, p[4]);
@@ -150,28 +150,28 @@ NOINLINE uint64_t Solution4::hashDNA(const string &s, const int len) {
         return h;
     }
 
-    __m128i mhash = _mm_set1_epi16(1);
-    __m128i mmul = _mm_set1_epi16(13);
-    __m128i mzero = _mm_setzero_si128();
+    __m128i mhash = vec_splat8sh(1);
+    __m128i mmul = vec_splat8sh(13);
+    __m128i mzero = vec_zero1q();
     __m128i m0, m1a, m1b;
     int i = 0;
     while (i + 16 < len) {
-        m0 = _mm_loadu_si128((__m128i*)&s[i]);
-        m1a = _mm_unpacklo_epi8(m0, mzero);
-        m1b = _mm_unpackhi_epi8(m0, mzero);
-        mhash = _mm_mullo_epi16(mhash, mmul);
-        mhash = _mm_add_epi16(mhash, m1a);
-        mhash = _mm_mullo_epi16(mhash, mmul);
-        mhash = _mm_add_epi16(mhash, m1b);
+        m0 = vec_load1qu((__m128i*)&s[i]);
+        m1a = vec_unpacklow88sb(m0, mzero);
+        m1b = vec_unpackhigh88sb(m0, mzero);
+        mhash = vec_multiply8sh(mhash, mmul);
+        mhash = vec_add8sh(mhash, m1a);
+        mhash = vec_multiply8sh(mhash, mmul);
+        mhash = vec_add8sh(mhash, m1b);
         i += 16;
     }
-    m0 = _mm_loadu_si128((__m128i*)&s[len - 16]);
-    m1a = _mm_unpacklo_epi8(m0, mzero);
-    m1b = _mm_unpackhi_epi8(m0, mzero);
-    mhash = _mm_mullo_epi16(mhash, mmul);
-    mhash = _mm_add_epi16(mhash, m1a);
-    mhash = _mm_mullo_epi16(mhash, mmul);
-    mhash = _mm_add_epi16(mhash, m1b);
+    m0 = vec_load1qu((__m128i*)&s[len - 16]);
+    m1a = vec_unpacklow88sb(m0, mzero);
+    m1b = vec_unpackhigh88sb(m0, mzero);
+    mhash = vec_multiply8sh(mhash, mmul);
+    mhash = vec_add8sh(mhash, m1a);
+    mhash = vec_multiply8sh(mhash, mmul);
+    mhash = vec_add8sh(mhash, m1b);
 
     /*
     uint32_t* p = (uint32_t*)&mhash;
@@ -180,14 +180,14 @@ NOINLINE uint64_t Solution4::hashDNA(const string &s, const int len) {
     h = h * 1337 + p[2];
     h = h * 1337 + p[3];
     */
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 0);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 1);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 2);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 3);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 4);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 5);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 6);
-    h = h * 1337 + (uint64_t)_mm_extract_epi16(mhash, 7);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 0);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 1);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 2);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 3);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 4);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 5);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 6);
+    h = h * 1337 + (uint64_t)vec_extract8sh(mhash, 7);
     return h;
 }
 
@@ -270,8 +270,8 @@ template <class T, bool BYTE> INLINE void Solution4::updateResult(int i, int cur
             }
 #ifdef FAST_UPDATE
         } else {
-            __m128i mopt = _mm_set1_epi8(opt);
-            for (int k = 0; k < len; k += 16) if (_mm_movemask_epi8(_mm_cmpeq_epi8(*(__m128i*)&((uint8_t*)M1)[k], mopt))) {
+            __m128i mopt = vec_splat16sb(opt);
+            for (int k = 0; k < len; k += 16) if (vec_extractupperbit16sb(vec_compareeq16sb(*(__m128i*)&((uint8_t*)M1)[k], mopt))) {
                 FOR(j, k, k + 16) {
                     if (MM[j] == opt) {
                         n_best++;
@@ -308,76 +308,76 @@ template <class T, int DEFAULT_VALUE> INLINE void Solution4::updateResultLast()
 
 template <int qec> NOINLINE void Solution4::processFastVariantB16BitA(const string &a, int mm, int mi, int o, int e) {
 
-    __m128i mo = _mm_set1_epi16(o + e);
-    __m128i me = _mm_set1_epi16(e);
-    __m128i minf = _mm_set1_epi16(-INF);
+    __m128i mo = vec_splat8sh(o + e);
+    __m128i me = vec_splat8sh(e);
+    __m128i minf = vec_splat8sh(-INF);
 
 
-    REP(i, segNo) M0[i] = _mm_setzero_si128();
+    REP(i, segNo) M0[i] = vec_zero1q();
     REP(i, segNo) V[i] = minf;
 
     REP(i, m) {
         __m128i mmin;            
         if (qec) {
-            mmin = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * mm, o + e + e * i));
+            mmin = vec_splat8sh(max(max(lastMax, opt) - (m - i) * mm, o + e + e * i));
         } else {
-            //mmin = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * (mm - e) + o, o + e + e * i));
-            mmin = _mm_set1_epi16(o + e + e * i);
+            //mmin = vec_splat8sh(max(max(lastMax, opt) - (m - i) * (mm - e) + o, o + e + e * i));
+            mmin = vec_splat8sh(o + e + e * i);
         }
 
-        __m128i mM = _mm_load_si128(&M0[segNo - 1]);
-        mM = _mm_slli_si128(mM, 2);
-        if (i) mM = _mm_insert_epi16(mM, o + e * i    , 0);
+        __m128i mM = vec_load1q(&M0[segNo - 1]);
+        mM = vec_shiftleftbytes1q(mM, 2);
+        if (i) mM = vec_insert8sh(mM, o + e * i    , 0);
 
-        __m128i mH = _mm_set1_epi16(o + e + e * i);
+        __m128i mH = vec_splat8sh(o + e + e * i);
 
         __m128i mMax;
         if (qec) 
-          mMax = _mm_setzero_si128();
+          mMax = vec_zero1q();
 
         __m128i *P = XP[(int)a[i]];
 
         REP(j, segNo) {
-            __m128i mP = _mm_load_si128(&P[j]);
-            __m128i mV = _mm_load_si128(&V[j]);
+            __m128i mP = vec_load1q(&P[j]);
+            __m128i mV = vec_load1q(&V[j]);
 
-            mM = _mm_add_epi16(mM, mP);
+            mM = vec_add8sh(mM, mP);
 
             if (qec)
-              mMax = _mm_max_epi16(mMax, mM);
+              mMax = vec_max8sh(mMax, mM);
 
-            mM = _mm_max_epi16(mM, mV);
-            mM = _mm_max_epi16(mM, mH);
-            mM = _mm_max_epi16(mM, mmin);
+            mM = vec_max8sh(mM, mV);
+            mM = vec_max8sh(mM, mH);
+            mM = vec_max8sh(mM, mmin);
 
-            _mm_store_si128(&M1[j], mM);
+            vec_store1q(&M1[j], mM);
 
-            mM = _mm_add_epi16(mM, mo);
-            mV = _mm_add_epi16(mV, me);
-            mH = _mm_add_epi16(mH, me);
-            mV = _mm_max_epi16(mV, mM);
-            mH = _mm_max_epi16(mH, mM);
+            mM = vec_add8sh(mM, mo);
+            mV = vec_add8sh(mV, me);
+            mH = vec_add8sh(mH, me);
+            mV = vec_max8sh(mV, mM);
+            mH = vec_max8sh(mH, mM);
 
-            mM  = _mm_load_si128(&M0[j]);
+            mM  = vec_load1q(&M0[j]);
 
-            _mm_store_si128(&V[j], mV);
+            vec_store1q(&V[j], mV);
         }
 
         while (true) {
-            mH = _mm_slli_si128(mH, 2);
-            mH = _mm_insert_epi16(mH, -INF, 0);
+            mH = vec_shiftleftbytes1q(mH, 2);
+            mH = vec_insert8sh(mH, -INF, 0);
 
             REP(j, segNo) {
-                mM = _mm_load_si128(&M1[j]);
+                mM = vec_load1q(&M1[j]);
 
-                __m128i mtmp = _mm_add_epi16(mM, mo);
-                __m128i mcmp = _mm_cmpgt_epi16(mH, mtmp);
-                if (_mm_movemask_epi8(mcmp) == 0) goto outB11;
+                __m128i mtmp = vec_add8sh(mM, mo);
+                __m128i mcmp = vec_comparegt8sh(mH, mtmp);
+                if (vec_extractupperbit16sb(mcmp) == 0) goto outB11;
 
-                mM = _mm_max_epi16(mM, mH);
-                _mm_store_si128(&M1[j], mM);
+                mM = vec_max8sh(mM, mH);
+                vec_store1q(&M1[j], mM);
 
-                mH = _mm_add_epi16(mH, me);
+                mH = vec_add8sh(mH, me);
             }
         }
 outB11:
@@ -401,76 +401,76 @@ outB11:
 
 template <int qec> NOINLINE void Solution4::processFastVariantB16BitB(const string &a, int mm, int mi, int o, int e) {
 
-    __m128i mo = _mm_set1_epi16(o + e);
-    __m128i me = _mm_set1_epi16(e);
-    __m128i minf = _mm_set1_epi16(-INF);
+    __m128i mo = vec_splat8sh(o + e);
+    __m128i me = vec_splat8sh(e);
+    __m128i minf = vec_splat8sh(-INF);
 
-    REP(i, segNo) M0[i] = _mm_setzero_si128();
+    REP(i, segNo) M0[i] = vec_zero1q();
     REP(i, segNo) V[i] = minf;
 
     int midstep = qec ? m : m * 4 / 5;
     REP(i, midstep) {
         __m128i mmin;            
         if (qec) {
-            mmin = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * mm, o + e + e * i));
+            mmin = vec_splat8sh(max(max(lastMax, opt) - (m - i) * mm, o + e + e * i));
         } else {
-            //mmin = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * (mm - e) + o, o + e + e * i));
-            mmin = _mm_set1_epi16(o + e + e * i);
+            //mmin = vec_splat8sh(max(max(lastMax, opt) - (m - i) * (mm - e) + o, o + e + e * i));
+            mmin = vec_splat8sh(o + e + e * i);
         }
 
-        __m128i mM = _mm_load_si128(&M0[segNo - 1]);
-        mM = _mm_slli_si128(mM, 2);
-        if (i) mM = _mm_insert_epi16(mM, o + e * i    , 0);
+        __m128i mM = vec_load1q(&M0[segNo - 1]);
+        mM = vec_shiftleftbytes1q(mM, 2);
+        if (i) mM = vec_insert8sh(mM, o + e * i    , 0);
 
-        __m128i mH = _mm_set1_epi16(o + e + e * i);
+        __m128i mH = vec_splat8sh(o + e + e * i);
 
         __m128i mMax;
         if (qec) 
-          mMax = _mm_setzero_si128();
+          mMax = vec_zero1q();
 
         __m128i *P = XP[(int)a[i]];
 
         REP(j, segNo) {
-            __m128i mP = _mm_load_si128(&P[j]);
-            __m128i mV = _mm_load_si128(&V[j]);
+            __m128i mP = vec_load1q(&P[j]);
+            __m128i mV = vec_load1q(&V[j]);
 
-            mM = _mm_add_epi16(mM, mP);
+            mM = vec_add8sh(mM, mP);
 
             if (qec)
-              mMax = _mm_max_epi16(mMax, mM);
+              mMax = vec_max8sh(mMax, mM);
 
-            mM = _mm_max_epi16(mM, mV);
-            mM = _mm_max_epi16(mM, mH);
-            mM = _mm_max_epi16(mM, mmin);
+            mM = vec_max8sh(mM, mV);
+            mM = vec_max8sh(mM, mH);
+            mM = vec_max8sh(mM, mmin);
 
-            _mm_store_si128(&M1[j], mM);
+            vec_store1q(&M1[j], mM);
 
-            mM = _mm_add_epi16(mM, mo);
-            mV = _mm_add_epi16(mV, me);
-            mH = _mm_add_epi16(mH, me);
-            mV = _mm_max_epi16(mV, mM);
-            mH = _mm_max_epi16(mH, mM);
+            mM = vec_add8sh(mM, mo);
+            mV = vec_add8sh(mV, me);
+            mH = vec_add8sh(mH, me);
+            mV = vec_max8sh(mV, mM);
+            mH = vec_max8sh(mH, mM);
 
-            mM  = _mm_load_si128(&M0[j]);
+            mM  = vec_load1q(&M0[j]);
 
-            _mm_store_si128(&V[j], mV);
+            vec_store1q(&V[j], mV);
         }
 
         while (true) {
-            mH = _mm_slli_si128(mH, 2);
-            mH = _mm_insert_epi16(mH, -INF, 0);
+            mH = vec_shiftleftbytes1q(mH, 2);
+            mH = vec_insert8sh(mH, -INF, 0);
 
             REP(j, segNo) {
-                mM = _mm_load_si128(&M1[j]);
+                mM = vec_load1q(&M1[j]);
 
-                __m128i mtmp = _mm_add_epi16(mM, mo);
-                __m128i mcmp = _mm_cmpgt_epi16(mH, mtmp);
-                if (_mm_movemask_epi8(mcmp) == 0) goto outB12;
+                __m128i mtmp = vec_add8sh(mM, mo);
+                __m128i mcmp = vec_comparegt8sh(mH, mtmp);
+                if (vec_extractupperbit16sb(mcmp) == 0) goto outB12;
 
-                mM = _mm_max_epi16(mM, mH);
-                _mm_store_si128(&M1[j], mM);
+                mM = vec_max8sh(mM, mH);
+                vec_store1q(&M1[j], mM);
 
-                mH = _mm_add_epi16(mH, me);
+                mH = vec_add8sh(mH, me);
             }
         }
 outB12:
@@ -490,65 +490,65 @@ outB12:
         FOR(i, midstep, m) {
             __m128i mmin;            
             if (qec) {
-                mmin = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * mm, o + e + e * i));
+                mmin = vec_splat8sh(max(max(lastMax, opt) - (m - i) * mm, o + e + e * i));
             } else {
-                mmin = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * (mm - e) + o, o + e + e * i));
-                //mmin = _mm_set1_epi16(o + e + e * i);
+                mmin = vec_splat8sh(max(max(lastMax, opt) - (m - i) * (mm - e) + o, o + e + e * i));
+                //mmin = vec_splat8sh(o + e + e * i);
             }
 
-            __m128i mM = _mm_load_si128(&M0[segNo - 1]);
-            mM = _mm_slli_si128(mM, 2);
-            if (i) mM = _mm_insert_epi16(mM, o + e * i    , 0);
+            __m128i mM = vec_load1q(&M0[segNo - 1]);
+            mM = vec_shiftleftbytes1q(mM, 2);
+            if (i) mM = vec_insert8sh(mM, o + e * i    , 0);
 
-            __m128i mH = _mm_set1_epi16(o + e + e * i);
+            __m128i mH = vec_splat8sh(o + e + e * i);
 
             __m128i mMax;
             //if (qec) 
-            mMax = _mm_setzero_si128();
+            mMax = vec_zero1q();
 
             __m128i *P = XP[(int)a[i]];
 
             REP(j, segNo) {
-                __m128i mP = _mm_load_si128(&P[j]);
-                __m128i mV = _mm_load_si128(&V[j]);
+                __m128i mP = vec_load1q(&P[j]);
+                __m128i mV = vec_load1q(&V[j]);
 
-                mM = _mm_add_epi16(mM, mP);
+                mM = vec_add8sh(mM, mP);
 
                 //if (qec)
-                mMax = _mm_max_epi16(mMax, mM);
+                mMax = vec_max8sh(mMax, mM);
 
-                mM = _mm_max_epi16(mM, mV);
-                mM = _mm_max_epi16(mM, mH);
-                mM = _mm_max_epi16(mM, mmin);
+                mM = vec_max8sh(mM, mV);
+                mM = vec_max8sh(mM, mH);
+                mM = vec_max8sh(mM, mmin);
 
-                _mm_store_si128(&M1[j], mM);
+                vec_store1q(&M1[j], mM);
 
-                mM = _mm_add_epi16(mM, mo);
-                mV = _mm_add_epi16(mV, me);
-                mH = _mm_add_epi16(mH, me);
-                mV = _mm_max_epi16(mV, mM);
-                mH = _mm_max_epi16(mH, mM);
+                mM = vec_add8sh(mM, mo);
+                mV = vec_add8sh(mV, me);
+                mH = vec_add8sh(mH, me);
+                mV = vec_max8sh(mV, mM);
+                mH = vec_max8sh(mH, mM);
 
-                mM  = _mm_load_si128(&M0[j]);
+                mM  = vec_load1q(&M0[j]);
 
-                _mm_store_si128(&V[j], mV);
+                vec_store1q(&V[j], mV);
             }
 
             while (true) {
-                mH = _mm_slli_si128(mH, 2);
-                mH = _mm_insert_epi16(mH, -INF, 0);
+                mH = vec_shiftleftbytes1q(mH, 2);
+                mH = vec_insert8sh(mH, -INF, 0);
 
                 REP(j, segNo) {
-                    mM = _mm_load_si128(&M1[j]);
+                    mM = vec_load1q(&M1[j]);
 
-                    __m128i mtmp = _mm_add_epi16(mM, mo);
-                    __m128i mcmp = _mm_cmpgt_epi16(mH, mtmp);
-                    if (_mm_movemask_epi8(mcmp) == 0) goto outB22;
+                    __m128i mtmp = vec_add8sh(mM, mo);
+                    __m128i mcmp = vec_comparegt8sh(mH, mtmp);
+                    if (vec_extractupperbit16sb(mcmp) == 0) goto outB22;
 
-                    mM = _mm_max_epi16(mM, mH);
-                    _mm_store_si128(&M1[j], mM);
+                    mM = vec_max8sh(mM, mH);
+                    vec_store1q(&M1[j], mM);
 
-                    mH = _mm_add_epi16(mH, me);
+                    mH = vec_add8sh(mH, me);
                 }
             }
 outB22:
@@ -572,10 +572,10 @@ outB22:
 }
 
 template <int qec> NOINLINE void Solution4::processFastVariantA16Bit(const string &a, int mm, int mi, int o, int e, int iter) {
-    __m128i mo = _mm_set1_epi16(o + e);
-    __m128i me = _mm_set1_epi16(e);
-    __m128i mmin = _mm_set1_epi16(MIN_VAL);
-    __m128i mmin0 = _mm_setr_epi16(MIN_VAL, 0, 0, 0, 0, 0, 0, 0);
+    __m128i mo = vec_splat8sh(o + e);
+    __m128i me = vec_splat8sh(e);
+    __m128i mmin = vec_splat8sh(MIN_VAL);
+    __m128i mmin0 = vec_setreverse8sh(MIN_VAL, 0, 0, 0, 0, 0, 0, 0);
 
     if (iter == -1) {
         REP(i, segNo) M0[i] = mmin;
@@ -587,15 +587,15 @@ template <int qec> NOINLINE void Solution4::processFastVariantA16Bit(const strin
 
         __m128i mco;
         if (qec) {
-            mco = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * mm, MIN_VAL));
+            mco = vec_splat8sh(max(max(lastMax, opt) - (m - i) * mm, MIN_VAL));
         } else {
-            //mco = _mm_set1_epi16(max(max(lastMax, opt) - (m - i) * (mm - e) + o, MIN_VAL));
+            //mco = vec_splat8sh(max(max(lastMax, opt) - (m - i) * (mm - e) + o, MIN_VAL));
         }
 
-        __m128i mM = _mm_load_si128(&M0[segNo - 1]);
-        mM = _mm_slli_si128(mM, 2);
-        mM = _mm_or_si128(mM, mmin0);
-        //mM = _mm_insert_epi16(mM, MIN_VAL, 0);
+        __m128i mM = vec_load1q(&M0[segNo - 1]);
+        mM = vec_shiftleftbytes1q(mM, 2);
+        mM = vec_bitor1q(mM, mmin0);
+        //mM = vec_insert8sh(mM, MIN_VAL, 0);
 
         __m128i *P = XP[(int)a[i]];
         __m128i mMax;
@@ -603,49 +603,49 @@ template <int qec> NOINLINE void Solution4::processFastVariantA16Bit(const strin
           mMax = mmin;
 
         REP(j, segNo) {
-            __m128i mV = _mm_load_si128(&V[j]);
+            __m128i mV = vec_load1q(&V[j]);
 
-            mM = _mm_adds_epi16(mM, P[j]);
+            mM = vec_addsaturating8sh(mM, P[j]);
 
             if (qec) 
-              mMax = _mm_max_epi16(mMax, mM);
+              mMax = vec_max8sh(mMax, mM);
 
-            mM = _mm_max_epi16(mM, mH);
-            mM = _mm_max_epi16(mM, mV);
+            mM = vec_max8sh(mM, mH);
+            mM = vec_max8sh(mM, mV);
 
-            _mm_store_si128(&M1[j], mM);
+            vec_store1q(&M1[j], mM);
 
-            mM = _mm_adds_epi16(mM, mo);
-            mV = _mm_adds_epi16(mV, me);
-            mV = _mm_max_epi16(mV, mM);
-            mH = _mm_adds_epi16(mH, me);
-            mH = _mm_max_epi16(mH, mM);
+            mM = vec_addsaturating8sh(mM, mo);
+            mV = vec_addsaturating8sh(mV, me);
+            mV = vec_max8sh(mV, mM);
+            mH = vec_addsaturating8sh(mH, me);
+            mH = vec_max8sh(mH, mM);
 
-            mM = _mm_load_si128(&M0[j]);
+            mM = vec_load1q(&M0[j]);
 
-            _mm_store_si128(&V[j], mV);
+            vec_store1q(&V[j], mV);
             ADD(count0);
         }
 
 
         while (true) {
-            mH = _mm_slli_si128(mH, 2);
-            mH = _mm_or_si128(mH, mmin0);
-            //mH = _mm_insert_epi16(mH, MIN_VAL, 0);
+            mH = vec_shiftleftbytes1q(mH, 2);
+            mH = vec_bitor1q(mH, mmin0);
+            //mH = vec_insert8sh(mH, MIN_VAL, 0);
             REP(j, segNo) {
                 ADD(count1);
-                mM = _mm_load_si128(&M1[j]);
+                mM = vec_load1q(&M1[j]);
 
-                __m128i mtmp = _mm_adds_epi16(mM, mo);
+                __m128i mtmp = vec_addsaturating8sh(mM, mo);
                 if (qec) 
-                  mtmp = _mm_max_epi16(mtmp, mco);
-                __m128i mcmp = _mm_cmpgt_epi16(mH, mtmp);
-                if (_mm_movemask_epi8(mcmp) == 0) goto outA16;
+                  mtmp = vec_max8sh(mtmp, mco);
+                __m128i mcmp = vec_comparegt8sh(mH, mtmp);
+                if (vec_extractupperbit16sb(mcmp) == 0) goto outA16;
 
-                mM = _mm_max_epi16(mM, mH);
-                _mm_store_si128(&M1[j], mM);
+                mM = vec_max8sh(mM, mH);
+                vec_store1q(&M1[j], mM);
 
-                mH = _mm_adds_epi16(mH, me);
+                mH = vec_addsaturating8sh(mH, me);
             }
         }
 outA16:
@@ -674,13 +674,13 @@ outA16:
 }
 
 template <int qec> NOINLINE int Solution4::processFastVariantA8Bit(const string &a, const int mm, const int mi, const int o, const int e) {
-    const __m128i mo = _mm_set1_epi8(-(o + e));
-    const __m128i me = _mm_set1_epi8(-e);
-    const __m128i m1 = _mm_set1_epi8(1);
-    //const __m128i m128 = _mm_set1_epi8(128);
-    const __m128i mmaxrange = _mm_set1_epi8(255 + mi - mm);
-    const __m128i mmi = _mm_set1_epi8(-mi);
-    const __m128i mzero = _mm_setzero_si128();
+    const __m128i mo = vec_splat16sb(-(o + e));
+    const __m128i me = vec_splat16sb(-e);
+    const __m128i m1 = vec_splat16sb(1);
+    //const __m128i m128 = vec_splat16sb(128);
+    const __m128i mmaxrange = vec_splat16sb(255 + mi - mm);
+    const __m128i mmi = vec_splat16sb(-mi);
+    const __m128i mzero = vec_zero1q();
 
     REP(i, segNo) M0[i] = mzero;
     REP(i, segNo) V[i] = mzero;
@@ -688,60 +688,60 @@ template <int qec> NOINLINE int Solution4::processFastVariantA8Bit(const string
     int midstep = qec ? m : m * 4 / 5;
     REP(i, midstep) {
         __m128i mco;
-        if (qec) mco = _mm_set1_epi8(max(max(lastMax, opt) - (m - i) * mm, 0));
+        if (qec) mco = vec_splat16sb(max(max(lastMax, opt) - (m - i) * mm, 0));
 
         __m128i mH = mzero;
 
-        __m128i mM = _mm_load_si128(&M0[segNo - 1]);
-        mM = _mm_slli_si128(mM, 1);
+        __m128i mM = vec_load1q(&M0[segNo - 1]);
+        mM = vec_shiftleftbytes1q(mM, 1);
 
         __m128i *P = XP[(int)a[i]];
         __m128i mMax = mzero;
 
         REP(j, segNo) {
-            __m128i mV = _mm_load_si128(&V[j]);
+            __m128i mV = vec_load1q(&V[j]);
 
-            mM = _mm_adds_epu8(mM, P[j]);
-            mM = _mm_subs_epu8(mM, mmi);
+            mM = vec_addsaturating16ub(mM, P[j]);
+            mM = vec_subtractsaturating16ub(mM, mmi);
 
-            mMax = _mm_max_epu8(mMax, mM);
+            mMax = vec_max16ub(mMax, mM);
 
-            mM = _mm_max_epu8(mM, mH);
-            mM = _mm_max_epu8(mM, mV);
+            mM = vec_max16ub(mM, mH);
+            mM = vec_max16ub(mM, mV);
 
-            _mm_store_si128(&M1[j], mM);
+            vec_store1q(&M1[j], mM);
 
-            mM = _mm_subs_epu8(mM, mo);
-            mV = _mm_subs_epu8(mV, me);
-            mV = _mm_max_epu8(mV, mM);
-            mH = _mm_subs_epu8(mH, me);
-            mH = _mm_max_epu8(mH, mM);
+            mM = vec_subtractsaturating16ub(mM, mo);
+            mV = vec_subtractsaturating16ub(mV, me);
+            mV = vec_max16ub(mV, mM);
+            mH = vec_subtractsaturating16ub(mH, me);
+            mH = vec_max16ub(mH, mM);
 
-            mM = _mm_load_si128(&M0[j]);
+            mM = vec_load1q(&M0[j]);
 
-            _mm_store_si128(&V[j], mV);
+            vec_store1q(&V[j], mV);
             ADD(count0);
         }
 
 
 
         while (true) {
-            mH = _mm_slli_si128(mH, 1);
+            mH = vec_shiftleftbytes1q(mH, 1);
             REP(j, segNo) {
                 ADD(count1);
-                mM = _mm_load_si128(&M1[j]);
+                mM = vec_load1q(&M1[j]);
 
-                __m128i mtmp = _mm_subs_epu8(mM, mo);
-                mtmp = _mm_adds_epu8(mtmp, m1);
-                if (qec) mtmp = _mm_max_epu8(mtmp, mco);
-                mtmp = _mm_max_epu8(mtmp, mH);
-                __m128i mcmp = _mm_cmpeq_epi8(mH, mtmp);
-                if (_mm_movemask_epi8(mcmp) == 0) goto outA81;
+                __m128i mtmp = vec_subtractsaturating16ub(mM, mo);
+                mtmp = vec_addsaturating16ub(mtmp, m1);
+                if (qec) mtmp = vec_max16ub(mtmp, mco);
+                mtmp = vec_max16ub(mtmp, mH);
+                __m128i mcmp = vec_compareeq16sb(mH, mtmp);
+                if (vec_extractupperbit16sb(mcmp) == 0) goto outA81;
 
-                mM = _mm_max_epu8(mM, mH);
-                _mm_store_si128(&M1[j], mM);
+                mM = vec_max16ub(mM, mH);
+                vec_store1q(&M1[j], mM);
 
-                mH = _mm_subs_epu8(mH, me);
+                mH = vec_subtractsaturating16ub(mH, me);
             }
         }
 outA81:
@@ -759,10 +759,10 @@ outA81:
                int c1 = (m - i) * -e + o;
             //if (c0 + c1 > curMax - 10) return i;
             c0 = min(24, c0);
-            __m128i mshift = _mm_set1_epi8(c0);
+            __m128i mshift = vec_splat16sb(c0);
             REP(j, segNo) {
-            M0[j] = _mm_subs_epu8(M0[j], mshift);
-            V[j] = _mm_subs_epu8(V[j], mshift);
+            M0[j] = vec_subtractsaturating16ub(M0[j], mshift);
+            V[j] = vec_subtractsaturating16ub(V[j], mshift);
             }
             shift += c0;
             opt -= c0;
@@ -775,9 +775,9 @@ outA81:
                 return -1;
             }
         } else {
-            mMax = _mm_max_epu8(mMax, mmaxrange);
-            __m128i mcmp = _mm_cmpgt_epi8(mMax, mmaxrange);
-            if (_mm_movemask_epi8(mcmp)) return i;
+            mMax = vec_max16ub(mMax, mmaxrange);
+            __m128i mcmp = vec_comparegt16sb(mMax, mmaxrange);
+            if (vec_extractupperbit16sb(mcmp)) return i;
         }
 
     }
@@ -786,63 +786,63 @@ outA81:
         FOR(i, midstep, m) {
             __m128i mco;
             if (qec) 
-              mco = _mm_set1_epi8(max(max(lastMax, opt) - (m - i) * mm, 0));
+              mco = vec_splat16sb(max(max(lastMax, opt) - (m - i) * mm, 0));
             else
-              mco = _mm_set1_epi8(max(max(lastMax, opt) - (m - i) * (mm - e) + o, 0));
+              mco = vec_splat16sb(max(max(lastMax, opt) - (m - i) * (mm - e) + o, 0));
 
             __m128i mH = mzero;
 
-            __m128i mM = _mm_load_si128(&M0[segNo - 1]);
-            mM = _mm_slli_si128(mM, 1);
+            __m128i mM = vec_load1q(&M0[segNo - 1]);
+            mM = vec_shiftleftbytes1q(mM, 1);
 
             __m128i *P = XP[(int)a[i]];
             __m128i mMax = mzero;
 
             REP(j, segNo) {
-                __m128i mV = _mm_load_si128(&V[j]);
+                __m128i mV = vec_load1q(&V[j]);
 
-                mM = _mm_adds_epu8(mM, P[j]);
-                mM = _mm_subs_epu8(mM, mmi);
+                mM = vec_addsaturating16ub(mM, P[j]);
+                mM = vec_subtractsaturating16ub(mM, mmi);
 
-                mMax = _mm_max_epu8(mMax, mM);
+                mMax = vec_max16ub(mMax, mM);
 
-                mM = _mm_max_epu8(mM, mH);
-                mM = _mm_max_epu8(mM, mV);
+                mM = vec_max16ub(mM, mH);
+                mM = vec_max16ub(mM, mV);
 
-                _mm_store_si128(&M1[j], mM);
+                vec_store1q(&M1[j], mM);
 
-                mM = _mm_subs_epu8(mM, mo);
-                mV = _mm_subs_epu8(mV, me);
-                mV = _mm_max_epu8(mV, mM);
-                mH = _mm_subs_epu8(mH, me);
-                mH = _mm_max_epu8(mH, mM);
+                mM = vec_subtractsaturating16ub(mM, mo);
+                mV = vec_subtractsaturating16ub(mV, me);
+                mV = vec_max16ub(mV, mM);
+                mH = vec_subtractsaturating16ub(mH, me);
+                mH = vec_max16ub(mH, mM);
 
-                mM = _mm_load_si128(&M0[j]);
+                mM = vec_load1q(&M0[j]);
 
-                _mm_store_si128(&V[j], mV);
+                vec_store1q(&V[j], mV);
                 ADD(count0);
             }
 
 
 
             while (true) {
-                mH = _mm_slli_si128(mH, 1);
+                mH = vec_shiftleftbytes1q(mH, 1);
                 REP(j, segNo) {
                     ADD(count1);
-                    mM = _mm_load_si128(&M1[j]);
+                    mM = vec_load1q(&M1[j]);
 
-                    __m128i mtmp = _mm_subs_epu8(mM, mo);
-                    mtmp = _mm_adds_epu8(mtmp, m1);
+                    __m128i mtmp = vec_subtractsaturating16ub(mM, mo);
+                    mtmp = vec_addsaturating16ub(mtmp, m1);
                     //if (qec) 
-                    mtmp = _mm_max_epu8(mtmp, mco);
-                    mtmp = _mm_max_epu8(mtmp, mH);
-                    __m128i mcmp = _mm_cmpeq_epi8(mH, mtmp);
-                    if (_mm_movemask_epi8(mcmp) == 0) goto outA82;
+                    mtmp = vec_max16ub(mtmp, mco);
+                    mtmp = vec_max16ub(mtmp, mH);
+                    __m128i mcmp = vec_compareeq16sb(mH, mtmp);
+                    if (vec_extractupperbit16sb(mcmp) == 0) goto outA82;
 
-                    mM = _mm_max_epu8(mM, mH);
-                    _mm_store_si128(&M1[j], mM);
+                    mM = vec_max16ub(mM, mH);
+                    vec_store1q(&M1[j], mM);
 
-                    mH = _mm_subs_epu8(mH, me);
+                    mH = vec_subtractsaturating16ub(mH, me);
                 }
             }
 outA82:
@@ -860,10 +860,10 @@ outA82:
                    int c1 = (m - i) * -e + o;
                 //if (c0 + c1 > curMax - 10) return i;
                 c0 = min(24, c0);
-                __m128i mshift = _mm_set1_epi8(c0);
+                __m128i mshift = vec_splat16sb(c0);
                 REP(j, segNo) {
-                M0[j] = _mm_subs_epu8(M0[j], mshift);
-                V[j] = _mm_subs_epu8(V[j], mshift);
+                M0[j] = vec_subtractsaturating16ub(M0[j], mshift);
+                V[j] = vec_subtractsaturating16ub(V[j], mshift);
                 }
                 shift += c0;
                 opt -= c0;
@@ -892,18 +892,18 @@ outA82:
 
 NOINLINE  void Solution4::convertTable(__m128i *T0, __m128i *T1) {
     const int segNoX = segNo / 2;
-    const __m128i mlo = _mm_set1_epi16(0x00FF);
-    const __m128i mminval = _mm_set1_epi16(MIN_VAL);
+    const __m128i mlo = vec_splat8sh(0x00FF);
+    const __m128i mminval = vec_splat8sh(MIN_VAL);
     __m128i *T2 = &T1[segNoX];
     REP(i, segNoX) {
-        __m128i m0 = _mm_load_si128(&T0[i]);
-        __m128i m1 = _mm_srli_si128(m0, 1);
-        m0 = _mm_and_si128(m0, mlo);
-        m0 = _mm_add_epi16(m0, mminval);
-        m1 = _mm_and_si128(m1, mlo);
-        m1 = _mm_add_epi16(m1, mminval);
-        _mm_store_si128(&T1[i], m0);
-        _mm_store_si128(&T2[i], m1);
+        __m128i m0 = vec_load1q(&T0[i]);
+        __m128i m1 = vec_shiftrightbytes1q(m0, 1);
+        m0 = vec_bitand1q(m0, mlo);
+        m0 = vec_add8sh(m0, mminval);
+        m1 = vec_bitand1q(m1, mlo);
+        m1 = vec_add8sh(m1, mminval);
+        vec_store1q(&T1[i], m0);
+        vec_store1q(&T2[i], m1);
     }
 }
 
@@ -955,29 +955,29 @@ NOINLINE void Solution4::convert16Bit(const string &b, int qsc, int mm, int mi)
     M1 = XM1;
     V = XV;
 
-    __m128i mPos = _mm_setr_epi16(0, segNo, segNo * 2, segNo * 3, segNo * 4, segNo * 5, segNo * 6, segNo * 7);
-    __m128i mOne = _mm_set1_epi16(1);
+    __m128i mPos = vec_setreverse8sh(0, segNo, segNo * 2, segNo * 3, segNo * 4, segNo * 5, segNo * 6, segNo * 7);
+    __m128i mOne = vec_splat8sh(1);
     REP(k, segNo) {
-        _mm_store_si128((__m128i*)&POS[k * 8], mPos);
-        mPos = _mm_add_epi16(mPos, mOne);
+        vec_store1q((__m128i*)&POS[k * 8], mPos);
+        mPos = vec_add8sh(mPos, mOne);
     }
 
     const int16_t SCHAR[4] = {'A', 'C', 'G', 'T'};
-    __m128i mmul = _mm_set1_epi16(mm - mi);
-    __m128i madd = _mm_set1_epi16(mi);
+    __m128i mmul = vec_splat8sh(mm - mi);
+    __m128i madd = vec_splat8sh(mi);
     REP(i, len) {
         if(n <= POS[i]) STARG[i] = 0;
         else STARG[i] = b[POS[i]];
     }
     REP(c, 4) {
-        __m128i mChar = _mm_set1_epi16(SCHAR[c]);
+        __m128i mChar = vec_splat8sh(SCHAR[c]);
         REP(k, segNo) {
-            __m128i mTarg = _mm_load_si128((__m128i*)&STARG[k * 8]);
-            __m128i mnew = _mm_cmpeq_epi16(mTarg, mChar);
-            mnew = _mm_srli_epi16(mnew, 1);
-            mnew = _mm_min_epi16(mnew, mmul);
-            mnew = _mm_add_epi16(mnew, madd);
-            _mm_store_si128(&XP[c][k], mnew);
+            __m128i mTarg = vec_load1q((__m128i*)&STARG[k * 8]);
+            __m128i mnew = vec_compareeq8sh(mTarg, mChar);
+            mnew = vec_shiftrightimmediate8sh(mnew, 1);
+            mnew = vec_min8sh(mnew, mmul);
+            mnew = vec_add8sh(mnew, madd);
+            vec_store1q(&XP[c][k], mnew);
         }
     }
 
@@ -1002,29 +1002,29 @@ NOINLINE void Solution4::preprocess16Bit(const string &b, int qsc, int mm, int m
     int16_t* STARG = &BUFFER[ptr];
 
       {
-        __m128i mPos = _mm_setr_epi16(0, segNo, segNo * 2, segNo * 3, segNo * 4, segNo * 5, segNo * 6, segNo * 7);
-        __m128i mOne = _mm_set1_epi16(1);
+        __m128i mPos = vec_setreverse8sh(0, segNo, segNo * 2, segNo * 3, segNo * 4, segNo * 5, segNo * 6, segNo * 7);
+        __m128i mOne = vec_splat8sh(1);
         REP(k, segNo) {
-            _mm_store_si128((__m128i*)&POS[k * 8], mPos);
-            mPos = _mm_add_epi16(mPos, mOne);
+            vec_store1q((__m128i*)&POS[k * 8], mPos);
+            mPos = vec_add8sh(mPos, mOne);
         }
 
         const int16_t SCHAR[4] = {'A', 'C', 'G', 'T'};
-        __m128i mmul = _mm_set1_epi16(mm - mi);
-        __m128i madd = _mm_set1_epi16(mi);
+        __m128i mmul = vec_splat8sh(mm - mi);
+        __m128i madd = vec_splat8sh(mi);
         REP(i, len) {
             if(n <= POS[i]) STARG[i] = 0;
             else STARG[i] = b[POS[i]];
         }
         REP(c, 4) {
-            __m128i mChar = _mm_set1_epi16(SCHAR[c]);
+            __m128i mChar = vec_splat8sh(SCHAR[c]);
             REP(k, segNo) {
-                __m128i mTarg = _mm_load_si128((__m128i*)&STARG[k * 8]);
-                __m128i mnew = _mm_cmpeq_epi16(mTarg, mChar);
-                mnew = _mm_srli_epi16(mnew, 1);
-                mnew = _mm_min_epi16(mnew, mmul);
-                mnew = _mm_add_epi16(mnew, madd);
-                _mm_store_si128(&XP[c][k], mnew);
+                __m128i mTarg = vec_load1q((__m128i*)&STARG[k * 8]);
+                __m128i mnew = vec_compareeq8sh(mTarg, mChar);
+                mnew = vec_shiftrightimmediate8sh(mnew, 1);
+                mnew = vec_min8sh(mnew, mmul);
+                mnew = vec_add8sh(mnew, madd);
+                vec_store1q(&XP[c][k], mnew);
             }
         }
       }
@@ -1049,29 +1049,29 @@ void Solution4::preprocess8Bit(const string &b, int qsc, int mm, int mi) {
     int8_t* STARG = (int8_t*)&BUFFER[ptr];
 
       {
-        __m128i mPos0 = _mm_setr_epi16(0, segNo, segNo * 2, segNo * 3, segNo * 4, segNo * 5, segNo * 6, segNo * 7);
-        __m128i mPos1 = _mm_setr_epi16(segNo * 8, segNo * 9, segNo * 10, segNo * 11, segNo * 12, segNo * 13, segNo * 14, segNo * 15);
-        __m128i mOne = _mm_set1_epi16(1);
+        __m128i mPos0 = vec_setreverse8sh(0, segNo, segNo * 2, segNo * 3, segNo * 4, segNo * 5, segNo * 6, segNo * 7);
+        __m128i mPos1 = vec_setreverse8sh(segNo * 8, segNo * 9, segNo * 10, segNo * 11, segNo * 12, segNo * 13, segNo * 14, segNo * 15);
+        __m128i mOne = vec_splat8sh(1);
         REP(k, segNo) {
-            _mm_store_si128((__m128i*)&POS[k * 16 + 0], mPos0);
-            _mm_store_si128((__m128i*)&POS[k * 16 + 8], mPos1);
-            mPos0 = _mm_add_epi16(mPos0, mOne);
-            mPos1 = _mm_add_epi16(mPos1, mOne);
+            vec_store1q((__m128i*)&POS[k * 16 + 0], mPos0);
+            vec_store1q((__m128i*)&POS[k * 16 + 8], mPos1);
+            mPos0 = vec_add8sh(mPos0, mOne);
+            mPos1 = vec_add8sh(mPos1, mOne);
         }
 
         const int8_t SCHAR[4] = {'A', 'C', 'G', 'T'};
-        __m128i mmul = _mm_set1_epi8(mm - mi);
+        __m128i mmul = vec_splat16sb(mm - mi);
         REP(i, len) {
             if(n <= POS[i]) STARG[i] = 0;
             else STARG[i] = b[POS[i]];
         }
         REP(c, 4) {
-            __m128i mChar = _mm_set1_epi8(SCHAR[c]);
+            __m128i mChar = vec_splat16sb(SCHAR[c]);
             REP(k, segNo) {
-                __m128i mTarg = _mm_load_si128((__m128i*)&STARG[k * 16]);
-                __m128i mnew = _mm_cmpeq_epi8(mTarg, mChar);
-                mnew = _mm_min_epu8(mnew, mmul);
-                _mm_store_si128(&XP[c][k], mnew);
+                __m128i mTarg = vec_load1q((__m128i*)&STARG[k * 16]);
+                __m128i mnew = vec_compareeq16sb(mTarg, mChar);
+                mnew = vec_min16ub(mnew, mmul);
+                vec_store1q(&XP[c][k], mnew);
             }
         }
       }
--- src/sw/lib/Solution5.cpp
+++ src/sw/lib/Solution5.cpp
@@ -27,9 +27,7 @@
 #include <cstring>
 #include <ctime>
 
-#include <emmintrin.h>
-#include <pmmintrin.h>
-#include <xmmintrin.h>
+#include <vec128int.h>
 #include "../../util/tmap_alloc.h"
 #include "../../util/tmap_definitions.h"
 #include "Solution5.h"
@@ -330,19 +328,19 @@ process(const string& s_target, const string& s_query, int qsc, int qec, int v_m
 void Solution5::shift_right16(__m128i &a,short first)
 {
   __m128i *p=(__m128i*)short_buffer;
-  _mm_store_si128(p,a);
+  vec_store1q(p,a);
   for (int i=7;i>0;i--) short_buffer[i]=short_buffer[i-1];
   short_buffer[0]=first;
-  a=_mm_load_si128(p);
+  a=vec_load1q(p);
 }
 
 void Solution5::shift_right8(__m128i &a,uchar first)
 {
   __m128i *p=(__m128i*)uchar_buffer;
-  _mm_store_si128(p,a);
+  vec_store1q(p,a);
   for (int i=15;i>0;i--) uchar_buffer[i]=uchar_buffer[i-1];
   uchar_buffer[0]=first;
-  a=_mm_load_si128(p);
+  a=vec_load1q(p);
 }
 
 #define ZERO (45)
@@ -381,13 +379,13 @@ void Solution5::brute_force()
         }
       char c_mismatch_score=mismatch_score;
       for (int pos=15;pos<length;pos+=16) if (shuffle[pos]>=m) cost8[0][pos]=c_mismatch_score;
-      __m128i m_gap_open=_mm_set1_epi8(gap_open);
-      __m128i m_gap_extension=_mm_set1_epi8(gap_extension);
-      __m128i m_gap_extension_x2=_mm_set1_epi8(gap_extension*2);
-      __m128i m_gap_extension_x3=_mm_set1_epi8(gap_extension*3);
-      __m128i m_zero=_mm_set1_epi8(ZERO);
-      __m128i m_lbound1=_mm_set1_epi8(LBOUND1);
-      __m128i m_lbound2=_mm_set1_epi8(LBOUND2);
+      __m128i m_gap_open=vec_splat16sb(gap_open);
+      __m128i m_gap_extension=vec_splat16sb(gap_extension);
+      __m128i m_gap_extension_x2=vec_splat16sb(gap_extension*2);
+      __m128i m_gap_extension_x3=vec_splat16sb(gap_extension*3);
+      __m128i m_zero=vec_splat16sb(ZERO);
+      __m128i m_lbound1=vec_splat16sb(LBOUND1);
+      __m128i m_lbound2=vec_splat16sb(LBOUND2);
       __m128i *p_buffer=(__m128i*)uchar_buffer;
       uchar *h[2];
       h[0]=hbuffer8;
@@ -409,60 +407,60 @@ void Solution5::brute_force()
           __m128i *tmp=vHLoad;
           vHLoad=vHStore;
           vHStore=tmp;
-          m01=_mm_load_si128(vHLoad+mk-1);
-          __m128i vH=_mm_max_epu8(m01,m_zero);
+          m01=vec_load1q(vHLoad+mk-1);
+          __m128i vH=vec_max16ub(m01,m_zero);
           shift_right8(vH,ZERO);
           int j=0;
           for (;j+1<mk;j+=2)            
             {
-              m01=_mm_load_si128(vProfile+j);
-              m02=_mm_load_si128(vE+j);
-              m09=_mm_load_si128(vHLoad+j);
-              m03=_mm_add_epi8(vH,m01);
-              m04=_mm_max_epu8(m02,m03);
-              vH2=_mm_max_epu8(m09,m_zero);
-              vH=_mm_max_epu8(vF,m04);
-              m08=_mm_add_epi8(vF,m_gap_extension);
-              m05=_mm_add_epi8(vH,m_gap_open);
-              vF2=_mm_max_epu8(m05,m08);
+              m01=vec_load1q(vProfile+j);
+              m02=vec_load1q(vE+j);
+              m09=vec_load1q(vHLoad+j);
+              m03=vec_add16sb(vH,m01);
+              m04=vec_max16ub(m02,m03);
+              vH2=vec_max16ub(m09,m_zero);
+              vH=vec_max16ub(vF,m04);
+              m08=vec_add16sb(vF,m_gap_extension);
+              m05=vec_add16sb(vH,m_gap_open);
+              vF2=vec_max16ub(m05,m08);
 
-              m11=_mm_load_si128(vProfile+j+1);
-              m12=_mm_load_si128(vE+j+1);
-              m13=_mm_add_epi8(vH2,m11);
-              m14=_mm_max_epu8(m12,m13);
-              vH2=_mm_max_epu8(vF2,m14);
+              m11=vec_load1q(vProfile+j+1);
+              m12=vec_load1q(vE+j+1);
+              m13=vec_add16sb(vH2,m11);
+              m14=vec_max16ub(m12,m13);
+              vH2=vec_max16ub(vF2,m14);
 
-              _mm_store_si128(vHStore+j,vH);
-              m19=_mm_load_si128(vHLoad+j+1);
-              vH=_mm_max_epu8(m19,m_zero);
-              m15=_mm_add_epi8(vH2,m_gap_open);
-              m18=_mm_add_epi8(vF2,m_gap_extension);
-              m16=_mm_add_epi8(m12,m_gap_extension);
-              m06=_mm_add_epi8(m02,m_gap_extension);
-              vF=_mm_max_epu8(m15,m18);
-              m17=_mm_max_epu8(m15,m16);
-              m07=_mm_max_epu8(m05,m06);
+              vec_store1q(vHStore+j,vH);
+              m19=vec_load1q(vHLoad+j+1);
+              vH=vec_max16ub(m19,m_zero);
+              m15=vec_add16sb(vH2,m_gap_open);
+              m18=vec_add16sb(vF2,m_gap_extension);
+              m16=vec_add16sb(m12,m_gap_extension);
+              m06=vec_add16sb(m02,m_gap_extension);
+              vF=vec_max16ub(m15,m18);
+              m17=vec_max16ub(m15,m16);
+              m07=vec_max16ub(m05,m06);
 
-              _mm_store_si128(vE+j,m07);
-              _mm_store_si128(vHStore+j+1,vH2);
-              _mm_store_si128(vE+j+1,m17);
+              vec_store1q(vE+j,m07);
+              vec_store1q(vHStore+j+1,vH2);
+              vec_store1q(vE+j+1,m17);
             }
           if (j<mk)
             {
-              m01=_mm_load_si128(vProfile+j);
-              m02=_mm_load_si128(vE+j);
-              m06=_mm_add_epi8(m02,m_gap_extension);
-              m08=_mm_add_epi8(vF,m_gap_extension);
-              m03=_mm_add_epi8(vH,m01);
-              m04=_mm_max_epu8(m02,m03);
-              vH=_mm_max_epu8(vF,m04);
-              m05=_mm_add_epi8(vH,m_gap_open);
-              vF=_mm_max_epu8(m05,m08);
-              m07=_mm_max_epu8(m05,m06);
-              _mm_store_si128(vHStore+j,vH);
-              _mm_store_si128(vE+j,m07);
-              m09=_mm_load_si128(vHLoad+j);
-              vH=_mm_max_epu8(m09,m_zero);
+              m01=vec_load1q(vProfile+j);
+              m02=vec_load1q(vE+j);
+              m06=vec_add16sb(m02,m_gap_extension);
+              m08=vec_add16sb(vF,m_gap_extension);
+              m03=vec_add16sb(vH,m01);
+              m04=vec_max16ub(m02,m03);
+              vH=vec_max16ub(vF,m04);
+              m05=vec_add16sb(vH,m_gap_open);
+              vF=vec_max16ub(m05,m08);
+              m07=vec_max16ub(m05,m06);
+              vec_store1q(vHStore+j,vH);
+              vec_store1q(vE+j,m07);
+              m09=vec_load1q(vHLoad+j);
+              vH=vec_max16ub(m09,m_zero);
             }
           shift_right8(vF,LBOUND2);
           for (int counter=0;counter<rounds;counter++)
@@ -471,65 +469,65 @@ void Solution5::brute_force()
               for (j=0;j+3<mk;j+=4)
                 if (j&15)
                   {
-                    m01=_mm_load_si128(vHStore+j);
-                    m12=_mm_add_epi8(vF,m_gap_extension);
-                    m06=_mm_add_epi8(vF,m_gap_extension_x2);
-                    m16=_mm_add_epi8(vF,m_gap_extension_x3);
-                    m11=_mm_load_si128(vHStore+j+1);
-                    m07=_mm_load_si128(vHStore+j+2);
-                    m17=_mm_load_si128(vHStore+j+3);
-                    m04=_mm_max_epu8(m01,vF);
-                    m14=_mm_max_epu8(m11,m12);
-                    m08=_mm_max_epu8(m06,m07);
-                    m18=_mm_max_epu8(m16,m17);
-                    _mm_store_si128(vHStore+j,m04);
-                    _mm_store_si128(vHStore+j+1,m14);
-                    _mm_store_si128(vHStore+j+2,m08);
-                    _mm_store_si128(vHStore+j+3,m18);
-                    m09=_mm_add_epi8(m16,m_gap_extension);
-                    vF=_mm_max_epu8(m09,m_lbound2);
+                    m01=vec_load1q(vHStore+j);
+                    m12=vec_add16sb(vF,m_gap_extension);
+                    m06=vec_add16sb(vF,m_gap_extension_x2);
+                    m16=vec_add16sb(vF,m_gap_extension_x3);
+                    m11=vec_load1q(vHStore+j+1);
+                    m07=vec_load1q(vHStore+j+2);
+                    m17=vec_load1q(vHStore+j+3);
+                    m04=vec_max16ub(m01,vF);
+                    m14=vec_max16ub(m11,m12);
+                    m08=vec_max16ub(m06,m07);
+                    m18=vec_max16ub(m16,m17);
+                    vec_store1q(vHStore+j,m04);
+                    vec_store1q(vHStore+j+1,m14);
+                    vec_store1q(vHStore+j+2,m08);
+                    vec_store1q(vHStore+j+3,m18);
+                    m09=vec_add16sb(m16,m_gap_extension);
+                    vF=vec_max16ub(m09,m_lbound2);
                   }
                 else
                   {
-                    m01=_mm_load_si128(vHStore+j);
-                    m02=_mm_add_epi8(m01,m_gap_open);
-                    m03=_mm_max_epu8(vF,m02);
-                    m06=_mm_cmpeq_epi8(vF,m03);
-                    _mm_store_si128(ck_zero,m06);
+                    m01=vec_load1q(vHStore+j);
+                    m02=vec_add16sb(m01,m_gap_open);
+                    m03=vec_max16ub(vF,m02);
+                    m06=vec_compareeq16sb(vF,m03);
+                    vec_store1q(ck_zero,m06);
                     if (int_buffer[0]==0 && int_buffer[1]==0 && int_buffer[2]==0 && int_buffer[3]==0) { flag=true; break; }
-                    m12=_mm_add_epi8(vF,m_gap_extension);
-                    m06=_mm_add_epi8(vF,m_gap_extension_x2);
-                    m16=_mm_add_epi8(vF,m_gap_extension_x3);
-                    m11=_mm_load_si128(vHStore+j+1);
-                    m07=_mm_load_si128(vHStore+j+2);
-                    m17=_mm_load_si128(vHStore+j+3);
-                    m04=_mm_max_epu8(m01,vF);
-                    m14=_mm_max_epu8(m11,m12);
-                    m08=_mm_max_epu8(m06,m07);
-                    m18=_mm_max_epu8(m16,m17);
-                    _mm_store_si128(vHStore+j,m04);
-                    _mm_store_si128(vHStore+j+1,m14);
-                    _mm_store_si128(vHStore+j+2,m08);
-                    _mm_store_si128(vHStore+j+3,m18);
-                    m09=_mm_add_epi8(m16,m_gap_extension);
-                    vF=_mm_max_epu8(m09,m_lbound2);
+                    m12=vec_add16sb(vF,m_gap_extension);
+                    m06=vec_add16sb(vF,m_gap_extension_x2);
+                    m16=vec_add16sb(vF,m_gap_extension_x3);
+                    m11=vec_load1q(vHStore+j+1);
+                    m07=vec_load1q(vHStore+j+2);
+                    m17=vec_load1q(vHStore+j+3);
+                    m04=vec_max16ub(m01,vF);
+                    m14=vec_max16ub(m11,m12);
+                    m08=vec_max16ub(m06,m07);
+                    m18=vec_max16ub(m16,m17);
+                    vec_store1q(vHStore+j,m04);
+                    vec_store1q(vHStore+j+1,m14);
+                    vec_store1q(vHStore+j+2,m08);
+                    vec_store1q(vHStore+j+3,m18);
+                    m09=vec_add16sb(m16,m_gap_extension);
+                    vF=vec_max16ub(m09,m_lbound2);
                   }
               if (flag) break;
               for (;j<mk;j++)
                 {
-                  m01=_mm_load_si128(vHStore+j);
+                  m01=vec_load1q(vHStore+j);
                   if (j+1==mk)
                     {
-                      m02=_mm_add_epi8(m01,m_gap_open);
-                      m03=_mm_max_epu8(vF,m02);
-                      m06=_mm_cmpeq_epi8(vF,m03);
-                      _mm_store_si128(ck_zero,m06);
+                      m02=vec_add16sb(m01,m_gap_open);
+                      m03=vec_max16ub(vF,m02);
+                      m06=vec_compareeq16sb(vF,m03);
+                      vec_store1q(ck_zero,m06);
                       if (int_buffer[0]==0 && int_buffer[1]==0 && int_buffer[2]==0 && int_buffer[3]==0) { flag=true; break; }
                     }
-                  m04=_mm_max_epu8(m01,vF);
-                  _mm_store_si128(vHStore+j,m04);
-                  m05=_mm_add_epi8(vF,m_gap_extension);
-                  vF=_mm_max_epu8(m05,m_lbound2);
+                  m04=vec_max16ub(m01,vF);
+                  vec_store1q(vHStore+j,m04);
+                  m05=vec_add16sb(vF,m_gap_extension);
+                  vF=vec_max16ub(m05,m_lbound2);
                 }
               if (flag) break;
               shift_right8(vF,LBOUND2);
@@ -538,28 +536,28 @@ void Solution5::brute_force()
           m05=m_zero; m06=m_zero; m07=m_zero; m08=m_zero; 
           for (j=0;j+7<mk;j+=8)
             {
-              m11=_mm_load_si128(vHStore+j); m12=_mm_load_si128(vHStore+j+1);
-              m13=_mm_load_si128(vHStore+j+2); m14=_mm_load_si128(vHStore+j+3);
-              m15=_mm_load_si128(vHStore+j+4); m16=_mm_load_si128(vHStore+j+5);
-              m17=_mm_load_si128(vHStore+j+6); m18=_mm_load_si128(vHStore+j+7);
-              m01=_mm_max_epu8(m01,m11); m02=_mm_max_epu8(m02,m12);
-              m03=_mm_max_epu8(m03,m13); m04=_mm_max_epu8(m04,m14);
-              m05=_mm_max_epu8(m05,m15); m06=_mm_max_epu8(m06,m16);
-              m07=_mm_max_epu8(m07,m17); m08=_mm_max_epu8(m08,m18);
+              m11=vec_load1q(vHStore+j); m12=vec_load1q(vHStore+j+1);
+              m13=vec_load1q(vHStore+j+2); m14=vec_load1q(vHStore+j+3);
+              m15=vec_load1q(vHStore+j+4); m16=vec_load1q(vHStore+j+5);
+              m17=vec_load1q(vHStore+j+6); m18=vec_load1q(vHStore+j+7);
+              m01=vec_max16ub(m01,m11); m02=vec_max16ub(m02,m12);
+              m03=vec_max16ub(m03,m13); m04=vec_max16ub(m04,m14);
+              m05=vec_max16ub(m05,m15); m06=vec_max16ub(m06,m16);
+              m07=vec_max16ub(m07,m17); m08=vec_max16ub(m08,m18);
             }
           for (;j<mk;j++)
             {
-              m11=_mm_load_si128(vHStore+j);
-              m01=_mm_max_epu8(m01,m11);
+              m11=vec_load1q(vHStore+j);
+              m01=vec_max16ub(m01,m11);
             }
-          m11=_mm_max_epu8(m01,m02);
-          m13=_mm_max_epu8(m03,m04);
-          m15=_mm_max_epu8(m05,m06);
-          m17=_mm_max_epu8(m07,m08);
-          m12=_mm_max_epu8(m11,m13);
-          m14=_mm_max_epu8(m15,m17);
-          m16=_mm_max_epu8(m12,m14);
-          _mm_store_si128(p_buffer,m16);
+          m11=vec_max16ub(m01,m02);
+          m13=vec_max16ub(m03,m04);
+          m15=vec_max16ub(m05,m06);
+          m17=vec_max16ub(m07,m08);
+          m12=vec_max16ub(m11,m13);
+          m14=vec_max16ub(m15,m17);
+          m16=vec_max16ub(m12,m14);
+          vec_store1q(p_buffer,m16);
           int local_opt=0;
           for (int d=0;d<16;d++) if ((int)uchar_buffer[d]-ZERO>local_opt) local_opt=uchar_buffer[d]-ZERO;
           if (task_id==1 && local_opt>=opt)
@@ -643,11 +641,11 @@ void Solution5::brute_force()
           pos++;
         }
     }
-  __m128i m_gap_open=_mm_set1_epi16(gap_open);
-  __m128i m_gap_extension=_mm_set1_epi16(gap_extension);
-  __m128i m_gap_extension_x2=_mm_set1_epi16(gap_extension*2);
-  __m128i m_gap_extension_x3=_mm_set1_epi16(gap_extension*3);
-  __m128i m_zero=_mm_set1_epi16(0);
+  __m128i m_gap_open=vec_splat8sh(gap_open);
+  __m128i m_gap_extension=vec_splat8sh(gap_extension);
+  __m128i m_gap_extension_x2=vec_splat8sh(gap_extension*2);
+  __m128i m_gap_extension_x3=vec_splat8sh(gap_extension*3);
+  __m128i m_zero=vec_splat8sh(0);
   short *h[2];
   h[0]=hbuffer16;
   h[1]=hbuffer16+length;
@@ -670,7 +668,7 @@ void Solution5::brute_force()
       for (int i=start_i;i<n;i++)
         {
           __m128i *vProfile=(__m128i*)cost16[a[i]];
-          __m128i vF=_mm_set1_epi16(-10000);
+          __m128i vF=vec_splat8sh(-10000);
           __m128i m01,m02,m03,m04,m05,m06,m07,m08,m09;
           __m128i m11,m12,m13,m14,m15,m16,m17,m18,m19;
           __m128i vH2,vF2;
@@ -679,114 +677,114 @@ void Solution5::brute_force()
           vHStore=tmp;
           if (task_id==1 || task_id==3)
             {
-              m01=_mm_load_si128(vHLoad+mk-1);
-              __m128i vH=_mm_max_epi16(m01,m_zero);
+              m01=vec_load1q(vHLoad+mk-1);
+              __m128i vH=vec_max8sh(m01,m_zero);
               shift_right16(vH,0);
               int j=0;
               for (;j+1<mk;j+=2)
                 {
-                  m01=_mm_load_si128(vProfile+j);
-                  m02=_mm_load_si128(vE+j);
-                  m09=_mm_load_si128(vHLoad+j);
-                  m03=_mm_add_epi16(vH,m01);
-                  m04=_mm_max_epi16(m02,m03);
-                  vH2=_mm_max_epi16(m09,m_zero);
-                  vH=_mm_max_epi16(vF,m04);
-                  m08=_mm_add_epi16(vF,m_gap_extension);
-                  m05=_mm_add_epi16(vH,m_gap_open);
-                  vF2=_mm_max_epi16(m05,m08);
+                  m01=vec_load1q(vProfile+j);
+                  m02=vec_load1q(vE+j);
+                  m09=vec_load1q(vHLoad+j);
+                  m03=vec_add8sh(vH,m01);
+                  m04=vec_max8sh(m02,m03);
+                  vH2=vec_max8sh(m09,m_zero);
+                  vH=vec_max8sh(vF,m04);
+                  m08=vec_add8sh(vF,m_gap_extension);
+                  m05=vec_add8sh(vH,m_gap_open);
+                  vF2=vec_max8sh(m05,m08);
 
-                  m11=_mm_load_si128(vProfile+j+1);
-                  m12=_mm_load_si128(vE+j+1);
-                  m13=_mm_add_epi16(vH2,m11);
-                  m14=_mm_max_epi16(m12,m13);
-                  vH2=_mm_max_epi16(vF2,m14);
+                  m11=vec_load1q(vProfile+j+1);
+                  m12=vec_load1q(vE+j+1);
+                  m13=vec_add8sh(vH2,m11);
+                  m14=vec_max8sh(m12,m13);
+                  vH2=vec_max8sh(vF2,m14);
 
-                  _mm_store_si128(vHStore+j,vH);
-                  m19=_mm_load_si128(vHLoad+j+1);
-                  vH=_mm_max_epi16(m19,m_zero);
-                  m15=_mm_add_epi16(vH2,m_gap_open);
-                  m18=_mm_add_epi16(vF2,m_gap_extension);
-                  m16=_mm_add_epi16(m12,m_gap_extension);
-                  m06=_mm_add_epi16(m02,m_gap_extension);
-                  vF=_mm_max_epi16(m15,m18);
-                  m17=_mm_max_epi16(m15,m16);
-                  m07=_mm_max_epi16(m05,m06);
+                  vec_store1q(vHStore+j,vH);
+                  m19=vec_load1q(vHLoad+j+1);
+                  vH=vec_max8sh(m19,m_zero);
+                  m15=vec_add8sh(vH2,m_gap_open);
+                  m18=vec_add8sh(vF2,m_gap_extension);
+                  m16=vec_add8sh(m12,m_gap_extension);
+                  m06=vec_add8sh(m02,m_gap_extension);
+                  vF=vec_max8sh(m15,m18);
+                  m17=vec_max8sh(m15,m16);
+                  m07=vec_max8sh(m05,m06);
 
-                  _mm_store_si128(vE+j,m07);
-                  _mm_store_si128(vHStore+j+1,vH2);
-                  _mm_store_si128(vE+j+1,m17);
+                  vec_store1q(vE+j,m07);
+                  vec_store1q(vHStore+j+1,vH2);
+                  vec_store1q(vE+j+1,m17);
                 }
               if (j<mk)
                 {
-                  m01=_mm_load_si128(vProfile+j);
-                  m02=_mm_load_si128(vE+j);
-                  m06=_mm_add_epi16(m02,m_gap_extension);
-                  m08=_mm_add_epi16(vF,m_gap_extension);
-                  m03=_mm_add_epi16(vH,m01);
-                  m04=_mm_max_epi16(m02,m03);
-                  vH=_mm_max_epi16(vF,m04);
-                  m05=_mm_add_epi16(vH,m_gap_open);
-                  vF=_mm_max_epi16(m05,m08);
-                  m07=_mm_max_epi16(m05,m06);
-                  _mm_store_si128(vHStore+j,vH);
-                  _mm_store_si128(vE+j,m07);
-                  m09=_mm_load_si128(vHLoad+j);
-                  vH=_mm_max_epi16(m09,m_zero);
+                  m01=vec_load1q(vProfile+j);
+                  m02=vec_load1q(vE+j);
+                  m06=vec_add8sh(m02,m_gap_extension);
+                  m08=vec_add8sh(vF,m_gap_extension);
+                  m03=vec_add8sh(vH,m01);
+                  m04=vec_max8sh(m02,m03);
+                  vH=vec_max8sh(vF,m04);
+                  m05=vec_add8sh(vH,m_gap_open);
+                  vF=vec_max8sh(m05,m08);
+                  m07=vec_max8sh(m05,m06);
+                  vec_store1q(vHStore+j,vH);
+                  vec_store1q(vE+j,m07);
+                  m09=vec_load1q(vHLoad+j);
+                  vH=vec_max8sh(m09,m_zero);
                 }
             }
           else
             {
-              __m128i vH=_mm_load_si128(vHLoad+mk-1);
+              __m128i vH=vec_load1q(vHLoad+mk-1);
               shift_right16(vH,(i==0)?0:(gap_open+(i-1)*gap_extension));
               int j=0;
               for (;j+1<mk;j+=2)
                 {
-                  m01=_mm_load_si128(vProfile+j);
-                  m02=_mm_load_si128(vE+j);
-                  m03=_mm_add_epi16(vH,m01);
-                  m04=_mm_max_epi16(m02,m03);
-                  vH=_mm_max_epi16(vF,m04);
-                  m08=_mm_add_epi16(vF,m_gap_extension);
-                  m05=_mm_add_epi16(vH,m_gap_open);
-                  vF2=_mm_max_epi16(m05,m08);
+                  m01=vec_load1q(vProfile+j);
+                  m02=vec_load1q(vE+j);
+                  m03=vec_add8sh(vH,m01);
+                  m04=vec_max8sh(m02,m03);
+                  vH=vec_max8sh(vF,m04);
+                  m08=vec_add8sh(vF,m_gap_extension);
+                  m05=vec_add8sh(vH,m_gap_open);
+                  vF2=vec_max8sh(m05,m08);
 
-                  vH2=_mm_load_si128(vHLoad+j);
-                  m11=_mm_load_si128(vProfile+j+1);
-                  m12=_mm_load_si128(vE+j+1);
-                  m13=_mm_add_epi16(vH2,m11);
-                  m14=_mm_max_epi16(m12,m13);
-                  vH2=_mm_max_epi16(vF2,m14);
+                  vH2=vec_load1q(vHLoad+j);
+                  m11=vec_load1q(vProfile+j+1);
+                  m12=vec_load1q(vE+j+1);
+                  m13=vec_add8sh(vH2,m11);
+                  m14=vec_max8sh(m12,m13);
+                  vH2=vec_max8sh(vF2,m14);
 
-                  _mm_store_si128(vHStore+j,vH);
-                  m15=_mm_add_epi16(vH2,m_gap_open);
-                  m18=_mm_add_epi16(vF2,m_gap_extension);
-                  m16=_mm_add_epi16(m12,m_gap_extension);
-                  m06=_mm_add_epi16(m02,m_gap_extension);
-                  vF=_mm_max_epi16(m15,m18);
-                  m17=_mm_max_epi16(m15,m16);
-                  m07=_mm_max_epi16(m05,m06);
-                  vH=_mm_load_si128(vHLoad+j+1);
+                  vec_store1q(vHStore+j,vH);
+                  m15=vec_add8sh(vH2,m_gap_open);
+                  m18=vec_add8sh(vF2,m_gap_extension);
+                  m16=vec_add8sh(m12,m_gap_extension);
+                  m06=vec_add8sh(m02,m_gap_extension);
+                  vF=vec_max8sh(m15,m18);
+                  m17=vec_max8sh(m15,m16);
+                  m07=vec_max8sh(m05,m06);
+                  vH=vec_load1q(vHLoad+j+1);
 
-                  _mm_store_si128(vE+j,m07);
-                  _mm_store_si128(vHStore+j+1,vH2);
-                  _mm_store_si128(vE+j+1,m17);
+                  vec_store1q(vE+j,m07);
+                  vec_store1q(vHStore+j+1,vH2);
+                  vec_store1q(vE+j+1,m17);
                 }
               if (j<mk)
                 {
-                  m01=_mm_load_si128(vProfile+j);
-                  m02=_mm_load_si128(vE+j);
-                  m06=_mm_add_epi16(m02,m_gap_extension);
-                  m08=_mm_add_epi16(vF,m_gap_extension);
-                  m03=_mm_add_epi16(vH,m01);
-                  m04=_mm_max_epi16(m02,m03);
-                  vH=_mm_max_epi16(vF,m04);
-                  m05=_mm_add_epi16(vH,m_gap_open);
-                  vF=_mm_max_epi16(m05,m08);
-                  m07=_mm_max_epi16(m05,m06);
-                  _mm_store_si128(vHStore+j,vH);
-                  _mm_store_si128(vE+j,m07);
-                  vH=_mm_load_si128(vHLoad+j);
+                  m01=vec_load1q(vProfile+j);
+                  m02=vec_load1q(vE+j);
+                  m06=vec_add8sh(m02,m_gap_extension);
+                  m08=vec_add8sh(vF,m_gap_extension);
+                  m03=vec_add8sh(vH,m01);
+                  m04=vec_max8sh(m02,m03);
+                  vH=vec_max8sh(vF,m04);
+                  m05=vec_add8sh(vH,m_gap_open);
+                  vF=vec_max8sh(m05,m08);
+                  m07=vec_max8sh(m05,m06);
+                  vec_store1q(vHStore+j,vH);
+                  vec_store1q(vE+j,m07);
+                  vH=vec_load1q(vHLoad+j);
                 }
             }
           if (task_id==4)
@@ -797,29 +795,29 @@ void Solution5::brute_force()
                   int j=0;
                   for (;j+3<mk;j+=4)
                     {
-                      m01=_mm_load_si128(vHStore+j);
-                      m13=_mm_add_epi16(vF,m_gap_extension);
-                      m04=_mm_add_epi16(vF,m_gap_extension_x2);
-                      m14=_mm_add_epi16(vF,m_gap_extension_x3);
-                      m11=_mm_load_si128(vHStore+j+1);
-                      m05=_mm_load_si128(vHStore+j+2);
-                      m15=_mm_load_si128(vHStore+j+3);
-                      m02=_mm_max_epi16(m01,vF);
-                      m12=_mm_max_epi16(m11,m13);
-                      m06=_mm_max_epi16(m05,m04);
-                      m16=_mm_max_epi16(m15,m14);
-                      _mm_store_si128(vHStore+j,m02);
-                      _mm_store_si128(vHStore+j+1,m12);
-                      _mm_store_si128(vHStore+j+2,m06);
-                      _mm_store_si128(vHStore+j+3,m16);
-                      vF=_mm_add_epi16(m14,m_gap_extension);
+                      m01=vec_load1q(vHStore+j);
+                      m13=vec_add8sh(vF,m_gap_extension);
+                      m04=vec_add8sh(vF,m_gap_extension_x2);
+                      m14=vec_add8sh(vF,m_gap_extension_x3);
+                      m11=vec_load1q(vHStore+j+1);
+                      m05=vec_load1q(vHStore+j+2);
+                      m15=vec_load1q(vHStore+j+3);
+                      m02=vec_max8sh(m01,vF);
+                      m12=vec_max8sh(m11,m13);
+                      m06=vec_max8sh(m05,m04);
+                      m16=vec_max8sh(m15,m14);
+                      vec_store1q(vHStore+j,m02);
+                      vec_store1q(vHStore+j+1,m12);
+                      vec_store1q(vHStore+j+2,m06);
+                      vec_store1q(vHStore+j+3,m16);
+                      vF=vec_add8sh(m14,m_gap_extension);
                     }
                   for (;j<mk;j++)
                     {
-                      m01=_mm_load_si128(vHStore+j);
-                      m04=_mm_max_epi16(m01,vF);
-                      _mm_store_si128(vHStore+j,m04);
-                      vF=_mm_add_epi16(vF,m_gap_extension);
+                      m01=vec_load1q(vHStore+j);
+                      m04=vec_max8sh(m01,vF);
+                      vec_store1q(vHStore+j,m04);
+                      vF=vec_add8sh(vF,m_gap_extension);
                     }
                   shift_right16(vF,-10000);
                 }
@@ -834,60 +832,60 @@ void Solution5::brute_force()
                   for (;j+3<mk;j+=4)
                     if (j&15)
                       {
-                        m01=_mm_load_si128(vHStore+j);
-                        m12=_mm_add_epi16(vF,m_gap_extension);
-                        m06=_mm_add_epi16(vF,m_gap_extension_x2);
-                        m16=_mm_add_epi16(vF,m_gap_extension_x3);
-                        m11=_mm_load_si128(vHStore+j+1);
-                        m07=_mm_load_si128(vHStore+j+2);
-                        m17=_mm_load_si128(vHStore+j+3);
-                        m04=_mm_max_epi16(m01,vF);
-                        m14=_mm_max_epi16(m11,m12);
-                        m08=_mm_max_epi16(m06,m07);
-                        m18=_mm_max_epi16(m16,m17);
-                        _mm_store_si128(vHStore+j,m04);
-                        _mm_store_si128(vHStore+j+1,m14);
-                        _mm_store_si128(vHStore+j+2,m08);
-                        _mm_store_si128(vHStore+j+3,m18);
-                        vF=_mm_add_epi16(m16,m_gap_extension);
+                        m01=vec_load1q(vHStore+j);
+                        m12=vec_add8sh(vF,m_gap_extension);
+                        m06=vec_add8sh(vF,m_gap_extension_x2);
+                        m16=vec_add8sh(vF,m_gap_extension_x3);
+                        m11=vec_load1q(vHStore+j+1);
+                        m07=vec_load1q(vHStore+j+2);
+                        m17=vec_load1q(vHStore+j+3);
+                        m04=vec_max8sh(m01,vF);
+                        m14=vec_max8sh(m11,m12);
+                        m08=vec_max8sh(m06,m07);
+                        m18=vec_max8sh(m16,m17);
+                        vec_store1q(vHStore+j,m04);
+                        vec_store1q(vHStore+j+1,m14);
+                        vec_store1q(vHStore+j+2,m08);
+                        vec_store1q(vHStore+j+3,m18);
+                        vF=vec_add8sh(m16,m_gap_extension);
                       }
                     else
                       {
-                        m01=_mm_load_si128(vHStore+j);
-                        m02=_mm_add_epi16(m01,m_gap_open);
-                        m03=_mm_cmpgt_epi16(vF,m02);
-                        _mm_store_si128(ck_zero,m03);
+                        m01=vec_load1q(vHStore+j);
+                        m02=vec_add8sh(m01,m_gap_open);
+                        m03=vec_comparegt8sh(vF,m02);
+                        vec_store1q(ck_zero,m03);
                         if (int_buffer[0]==0 && int_buffer[1]==0 && int_buffer[2]==0 && int_buffer[3]==0) { flag=true; break; }
-                        m12=_mm_add_epi16(vF,m_gap_extension);
-                        m06=_mm_add_epi16(vF,m_gap_extension_x2);
-                        m16=_mm_add_epi16(vF,m_gap_extension_x3);
-                        m11=_mm_load_si128(vHStore+j+1);
-                        m07=_mm_load_si128(vHStore+j+2);
-                        m17=_mm_load_si128(vHStore+j+3);
-                        m04=_mm_max_epi16(m01,vF);
-                        m14=_mm_max_epi16(m11,m12);
-                        m08=_mm_max_epi16(m06,m07);
-                        m18=_mm_max_epi16(m16,m17);
-                        _mm_store_si128(vHStore+j,m04);
-                        _mm_store_si128(vHStore+j+1,m14);
-                        _mm_store_si128(vHStore+j+2,m08);
-                        _mm_store_si128(vHStore+j+3,m18);
-                        vF=_mm_add_epi16(m16,m_gap_extension);
+                        m12=vec_add8sh(vF,m_gap_extension);
+                        m06=vec_add8sh(vF,m_gap_extension_x2);
+                        m16=vec_add8sh(vF,m_gap_extension_x3);
+                        m11=vec_load1q(vHStore+j+1);
+                        m07=vec_load1q(vHStore+j+2);
+                        m17=vec_load1q(vHStore+j+3);
+                        m04=vec_max8sh(m01,vF);
+                        m14=vec_max8sh(m11,m12);
+                        m08=vec_max8sh(m06,m07);
+                        m18=vec_max8sh(m16,m17);
+                        vec_store1q(vHStore+j,m04);
+                        vec_store1q(vHStore+j+1,m14);
+                        vec_store1q(vHStore+j+2,m08);
+                        vec_store1q(vHStore+j+3,m18);
+                        vF=vec_add8sh(m16,m_gap_extension);
                       }
                   if (flag) break;
                   for (;j<mk;j++)
                     {
-                      m01=_mm_load_si128(vHStore+j);
+                      m01=vec_load1q(vHStore+j);
                       if (j+1==mk)
                         {
-                          m02=_mm_add_epi16(m01,m_gap_open);
-                          m03=_mm_cmpgt_epi16(vF,m02);
-                          _mm_store_si128(ck_zero,m03);
+                          m02=vec_add8sh(m01,m_gap_open);
+                          m03=vec_comparegt8sh(vF,m02);
+                          vec_store1q(ck_zero,m03);
                           if (int_buffer[0]==0 && int_buffer[1]==0 && int_buffer[2]==0 && int_buffer[3]==0) { flag=true; break; }
                         }
-                      m04=_mm_max_epi16(m01,vF);
-                      _mm_store_si128(vHStore+j,m04);
-                      vF=_mm_add_epi16(vF,m_gap_extension);
+                      m04=vec_max8sh(m01,vF);
+                      vec_store1q(vHStore+j,m04);
+                      vF=vec_add8sh(vF,m_gap_extension);
                     }
                   if (flag) break;
                   shift_right16(vF,-10000);
@@ -900,28 +898,28 @@ void Solution5::brute_force()
               int j=0;    
               for (;j+7<mk;j+=8)
                 {
-                  m11=_mm_load_si128(vHStore+j); m12=_mm_load_si128(vHStore+j+1);
-                  m13=_mm_load_si128(vHStore+j+2); m14=_mm_load_si128(vHStore+j+3);
-                  m15=_mm_load_si128(vHStore+j+4); m16=_mm_load_si128(vHStore+j+5);
-                  m17=_mm_load_si128(vHStore+j+6); m18=_mm_load_si128(vHStore+j+7);
-                  m01=_mm_max_epi16(m01,m11); m02=_mm_max_epi16(m02,m12);
-                  m03=_mm_max_epi16(m03,m13); m04=_mm_max_epi16(m04,m14);
-                  m05=_mm_max_epi16(m05,m15); m06=_mm_max_epi16(m06,m16);
-                  m07=_mm_max_epi16(m07,m17); m08=_mm_max_epi16(m08,m18);
+                  m11=vec_load1q(vHStore+j); m12=vec_load1q(vHStore+j+1);
+                  m13=vec_load1q(vHStore+j+2); m14=vec_load1q(vHStore+j+3);
+                  m15=vec_load1q(vHStore+j+4); m16=vec_load1q(vHStore+j+5);
+                  m17=vec_load1q(vHStore+j+6); m18=vec_load1q(vHStore+j+7);
+                  m01=vec_max8sh(m01,m11); m02=vec_max8sh(m02,m12);
+                  m03=vec_max8sh(m03,m13); m04=vec_max8sh(m04,m14);
+                  m05=vec_max8sh(m05,m15); m06=vec_max8sh(m06,m16);
+                  m07=vec_max8sh(m07,m17); m08=vec_max8sh(m08,m18);
                 }
               for (;j<mk;j++)
                 {
-                  m11=_mm_load_si128(vHStore+j);
-                  m01=_mm_max_epi16(m01,m11);
+                  m11=vec_load1q(vHStore+j);
+                  m01=vec_max8sh(m01,m11);
                 }
-              m11=_mm_max_epi16(m01,m02);
-              m13=_mm_max_epi16(m03,m04);
-              m15=_mm_max_epi16(m05,m06);
-              m17=_mm_max_epi16(m07,m08);
-              m12=_mm_max_epi16(m11,m13);
-              m14=_mm_max_epi16(m15,m17);
-              m16=_mm_max_epi16(m12,m14);
-              _mm_store_si128(p_buffer,m16);
+              m11=vec_max8sh(m01,m02);
+              m13=vec_max8sh(m03,m04);
+              m15=vec_max8sh(m05,m06);
+              m17=vec_max8sh(m07,m08);
+              m12=vec_max8sh(m11,m13);
+              m14=vec_max8sh(m15,m17);
+              m16=vec_max8sh(m12,m14);
+              vec_store1q(p_buffer,m16);
               int local_opt=0;
               for (int d=0;d<8;d++) if ((int)short_buffer[d]>local_opt) local_opt=short_buffer[d];
               if (local_opt>0 && local_opt>=opt)
--- src/sw/lib/Solution6.cpp
+++ src/sw/lib/Solution6.cpp
@@ -6,10 +6,7 @@
 #include <fstream>
 #include <string>
 #include <algorithm>
-#include <mmintrin.h>
-#include <xmmintrin.h>
-#include <emmintrin.h>
-#include <pmmintrin.h>
+#include <vec128int.h>
 #include "Solution6.h"
 
 using namespace std;
@@ -49,16 +46,16 @@ int Solution6::process(const string& bs, const string& as, int qsc, int qec,
     copy(as.c_str(), as.c_str() + m, abuf);
 
     for (int i=0; i<n; i++)
-      X[i+8].m = _mm_setzero_si128();
+      X[i+8].m = vec_zero1q();
 
-    const __m128i mme = _mm_set1_epi16(e);
-    const __m128i mmm = _mm_set1_epi16(mm);
-    const __m128i mmmi = _mm_set1_epi16(mm-mi);
-    const __m128i mmi = _mm_set1_epi16(mi);
+    const __m128i mme = vec_splat8sh(e);
+    const __m128i mmm = vec_splat8sh(mm);
+    const __m128i mmmi = vec_splat8sh(mm-mi);
+    const __m128i mmi = vec_splat8sh(mi);
 
-    const __m128i mmoe = _mm_set1_epi16(oe);
-    const __m128i mNINF = _mm_set1_epi16(NINF);
-    const __m128i mnegones = _mm_set1_epi16(-1);
+    const __m128i mmoe = vec_splat8sh(oe);
+    const __m128i mNINF = vec_splat8sh(NINF);
+    const __m128i mnegones = vec_splat8sh(-1);
 
     const int nb = (m+7)/8*8;
     const short* b;
@@ -68,61 +65,61 @@ int Solution6::process(const string& bs, const string& as, int qsc, int qec,
     if (qec == 0) {
         if (qsc == 0) {
             for (int bl=0; bl < nb; bl+=8) {
-                A = _mm_lddqu_si128((__m128i*)a);
+                A = vec_load1qu((__m128i*)a);
                 a += 8;
                 b = B + n+7;
                   {
-                    __m128i tmp = _mm_set1_epi16(oe + e * bl);
-                    X[0].m = _mm_sub_epi16(tmp, mme);
+                    __m128i tmp = vec_splat8sh(oe + e * bl);
+                    X[0].m = vec_subtract8sh(tmp, mme);
                     X[1].m = tmp;
-                    __m128i h = _mm_add_epi16(tmp, mme);
+                    __m128i h = vec_add8sh(tmp, mme);
                     __m128i v = mNINF;
                     __m128i maskF = mnegones;
 
                     for (int t=0; t < 7; t++) {
-                        maskF = _mm_slli_si128(maskF, 2);
+                        maskF = vec_shiftleftbytes1q(maskF, 2);
 
-                        __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                        v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                        __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                        v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                        __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                        __m128i comp = _mm_or_si128(_mm_and_si128(mask, mmm), _mm_andnot_si128(mask, mmi)); //common
+                        __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                        __m128i comp = vec_bitor1q(vec_bitand1q(mask, mmm), vec_bitandnotleft1q(mask, mmi)); //common
 
-                        comp = _mm_or_si128(_mm_andnot_si128(maskF, comp), _mm_and_si128(maskF, mNINF));
+                        comp = vec_bitor1q(vec_bitandnotleft1q(maskF, comp), vec_bitand1q(maskF, mNINF));
 
-                        const __m128i M = _mm_add_epi16(x, comp);
-                        x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                        const __m128i M = vec_add8sh(x, comp);
+                        x = vec_max8sh(vec_max8sh(h, v), M); //common
 
                         X[t+2].m = x; //common
 
-                        const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                        h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                        v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                        const __m128i Moe = vec_add8sh(M, mmoe); //common
+                        h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                        v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                        MV[t+1] = _mm_extract_epi16(v, 7); //common
-                        v = _mm_slli_si128(v, 2); //common
+                        MV[t+1] = vec_extract8sh(v, 7); //common
+                        v = vec_shiftleftbytes1q(v, 2); //common
                         --b; //common
                     }
 
                     for (int t=7; t < n+7; t++) {
-                        __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                        v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                        __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                        v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                        __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                        const __m128i comp = _mm_add_epi16(x, mmi);
-                        mask = _mm_and_si128(mask, mmmi);
+                        __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                        const __m128i comp = vec_add8sh(x, mmi);
+                        mask = vec_bitand1q(mask, mmmi);
 
-                        const __m128i M = _mm_add_epi16(mask, comp);
-                        x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                        const __m128i M = vec_add8sh(mask, comp);
+                        x = vec_max8sh(vec_max8sh(h, v), M); //common
 
                         X[t+2].m = x; //common
 
-                        const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                        h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                        v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                        const __m128i Moe = vec_add8sh(M, mmoe); //common
+                        h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                        v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                        MV[t+1] = _mm_extract_epi16(v, 7); //common
-                        v = _mm_slli_si128(v, 2); //common
+                        MV[t+1] = vec_extract8sh(v, 7); //common
+                        v = vec_shiftleftbytes1q(v, 2); //common
                         --b; //common
                     }
                   }
@@ -130,34 +127,34 @@ int Solution6::process(const string& bs, const string& as, int qsc, int qec,
         }
         else {
             for (int bl=0; bl < nb; bl+=8) {
-                A = _mm_lddqu_si128((__m128i*)a);
+                A = vec_load1qu((__m128i*)a);
                 a += 8;
                 b = B + n+7;
                   {
-                    X[0].m = _mm_setzero_si128();
-                    X[1].m = _mm_setzero_si128();
+                    X[0].m = vec_zero1q();
+                    X[1].m = vec_zero1q();
                     __m128i h = mmoe;
                     __m128i v = mmoe;
 
                     for (int t=0; t < n+7; t++) {
-                        __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                        v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                        __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                        v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                        __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                        const __m128i comp = _mm_add_epi16(x, mmi);
-                        mask = _mm_and_si128(mask, mmmi);
+                        __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                        const __m128i comp = vec_add8sh(x, mmi);
+                        mask = vec_bitand1q(mask, mmmi);
 
-                        const __m128i M = _mm_max_epi16(_mm_setzero_si128(), _mm_add_epi16(mask, comp));
-                        x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                        const __m128i M = vec_max8sh(vec_zero1q(), vec_add8sh(mask, comp));
+                        x = vec_max8sh(vec_max8sh(h, v), M); //common
 
                         X[t+2].m = x; //common
 
-                        const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                        h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                        v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                        const __m128i Moe = vec_add8sh(M, mmoe); //common
+                        h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                        v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                        MV[t+1] = _mm_extract_epi16(v, 7); //common
-                        v = _mm_slli_si128(v, 2); //common
+                        MV[t+1] = vec_extract8sh(v, 7); //common
+                        v = vec_shiftleftbytes1q(v, 2); //common
                         --b; //common
                     }
                   }
@@ -225,71 +222,71 @@ int Solution6::process(const string& bs, const string& as, int qsc, int qec,
             if (n < 7) {
                 for (int bl=0; bl < nb; bl+=8) {
                     m128si16 tmp_opt;
-                    A = _mm_lddqu_si128((__m128i*)a);
+                    A = vec_load1qu((__m128i*)a);
                     a += 8;
                     b = B + n+7;
                       {
-                        __m128i tmp = _mm_set1_epi16(oe + e * bl);
-                        X[0].m = _mm_sub_epi16(tmp, mme);
+                        __m128i tmp = vec_splat8sh(oe + e * bl);
+                        X[0].m = vec_subtract8sh(tmp, mme);
                         X[1].m = tmp;
                         tmp_opt.m = mNINF;
-                        __m128i h = _mm_add_epi16(tmp, mme);
+                        __m128i h = vec_add8sh(tmp, mme);
                         __m128i v = mNINF;
                         __m128i maskF = mnegones;
 
                         for (int t=0; t < n; t++) {
-                            maskF = _mm_slli_si128(maskF, 2);
+                            maskF = vec_shiftleftbytes1q(maskF, 2);
 
-                            __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                            v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                            __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                            v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                            __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                            __m128i comp = _mm_or_si128(_mm_and_si128(mask, mmm), _mm_andnot_si128(mask, mmi)); //common
+                            __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                            __m128i comp = vec_bitor1q(vec_bitand1q(mask, mmm), vec_bitandnotleft1q(mask, mmi)); //common
 
-                            comp = _mm_or_si128(_mm_andnot_si128(maskF, comp), _mm_and_si128(maskF, mNINF));
+                            comp = vec_bitor1q(vec_bitandnotleft1q(maskF, comp), vec_bitand1q(maskF, mNINF));
 
-                            const __m128i M = _mm_add_epi16(x, comp);
-                            x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                            const __m128i M = vec_add8sh(x, comp);
+                            x = vec_max8sh(vec_max8sh(h, v), M); //common
 
-                            tmp_opt.m = _mm_max_epi16(tmp_opt.m, x); //common
+                            tmp_opt.m = vec_max8sh(tmp_opt.m, x); //common
                             X[t+2].m = x; //common
 
-                            const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                            h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                            v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                            const __m128i Moe = vec_add8sh(M, mmoe); //common
+                            h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                            v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                            MV[t+1] = _mm_extract_epi16(v, 7); //common
-                            v = _mm_slli_si128(v, 2); //common
+                            MV[t+1] = vec_extract8sh(v, 7); //common
+                            v = vec_shiftleftbytes1q(v, 2); //common
                             --b; //common
                         }
 
                         __m128i maskL = mnegones;
 
                         for (int t=n; t < n+7; t++) {
-                            maskF = _mm_slli_si128(maskF, 2);
-                            maskL = _mm_slli_si128(maskL, 2);
+                            maskF = vec_shiftleftbytes1q(maskF, 2);
+                            maskL = vec_shiftleftbytes1q(maskL, 2);
 
-                            __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                            v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                            __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                            v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                            __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                            __m128i comp = _mm_or_si128(_mm_and_si128(mask, mmm), _mm_andnot_si128(mask, mmi)); //common
+                            __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                            __m128i comp = vec_bitor1q(vec_bitand1q(mask, mmm), vec_bitandnotleft1q(mask, mmi)); //common
 
-                            comp = _mm_or_si128(_mm_andnot_si128(maskF, comp), _mm_and_si128(maskF, mNINF));
+                            comp = vec_bitor1q(vec_bitandnotleft1q(maskF, comp), vec_bitand1q(maskF, mNINF));
 
-                            const __m128i M = _mm_add_epi16(x, comp);
-                            x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                            const __m128i M = vec_add8sh(x, comp);
+                            x = vec_max8sh(vec_max8sh(h, v), M); //common
 
-                            x = _mm_or_si128(_mm_and_si128(maskL, x), _mm_andnot_si128(maskL, mNINF));
-                            tmp_opt.m = _mm_max_epi16(tmp_opt.m, x); //common
+                            x = vec_bitor1q(vec_bitand1q(maskL, x), vec_bitandnotleft1q(maskL, mNINF));
+                            tmp_opt.m = vec_max8sh(tmp_opt.m, x); //common
                             X[t+2].m = x; //common
 
-                            const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                            h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                            v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                            const __m128i Moe = vec_add8sh(M, mmoe); //common
+                            h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                            v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                            MV[t+1] = _mm_extract_epi16(v, 7); //common
-                            v = _mm_slli_si128(v, 2); //common
+                            MV[t+1] = vec_extract8sh(v, 7); //common
+                            v = vec_shiftleftbytes1q(v, 2); //common
                             --b; //common
                         }
                       }
@@ -404,92 +401,92 @@ int Solution6::process(const string& bs, const string& as, int qsc, int qec,
             else {
                 for (int bl=0; bl < nb; bl+=8) {
                     m128si16 tmp_opt;
-                    A = _mm_lddqu_si128((__m128i*)a);
+                    A = vec_load1qu((__m128i*)a);
                     a += 8;
                     b = B + n+7;
                       {
-                        __m128i tmp = _mm_set1_epi16(oe + e * bl);
-                        X[0].m = _mm_sub_epi16(tmp, mme);
+                        __m128i tmp = vec_splat8sh(oe + e * bl);
+                        X[0].m = vec_subtract8sh(tmp, mme);
                         X[1].m = tmp;
                         tmp_opt.m = mNINF;
-                        __m128i h = _mm_add_epi16(tmp, mme);
+                        __m128i h = vec_add8sh(tmp, mme);
                         __m128i v = mNINF;
                         __m128i maskF = mnegones;
 
                         for (int t=0; t < 7; t++) {
-                            maskF = _mm_slli_si128(maskF, 2);
+                            maskF = vec_shiftleftbytes1q(maskF, 2);
 
-                            __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                            v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                            __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                            v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                            __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                            __m128i comp = _mm_or_si128(_mm_and_si128(mask, mmm), _mm_andnot_si128(mask, mmi)); //common
+                            __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                            __m128i comp = vec_bitor1q(vec_bitand1q(mask, mmm), vec_bitandnotleft1q(mask, mmi)); //common
 
-                            comp = _mm_or_si128(_mm_andnot_si128(maskF, comp), _mm_and_si128(maskF, mNINF));
+                            comp = vec_bitor1q(vec_bitandnotleft1q(maskF, comp), vec_bitand1q(maskF, mNINF));
 
-                            const __m128i M = _mm_add_epi16(x, comp);
-                            x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                            const __m128i M = vec_add8sh(x, comp);
+                            x = vec_max8sh(vec_max8sh(h, v), M); //common
 
-                            tmp_opt.m = _mm_max_epi16(tmp_opt.m, x); //common
+                            tmp_opt.m = vec_max8sh(tmp_opt.m, x); //common
                             X[t+2].m = x; //common
 
-                            const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                            h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                            v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                            const __m128i Moe = vec_add8sh(M, mmoe); //common
+                            h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                            v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                            MV[t+1] = _mm_extract_epi16(v, 7); //common
-                            v = _mm_slli_si128(v, 2); //common
+                            MV[t+1] = vec_extract8sh(v, 7); //common
+                            v = vec_shiftleftbytes1q(v, 2); //common
                             --b; //common
                         }
 
                         for (int t=7; t < n; t++) {
-                            __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                            v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                            __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                            v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                            __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                            const __m128i comp = _mm_add_epi16(x, mmi);
-                            mask = _mm_and_si128(mask, mmmi);
+                            __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                            const __m128i comp = vec_add8sh(x, mmi);
+                            mask = vec_bitand1q(mask, mmmi);
 
-                            const __m128i M = _mm_add_epi16(mask, comp);
-                            x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                            const __m128i M = vec_add8sh(mask, comp);
+                            x = vec_max8sh(vec_max8sh(h, v), M); //common
 
-                            tmp_opt.m = _mm_max_epi16(tmp_opt.m, x); //common
+                            tmp_opt.m = vec_max8sh(tmp_opt.m, x); //common
                             X[t+2].m = x; //common
 
-                            const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                            h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                            v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                            const __m128i Moe = vec_add8sh(M, mmoe); //common
+                            h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                            v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                            MV[t+1] = _mm_extract_epi16(v, 7); //common
-                            v = _mm_slli_si128(v, 2); //common
+                            MV[t+1] = vec_extract8sh(v, 7); //common
+                            v = vec_shiftleftbytes1q(v, 2); //common
                             --b; //common
                         }
 
                         __m128i maskL = mnegones;
 
                         for (int t=n; t < n+7; t++) {
-                            maskL = _mm_slli_si128(maskL, 2);
+                            maskL = vec_shiftleftbytes1q(maskL, 2);
 
-                            __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                            v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                            __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                            v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                            __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                            const __m128i comp = _mm_add_epi16(x, mmi);
-                            mask = _mm_and_si128(mask, mmmi);
+                            __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                            const __m128i comp = vec_add8sh(x, mmi);
+                            mask = vec_bitand1q(mask, mmmi);
 
-                            const __m128i M = _mm_add_epi16(mask, comp);
-                            x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                            const __m128i M = vec_add8sh(mask, comp);
+                            x = vec_max8sh(vec_max8sh(h, v), M); //common
 
-                            x = _mm_or_si128(_mm_and_si128(maskL, x), _mm_andnot_si128(maskL, mNINF));
-                            tmp_opt.m = _mm_max_epi16(tmp_opt.m, x); //common
+                            x = vec_bitor1q(vec_bitand1q(maskL, x), vec_bitandnotleft1q(maskL, mNINF));
+                            tmp_opt.m = vec_max8sh(tmp_opt.m, x); //common
                             X[t+2].m = x; //common
 
-                            const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                            h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                            v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                            const __m128i Moe = vec_add8sh(M, mmoe); //common
+                            h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                            v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                            MV[t+1] = _mm_extract_epi16(v, 7); //common
-                            v = _mm_slli_si128(v, 2); //common
+                            MV[t+1] = vec_extract8sh(v, 7); //common
+                            v = vec_shiftleftbytes1q(v, 2); //common
                             --b; //common
                         }
                       }
@@ -605,64 +602,64 @@ int Solution6::process(const string& bs, const string& as, int qsc, int qec,
         else {
             for (int bl=0; bl < nb; bl+=8) {
                 m128si16 tmp_opt;
-                A = _mm_lddqu_si128((__m128i*)a);
+                A = vec_load1qu((__m128i*)a);
                 a += 8;
                 b = B + n+7;
                   {
-                    X[0].m = _mm_setzero_si128();
-                    X[1].m = _mm_setzero_si128();
-                    tmp_opt.m = _mm_setzero_si128();
+                    X[0].m = vec_zero1q();
+                    X[1].m = vec_zero1q();
+                    tmp_opt.m = vec_zero1q();
                     __m128i h = mmoe;
                     __m128i v = mmoe;
 
                     for (int t=0; t < n; t++) {
-                        __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                        v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                        __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                        v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                        __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                        const __m128i comp = _mm_add_epi16(x, mmi);
-                        mask = _mm_and_si128(mask, mmmi);
+                        __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                        const __m128i comp = vec_add8sh(x, mmi);
+                        mask = vec_bitand1q(mask, mmmi);
 
-                        const __m128i M = _mm_max_epi16(_mm_setzero_si128(), _mm_add_epi16(mask, comp));
-                        x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                        const __m128i M = vec_max8sh(vec_zero1q(), vec_add8sh(mask, comp));
+                        x = vec_max8sh(vec_max8sh(h, v), M); //common
 
                         X[t+2].m = x; //common
-                        tmp_opt.m = _mm_max_epi16(tmp_opt.m, x); //common
+                        tmp_opt.m = vec_max8sh(tmp_opt.m, x); //common
 
-                        const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                        h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                        v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                        const __m128i Moe = vec_add8sh(M, mmoe); //common
+                        h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                        v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                        MV[t+1] = _mm_extract_epi16(v, 7); //common
-                        v = _mm_slli_si128(v, 2); //common
+                        MV[t+1] = vec_extract8sh(v, 7); //common
+                        v = vec_shiftleftbytes1q(v, 2); //common
                         --b; //common
                     }
 
                     __m128i maskL = mnegones;
 
                     for (int t=n; t < n+7; t++) {
-                        maskL = _mm_slli_si128(maskL, 2);
+                        maskL = vec_shiftleftbytes1q(maskL, 2);
 
-                        __m128i x = _mm_or_si128(_mm_slli_si128(X[t].m, 2), _mm_srli_si128(X[t+8].m, 14)); //common
-                        v = _mm_insert_epi16(v, MV[t+8], 0); //common
+                        __m128i x = vec_bitor1q(vec_shiftleftbytes1q(X[t].m, 2), vec_shiftrightbytes1q(X[t+8].m, 14)); //common
+                        v = vec_insert8sh(v, MV[t+8], 0); //common
 
-                        __m128i mask = _mm_cmpeq_epi16(A, _mm_lddqu_si128((__m128i*)b)); //common
-                        const __m128i comp = _mm_add_epi16(x, mmi);
-                        mask = _mm_and_si128(mask, mmmi);
+                        __m128i mask = vec_compareeq8sh(A, vec_load1qu((__m128i*)b)); //common
+                        const __m128i comp = vec_add8sh(x, mmi);
+                        mask = vec_bitand1q(mask, mmmi);
 
-                        const __m128i M = _mm_max_epi16(_mm_setzero_si128(), _mm_add_epi16(mask, comp));
-                        x = _mm_max_epi16(_mm_max_epi16(h, v), M); //common
+                        const __m128i M = vec_max8sh(vec_zero1q(), vec_add8sh(mask, comp));
+                        x = vec_max8sh(vec_max8sh(h, v), M); //common
 
                         X[t+2].m = x; //common
-                        x = _mm_and_si128(x, maskL);
-                        tmp_opt.m = _mm_max_epi16(tmp_opt.m, x); //common
+                        x = vec_bitand1q(x, maskL);
+                        tmp_opt.m = vec_max8sh(tmp_opt.m, x); //common
 
-                        const __m128i Moe = _mm_add_epi16(M, mmoe); //common
-                        h = _mm_max_epi16(_mm_add_epi16(h, mme), Moe); //common
-                        v = _mm_max_epi16(_mm_add_epi16(v, mme), Moe); //common
+                        const __m128i Moe = vec_add8sh(M, mmoe); //common
+                        h = vec_max8sh(vec_add8sh(h, mme), Moe); //common
+                        v = vec_max8sh(vec_add8sh(v, mme), Moe); //common
 
-                        MV[t+1] = _mm_extract_epi16(v, 7); //common
-                        v = _mm_slli_si128(v, 2); //common
+                        MV[t+1] = vec_extract8sh(v, 7); //common
+                        v = vec_shiftleftbytes1q(v, 2); //common
                         --b; //common
                     }
                   }
--- src/sw/lib/Solution7.cpp
+++ src/sw/lib/Solution7.cpp
@@ -7,8 +7,7 @@
 #include <cstdio>
 #include <cstdlib>
 #include <cstring>
-#include <xmmintrin.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include "Solution7.h"
 
 using namespace std;
@@ -49,8 +48,8 @@ void Solution7::process_with_start_clip(
       int zero = offset_u8 = 1 + neg_gap_open + neg_gap_ext;
 
       // setup constant vector
-      __m128i v_neg_inf = _mm_setzero_si128();
-      __m128i v_pos_inf; v_pos_inf = _mm_cmpeq_epi8(v_pos_inf, v_pos_inf);
+      __m128i v_neg_inf = vec_zero1q();
+      __m128i v_pos_inf; v_pos_inf = vec_compareeq16sb(v_pos_inf, v_pos_inf);
       __m128i v_zero; mm_set1_epi8(v_zero, zero);
       __m128i v_mi; mm_set1_epi8(v_mi, neg_mismatch);
       __m128i v_ma; mm_set1_epi8(v_ma, match_score);
@@ -59,9 +58,9 @@ void Solution7::process_with_start_clip(
 
       // clear line buffers
       for(int i = 0; i < block_num_u8; ++i){
-          _mm_store_si128((__m128i *)(blocks_u8[i].data[0].M), v_zero);
-          _mm_store_si128((__m128i *)(blocks_u8[i].data[0].V), v_neg_inf);
-          _mm_store_si128((__m128i *)(blocks_u8[i].data[0].H), v_neg_inf);
+          vec_store1q((__m128i *)(blocks_u8[i].data[0].M), v_zero);
+          vec_store1q((__m128i *)(blocks_u8[i].data[0].V), v_neg_inf);
+          vec_store1q((__m128i *)(blocks_u8[i].data[0].H), v_neg_inf);
       }
 
       // calculate
@@ -70,68 +69,68 @@ void Solution7::process_with_start_clip(
           __m128i v_query; mm_set1_epi8(v_query, query[i]);
 
           const int tail = block_num_u8 - 1;
-          __m128i m0 = _mm_load_si128((__m128i *)(blocks_u8[tail].data[load].M));
-          m0 = _mm_or_si128(_mm_slli_si128(m0, 1), _mm_srli_si128(v_zero, 15));
-          __m128i v0 = _mm_load_si128((__m128i *)(blocks_u8[tail].data[load].V));
-          v0 = _mm_slli_si128(v0, 1);
-          __m128i h0 = _mm_load_si128((__m128i *)(blocks_u8[tail].data[load].H));
-          h0 = _mm_slli_si128(h0, 1);
+          __m128i m0 = vec_load1q((__m128i *)(blocks_u8[tail].data[load].M));
+          m0 = vec_bitor1q(vec_shiftleftbytes1q(m0, 1), vec_shiftrightbytes1q(v_zero, 15));
+          __m128i v0 = vec_load1q((__m128i *)(blocks_u8[tail].data[load].V));
+          v0 = vec_shiftleftbytes1q(v0, 1);
+          __m128i h0 = vec_load1q((__m128i *)(blocks_u8[tail].data[load].H));
+          h0 = vec_shiftleftbytes1q(h0, 1);
           __m128i m2 = v_zero, h2 = v_neg_inf;
 
           __m128i v_row_max = v_neg_inf;
           for(int j = 0; j < block_num_u8; ++j){
-              __m128i v_target = _mm_load_si128((__m128i *)(blocks_u8[j].target));
-              __m128i match = _mm_cmpeq_epi8(v_query, v_target);
-              __m128i score_add = _mm_and_si128(match, v_ma);
-              __m128i score_sub = _mm_andnot_si128(match, v_mi);
-
-              __m128i m3 = _mm_max_epu8(_mm_max_epu8(m0, v0), h0);
-              m3 = _mm_adds_epu8(m3, score_add);
-              m3 = _mm_subs_epu8(m3, score_sub);
-              m3 = _mm_max_epu8(m3, v_zero);
-              v_row_max = _mm_max_epu8(v_row_max, m3);
-              __m128i m1 = _mm_load_si128((__m128i *)(blocks_u8[j].data[load].M));
-              _mm_store_si128((__m128i *)(blocks_u8[j].data[store].M), m3);
-
-              __m128i v1 = _mm_load_si128((__m128i *)(blocks_u8[j].data[load].V));
-              __m128i m1o = _mm_subs_epu8(m1, v_gap_open);
-              __m128i v3 = _mm_subs_epu8(_mm_max_epu8(m1o, v1), v_gap_ext);
-              _mm_store_si128((__m128i *)(blocks_u8[j].data[store].V), v3);
-
-              __m128i m2o = _mm_subs_epu8(m2, v_gap_open);
-              __m128i h3 = _mm_subs_epu8(_mm_max_epu8(m2o, h2), v_gap_ext);
-              __m128i h1 = _mm_load_si128((__m128i *)(blocks_u8[j].data[load].H));
-              _mm_store_si128((__m128i *)(blocks_u8[j].data[store].H), h3);
+              __m128i v_target = vec_load1q((__m128i *)(blocks_u8[j].target));
+              __m128i match = vec_compareeq16sb(v_query, v_target);
+              __m128i score_add = vec_bitand1q(match, v_ma);
+              __m128i score_sub = vec_bitandnotleft1q(match, v_mi);
+
+              __m128i m3 = vec_max16ub(vec_max16ub(m0, v0), h0);
+              m3 = vec_addsaturating16ub(m3, score_add);
+              m3 = vec_subtractsaturating16ub(m3, score_sub);
+              m3 = vec_max16ub(m3, v_zero);
+              v_row_max = vec_max16ub(v_row_max, m3);
+              __m128i m1 = vec_load1q((__m128i *)(blocks_u8[j].data[load].M));
+              vec_store1q((__m128i *)(blocks_u8[j].data[store].M), m3);
+
+              __m128i v1 = vec_load1q((__m128i *)(blocks_u8[j].data[load].V));
+              __m128i m1o = vec_subtractsaturating16ub(m1, v_gap_open);
+              __m128i v3 = vec_subtractsaturating16ub(vec_max16ub(m1o, v1), v_gap_ext);
+              vec_store1q((__m128i *)(blocks_u8[j].data[store].V), v3);
+
+              __m128i m2o = vec_subtractsaturating16ub(m2, v_gap_open);
+              __m128i h3 = vec_subtractsaturating16ub(vec_max16ub(m2o, h2), v_gap_ext);
+              __m128i h1 = vec_load1q((__m128i *)(blocks_u8[j].data[load].H));
+              vec_store1q((__m128i *)(blocks_u8[j].data[store].H), h3);
 
               m0 = m1; m2 = m3; v0 = v1; h0 = h1; h2 = h3;
           }
-          if(_mm_movemask_epi8(_mm_cmpeq_epi8(v_row_max, v_pos_inf))){
+          if(vec_extractupperbit16sb(vec_compareeq16sb(v_row_max, v_pos_inf))){
               i16_begin_line = i;
               break;
           }
             {
-              __m128i m2 = _mm_load_si128((__m128i *)(blocks_u8[tail].data[store].M));
-              m2 = _mm_or_si128(_mm_slli_si128(m2, 1), _mm_srli_si128(v_zero, 15));
-              __m128i h2 = _mm_load_si128((__m128i *)(blocks_u8[tail].data[store].H));
-              h2 = _mm_slli_si128(h2, 1);
+              __m128i m2 = vec_load1q((__m128i *)(blocks_u8[tail].data[store].M));
+              m2 = vec_bitor1q(vec_shiftleftbytes1q(m2, 1), vec_shiftrightbytes1q(v_zero, 15));
+              __m128i h2 = vec_load1q((__m128i *)(blocks_u8[tail].data[store].H));
+              h2 = vec_shiftleftbytes1q(h2, 1);
               int j = 0;
               while(true){
-                  __m128i h3_old = _mm_load_si128((__m128i *)(blocks_u8[j].data[store].H));
-                  __m128i m2o = _mm_subs_epu8(m2, v_gap_open);
-                  __m128i h3_new = _mm_subs_epu8(_mm_max_epu8(m2o, h2), v_gap_ext);
-                  __m128i cmpgt = _mm_cmpeq_epi8(
-                                                 _mm_setzero_si128(), _mm_subs_epu8(h3_new, h3_old));
-                  int cmp = _mm_movemask_epi8(cmpgt) ^ 0xffff;
+                  __m128i h3_old = vec_load1q((__m128i *)(blocks_u8[j].data[store].H));
+                  __m128i m2o = vec_subtractsaturating16ub(m2, v_gap_open);
+                  __m128i h3_new = vec_subtractsaturating16ub(vec_max16ub(m2o, h2), v_gap_ext);
+                  __m128i cmpgt = vec_compareeq16sb(
+                                                 vec_zero1q(), vec_subtractsaturating16ub(h3_new, h3_old));
+                  int cmp = vec_extractupperbit16sb(cmpgt) ^ 0xffff;
                   if(!cmp){ break; }
-                  _mm_store_si128((__m128i *)(blocks_u8[j].data[store].H), h3_new);
+                  vec_store1q((__m128i *)(blocks_u8[j].data[store].H), h3_new);
                   if(j == tail){
-                      m2 = _mm_load_si128((__m128i *)(blocks_u8[tail].data[store].M));
-                      m2 = _mm_or_si128(_mm_slli_si128(m2, 1), _mm_srli_si128(v_zero, 15));
-                      h2 = _mm_load_si128((__m128i *)(blocks_u8[tail].data[store].H));
-                      h2 = _mm_slli_si128(h2, 1);
+                      m2 = vec_load1q((__m128i *)(blocks_u8[tail].data[store].M));
+                      m2 = vec_bitor1q(vec_shiftleftbytes1q(m2, 1), vec_shiftrightbytes1q(v_zero, 15));
+                      h2 = vec_load1q((__m128i *)(blocks_u8[tail].data[store].H));
+                      h2 = vec_shiftleftbytes1q(h2, 1);
                       j = 0;
                   }else{
-                      m2 = _mm_load_si128((__m128i *)(blocks_u8[j].data[store].M));
+                      m2 = vec_load1q((__m128i *)(blocks_u8[j].data[store].M));
                       h2 = h3_new;
                       ++j;
                   }
@@ -147,15 +146,15 @@ void Solution7::process_with_start_clip(
                       n_best = 0;
                   }
                   __m128i v_opt; mm_set1_epi8(v_opt, opt);
-                  __m128i v_count = _mm_setzero_si128();
+                  __m128i v_count = vec_zero1q();
                   /*
                   if(dir){
                       int local_position = 0;
                       for(int j = 0; j < block_num_u8; ++j){
-                          __m128i m = _mm_load_si128((__m128i *)(blocks_u8[j].data[store].M));
-                          __m128i f = _mm_cmpeq_epi8(m, v_opt);
-                          v_count = _mm_add_epi8(v_count, f);
-                          const int mask = _mm_movemask_epi8(f);
+                          __m128i m = vec_load1q((__m128i *)(blocks_u8[j].data[store].M));
+                          __m128i f = vec_compareeq16sb(m, v_opt);
+                          v_count = vec_add16sb(v_count, f);
+                          const int mask = vec_extractupperbit16sb(f);
                           const int k = 31 - __builtin_clz(mask);
                           const int p = k * block_num_u8 + j;
                           if(mask){ local_position = max(p, local_position); }
@@ -165,10 +164,10 @@ void Solution7::process_with_start_clip(
                   }else{
                       int local_position = 0x7fffffff;
                       for(int j = block_num_u8 - 1; j >= 0; --j){
-                          __m128i m = _mm_load_si128((__m128i *)(blocks_u8[j].data[store].M));
-                          __m128i f = _mm_cmpeq_epi8(m, v_opt);
-                          v_count = _mm_add_epi8(v_count, f);
-                          const int mask = _mm_movemask_epi8(f);
+                          __m128i m = vec_load1q((__m128i *)(blocks_u8[j].data[store].M));
+                          __m128i f = vec_compareeq16sb(m, v_opt);
+                          v_count = vec_add16sb(v_count, f);
+                          const int mask = vec_extractupperbit16sb(f);
                           const int k = __builtin_ctz(mask);
                           const int p = k * block_num_u8 + j;
                           if(mask){ local_position = min(p, local_position); }
@@ -179,10 +178,10 @@ void Solution7::process_with_start_clip(
                   */
                   int local_position = 0;
                   for(int j = 0; j < block_num_u8; ++j){
-                      __m128i m = _mm_load_si128((__m128i *)(blocks_u8[j].data[store].M));
-                      __m128i f = _mm_cmpeq_epi8(m, v_opt);
-                      v_count = _mm_add_epi8(v_count, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i m = vec_load1q((__m128i *)(blocks_u8[j].data[store].M));
+                      __m128i f = vec_compareeq16sb(m, v_opt);
+                      v_count = vec_add16sb(v_count, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int k = 31 - __builtin_clz(mask);
                       const int p = k * block_num_u8 + j;
                       if(mask){ local_position = max(p, local_position); }
@@ -192,43 +191,43 @@ void Solution7::process_with_start_clip(
                   } else {
                       if(i < qe || (i == qe && te < local_position)) qe = i, te = local_position;
                   }
-                  v_count = _mm_sub_epi8(_mm_setzero_si128(), v_count);
-                  v_count = _mm_sad_epu8(_mm_setzero_si128(), v_count);
-                  n_best += _mm_extract_epi16(v_count, 0) + _mm_extract_epi16(v_count, 4);
+                  v_count = vec_subtract16sb(vec_zero1q(), v_count);
+                  v_count = vec_sumabsdiffs16ub(vec_zero1q(), v_count);
+                  n_best += vec_extract8sh(v_count, 0) + vec_extract8sh(v_count, 4);
               }
           }else if(i == m - 1){
               const int head_mask_shift = (ceil_n_u8 - n) / block_num_u8;
               const int tail_mask_threshold = block_num_u8 - (ceil_n_u8 - n) % block_num_u8;
-              __m128i head_mask; head_mask = _mm_cmpeq_epi8(head_mask, head_mask);
-              if(head_mask_shift & 8){ head_mask = _mm_srli_si128(head_mask, 8); }
-              if(head_mask_shift & 4){ head_mask = _mm_srli_si128(head_mask, 4); }
-              if(head_mask_shift & 2){ head_mask = _mm_srli_si128(head_mask, 2); }
-              if(head_mask_shift & 1){ head_mask = _mm_srli_si128(head_mask, 1); }
-              __m128i tail_mask = _mm_srli_si128(head_mask, 1);
+              __m128i head_mask; head_mask = vec_compareeq16sb(head_mask, head_mask);
+              if(head_mask_shift & 8){ head_mask = vec_shiftrightbytes1q(head_mask, 8); }
+              if(head_mask_shift & 4){ head_mask = vec_shiftrightbytes1q(head_mask, 4); }
+              if(head_mask_shift & 2){ head_mask = vec_shiftrightbytes1q(head_mask, 2); }
+              if(head_mask_shift & 1){ head_mask = vec_shiftrightbytes1q(head_mask, 1); }
+              __m128i tail_mask = vec_shiftrightbytes1q(head_mask, 1);
               v_row_max = v_neg_inf;
               for(int j = 0; j < block_num_u8; ++j){
-                  __m128i m = _mm_load_si128((__m128i *)(blocks_u8[j].data[store].M));
-                  __m128i v = _mm_load_si128((__m128i *)(blocks_u8[j].data[store].V));
-                  __m128i mx = _mm_max_epu8(m, v);
+                  __m128i m = vec_load1q((__m128i *)(blocks_u8[j].data[store].M));
+                  __m128i v = vec_load1q((__m128i *)(blocks_u8[j].data[store].V));
+                  __m128i mx = vec_max16ub(m, v);
                   if(j < tail_mask_threshold){
-                      mx = _mm_and_si128(mx, head_mask);
+                      mx = vec_bitand1q(mx, head_mask);
                   }else{
-                      mx = _mm_and_si128(mx, tail_mask);
+                      mx = vec_bitand1q(mx, tail_mask);
                   }
-                  v_row_max = _mm_max_epu8(v_row_max, mx);
-                  _mm_store_si128((__m128i *)(X_u8 + (j << 4)), mx);
+                  v_row_max = vec_max16ub(v_row_max, mx);
+                  vec_store1q((__m128i *)(X_u8 + (j << 4)), mx);
               }
               mm_reduction_max_epu8(opt, v_row_max);
               __m128i v_opt; mm_set1_epi8(v_opt, opt);
-              __m128i v_count = _mm_setzero_si128();
+              __m128i v_count = vec_zero1q();
               /*
               if(dir){
                   position = 0;
                   for(int j = 0; j < block_num_u8; ++j){
-                      __m128i m = _mm_load_si128((__m128i *)(X_u8 + (j << 4)));
-                      __m128i f = _mm_cmpeq_epi8(m, v_opt);
-                      v_count = _mm_add_epi8(v_count, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i m = vec_load1q((__m128i *)(X_u8 + (j << 4)));
+                      __m128i f = vec_compareeq16sb(m, v_opt);
+                      v_count = vec_add16sb(v_count, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int k = 31 - __builtin_clz(mask);
                       const int p = k * block_num_u8 + j;
                       if(mask){ position = max(p, position); }
@@ -236,10 +235,10 @@ void Solution7::process_with_start_clip(
               }else{
                   position = 0x7fffffff;
                   for(int j = block_num_u8 - 1; j >= 0; --j){
-                      __m128i m = _mm_load_si128((__m128i *)(X_u8 + (j << 4)));
-                      __m128i f = _mm_cmpeq_epi8(m, v_opt);
-                      v_count = _mm_add_epi8(v_count, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i m = vec_load1q((__m128i *)(X_u8 + (j << 4)));
+                      __m128i f = vec_compareeq16sb(m, v_opt);
+                      v_count = vec_add16sb(v_count, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int k = __builtin_ctz(mask);
                       const int p = k * block_num_u8 + j;
                       if(mask){ position = min(p, position); }
@@ -248,17 +247,17 @@ void Solution7::process_with_start_clip(
               */
               te = 0;
               for(int j = 0; j < block_num_u8; ++j){
-                  __m128i m = _mm_load_si128((__m128i *)(X_u8 + (j << 4)));
-                  __m128i f = _mm_cmpeq_epi8(m, v_opt);
-                  v_count = _mm_add_epi8(v_count, f);
-                  const int mask = _mm_movemask_epi8(f);
+                  __m128i m = vec_load1q((__m128i *)(X_u8 + (j << 4)));
+                  __m128i f = vec_compareeq16sb(m, v_opt);
+                  v_count = vec_add16sb(v_count, f);
+                  const int mask = vec_extractupperbit16sb(f);
                   const int k = 31 - __builtin_clz(mask);
                   const int p = k * block_num_u8 + j;
                   if(mask){ te = max(p, te); }
               }
-              v_count = _mm_sub_epi8(_mm_setzero_si128(), v_count);
-              v_count = _mm_sad_epu8(_mm_setzero_si128(), v_count);
-              n_best += _mm_extract_epi16(v_count, 0) + _mm_extract_epi16(v_count, 4);
+              v_count = vec_subtract16sb(vec_zero1q(), v_count);
+              v_count = vec_sumabsdiffs16ub(vec_zero1q(), v_count);
+              n_best += vec_extract8sh(v_count, 0) + vec_extract8sh(v_count, 4);
               qe = i;
               //position |= (i << 16);
           }
@@ -281,8 +280,8 @@ void Solution7::process_with_start_clip(
       }
 
       // constant vectors
-      __m128i v_neg_inf; v_neg_inf = _mm_cmpeq_epi16(v_neg_inf, v_neg_inf);
-      v_neg_inf = _mm_slli_epi16(v_neg_inf, 15);
+      __m128i v_neg_inf; v_neg_inf = vec_compareeq8sh(v_neg_inf, v_neg_inf);
+      v_neg_inf = vec_shiftleftimmediate8sh(v_neg_inf, 15);
       __m128i v_ma; mm_set1_epi16(v_ma, match_score - mismatch_score);
       __m128i v_mi; mm_set1_epi16(v_mi, mismatch_score);
       __m128i v_gap_open; mm_set1_epi16(v_gap_open, gap_open);
@@ -310,59 +309,59 @@ void Solution7::process_with_start_clip(
       for(int i = i16_begin_line; i < m; ++i){
           __m128i v_query; mm_set1_epi16(v_query, query[i]);
 
-          __m128i m0 = _mm_load_si128((__m128i *)(blocks[block_num - 1].M));
-          m0 = _mm_slli_si128(m0, 2);
-          __m128i v0 = _mm_load_si128((__m128i *)(blocks[block_num - 1].V));
-          v0 = _mm_or_si128(_mm_slli_si128(v0, 2), _mm_srli_si128(v_neg_inf, 14));
-          __m128i h0 = _mm_load_si128((__m128i *)(blocks[block_num - 1].H));
-          h0 = _mm_or_si128(_mm_slli_si128(h0, 2), _mm_srli_si128(v_neg_inf, 14));
-          __m128i m2 = _mm_setzero_si128(), h2 = v_neg_inf;
+          __m128i m0 = vec_load1q((__m128i *)(blocks[block_num - 1].M));
+          m0 = vec_shiftleftbytes1q(m0, 2);
+          __m128i v0 = vec_load1q((__m128i *)(blocks[block_num - 1].V));
+          v0 = vec_bitor1q(vec_shiftleftbytes1q(v0, 2), vec_shiftrightbytes1q(v_neg_inf, 14));
+          __m128i h0 = vec_load1q((__m128i *)(blocks[block_num - 1].H));
+          h0 = vec_bitor1q(vec_shiftleftbytes1q(h0, 2), vec_shiftrightbytes1q(v_neg_inf, 14));
+          __m128i m2 = vec_zero1q(), h2 = v_neg_inf;
 
           __m128i v_row_max = v_neg_inf;
           for(int j = 0; j < block_num; ++j){
-              __m128i v_target = _mm_load_si128((__m128i *)(blocks[j].target));
-              __m128i match = _mm_cmpeq_epi16(v_query, v_target);
-              __m128i score = _mm_add_epi16(_mm_and_si128(match, v_ma), v_mi);
-
-              __m128i m3 = _mm_max_epi16(_mm_max_epi16(m0, v0), h0);
-              m3 = _mm_max_epi16(_mm_adds_epi16(m3, score), _mm_setzero_si128());
-              if(query_end_clip){ v_row_max = _mm_max_epi16(v_row_max, m3); }
-              __m128i m1 = _mm_load_si128((__m128i *)(blocks[j].M));
-              _mm_store_si128((__m128i *)(blocks[j].M), m3);
-
-              __m128i v1 = _mm_load_si128((__m128i *)(blocks[j].V));
-              __m128i m1o = _mm_adds_epi16(m1, v_gap_open);
-              __m128i v3 = _mm_adds_epi16(_mm_max_epi16(m1o, v1), v_gap_ext);
-              _mm_store_si128((__m128i *)(blocks[j].V), v3);
-
-              __m128i m2o = _mm_adds_epi16(m2, v_gap_open);
-              __m128i h3 = _mm_adds_epi16(_mm_max_epi16(m2o, h2), v_gap_ext);
-              __m128i h1 = _mm_load_si128((__m128i *)(blocks[j].H));
-              _mm_store_si128((__m128i *)(blocks[j].H), h3);
+              __m128i v_target = vec_load1q((__m128i *)(blocks[j].target));
+              __m128i match = vec_compareeq8sh(v_query, v_target);
+              __m128i score = vec_add8sh(vec_bitand1q(match, v_ma), v_mi);
+
+              __m128i m3 = vec_max8sh(vec_max8sh(m0, v0), h0);
+              m3 = vec_max8sh(vec_addsaturating8sh(m3, score), vec_zero1q());
+              if(query_end_clip){ v_row_max = vec_max8sh(v_row_max, m3); }
+              __m128i m1 = vec_load1q((__m128i *)(blocks[j].M));
+              vec_store1q((__m128i *)(blocks[j].M), m3);
+
+              __m128i v1 = vec_load1q((__m128i *)(blocks[j].V));
+              __m128i m1o = vec_addsaturating8sh(m1, v_gap_open);
+              __m128i v3 = vec_addsaturating8sh(vec_max8sh(m1o, v1), v_gap_ext);
+              vec_store1q((__m128i *)(blocks[j].V), v3);
+
+              __m128i m2o = vec_addsaturating8sh(m2, v_gap_open);
+              __m128i h3 = vec_addsaturating8sh(vec_max8sh(m2o, h2), v_gap_ext);
+              __m128i h1 = vec_load1q((__m128i *)(blocks[j].H));
+              vec_store1q((__m128i *)(blocks[j].H), h3);
 
               m0 = m1; m2 = m3; v0 = v1; h0 = h1; h2 = h3;
           }
             {
-              __m128i m2 = _mm_load_si128((__m128i *)(blocks[block_num - 1].M));
-              m2 = _mm_slli_si128(m2, 2);
-              __m128i h2 = _mm_load_si128((__m128i *)(blocks[block_num - 1].H));
-              h2 = _mm_or_si128(_mm_slli_si128(h2, 2), _mm_srli_si128(v_neg_inf, 14));
+              __m128i m2 = vec_load1q((__m128i *)(blocks[block_num - 1].M));
+              m2 = vec_shiftleftbytes1q(m2, 2);
+              __m128i h2 = vec_load1q((__m128i *)(blocks[block_num - 1].H));
+              h2 = vec_bitor1q(vec_shiftleftbytes1q(h2, 2), vec_shiftrightbytes1q(v_neg_inf, 14));
               int j = 0;
               while(true){
-                  __m128i h3_old = _mm_load_si128((__m128i *)(blocks[j].H));
-                  __m128i m2o = _mm_adds_epi16(m2, v_gap_open);
-                  __m128i h3_new = _mm_adds_epi16(_mm_max_epi16(m2o, h2), v_gap_ext);
-                  int cmp = _mm_movemask_epi8(_mm_cmpgt_epi16(h3_new, h3_old));
+                  __m128i h3_old = vec_load1q((__m128i *)(blocks[j].H));
+                  __m128i m2o = vec_addsaturating8sh(m2, v_gap_open);
+                  __m128i h3_new = vec_addsaturating8sh(vec_max8sh(m2o, h2), v_gap_ext);
+                  int cmp = vec_extractupperbit16sb(vec_comparegt8sh(h3_new, h3_old));
                   if(!cmp){ break; }
-                  _mm_store_si128((__m128i *)(blocks[j].H), h3_new);
+                  vec_store1q((__m128i *)(blocks[j].H), h3_new);
                   if(j == block_num - 1){
-                      m2 = _mm_load_si128((__m128i *)(blocks[j].M));
-                      m2 = _mm_slli_si128(m2, 2);
-                      h2 = _mm_load_si128((__m128i *)(blocks[j].H));
-                      h2 = _mm_or_si128(_mm_slli_si128(h2, 2), _mm_srli_si128(v_neg_inf, 14));
+                      m2 = vec_load1q((__m128i *)(blocks[j].M));
+                      m2 = vec_shiftleftbytes1q(m2, 2);
+                      h2 = vec_load1q((__m128i *)(blocks[j].H));
+                      h2 = vec_bitor1q(vec_shiftleftbytes1q(h2, 2), vec_shiftrightbytes1q(v_neg_inf, 14));
                       j = 0;
                   }else{
-                      m2 = _mm_load_si128((__m128i *)(blocks[j].M));
+                      m2 = vec_load1q((__m128i *)(blocks[j].M));
                       h2 = h3_new;
                       ++j;
                   }
@@ -378,15 +377,15 @@ void Solution7::process_with_start_clip(
                       n_best = 0;
                   }
                   __m128i v_opt; mm_set1_epi16(v_opt, opt);
-                  __m128i v_count = _mm_setzero_si128();
+                  __m128i v_count = vec_zero1q();
                   /*
                   if(dir){
                       int local_position = 0;
                       for(int j = 0; j < block_num; ++j){
-                          __m128i m = _mm_load_si128((__m128i *)(blocks[j].M));
-                          __m128i f = _mm_cmpeq_epi16(m, v_opt);
-                          v_count = _mm_add_epi8(v_count, f);
-                          const int mask = _mm_movemask_epi8(f);
+                          __m128i m = vec_load1q((__m128i *)(blocks[j].M));
+                          __m128i f = vec_compareeq8sh(m, v_opt);
+                          v_count = vec_add16sb(v_count, f);
+                          const int mask = vec_extractupperbit16sb(f);
                           const int k = (31 - __builtin_clz(mask)) >> 1;
                           const int p = k * block_num + j;
                           if(mask){ local_position = max(p, local_position); }
@@ -396,10 +395,10 @@ void Solution7::process_with_start_clip(
                   }else{
                       int local_position = 0x7fffffff;
                       for(int j = block_num - 1; j >= 0; --j){
-                          __m128i m = _mm_load_si128((__m128i *)(blocks[j].M));
-                          __m128i f = _mm_cmpeq_epi16(m, v_opt);
-                          v_count = _mm_add_epi8(v_count, f);
-                          const int mask = _mm_movemask_epi8(f);
+                          __m128i m = vec_load1q((__m128i *)(blocks[j].M));
+                          __m128i f = vec_compareeq8sh(m, v_opt);
+                          v_count = vec_add16sb(v_count, f);
+                          const int mask = vec_extractupperbit16sb(f);
                           const int k = __builtin_ctz(mask) >> 1;
                           const int p = k * block_num + j;
                           if(mask){ local_position = min(p, local_position); }
@@ -410,10 +409,10 @@ void Solution7::process_with_start_clip(
                   */
                   int local_position = 0;
                   for(int j = 0; j < block_num; ++j){
-                      __m128i m = _mm_load_si128((__m128i *)(blocks[j].M));
-                      __m128i f = _mm_cmpeq_epi16(m, v_opt);
-                      v_count = _mm_add_epi8(v_count, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i m = vec_load1q((__m128i *)(blocks[j].M));
+                      __m128i f = vec_compareeq8sh(m, v_opt);
+                      v_count = vec_add16sb(v_count, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int k = (31 - __builtin_clz(mask)) >> 1;
                       const int p = k * block_num + j;
                       if(mask){ local_position = max(p, local_position); }
@@ -423,42 +422,42 @@ void Solution7::process_with_start_clip(
                   } else {
                       if(i < qe || (i == qe && te < local_position)) qe = i, te = local_position;
                   }
-                  v_count = _mm_sub_epi8(_mm_setzero_si128(), v_count);
-                  v_count = _mm_sad_epu8(_mm_setzero_si128(), v_count);
-                  n_best += (_mm_extract_epi16(v_count, 0) + _mm_extract_epi16(v_count, 4)) >> 1;
+                  v_count = vec_subtract16sb(vec_zero1q(), v_count);
+                  v_count = vec_sumabsdiffs16ub(vec_zero1q(), v_count);
+                  n_best += (vec_extract8sh(v_count, 0) + vec_extract8sh(v_count, 4)) >> 1;
               }
           }else if(i == m - 1){
               const int head_mask_shift = (ceil_n - n) / block_num;
               const int tail_mask_threshold = block_num - (ceil_n - n) % block_num;
-              __m128i head_mask; head_mask = _mm_cmpeq_epi16(head_mask, head_mask);
-              if(head_mask_shift & 4){ head_mask = _mm_srli_si128(head_mask, 8); }
-              if(head_mask_shift & 2){ head_mask = _mm_srli_si128(head_mask, 4); }
-              if(head_mask_shift & 1){ head_mask = _mm_srli_si128(head_mask, 2); }
-              __m128i tail_mask = _mm_srli_si128(head_mask, 2);
+              __m128i head_mask; head_mask = vec_compareeq8sh(head_mask, head_mask);
+              if(head_mask_shift & 4){ head_mask = vec_shiftrightbytes1q(head_mask, 8); }
+              if(head_mask_shift & 2){ head_mask = vec_shiftrightbytes1q(head_mask, 4); }
+              if(head_mask_shift & 1){ head_mask = vec_shiftrightbytes1q(head_mask, 2); }
+              __m128i tail_mask = vec_shiftrightbytes1q(head_mask, 2);
               v_row_max = v_neg_inf;
               for(int j = 0; j < block_num; ++j){
-                  __m128i m = _mm_load_si128((__m128i *)(blocks[j].M));
-                  __m128i v = _mm_load_si128((__m128i *)(blocks[j].V));
-                  __m128i mx = _mm_max_epi16(m, v);
+                  __m128i m = vec_load1q((__m128i *)(blocks[j].M));
+                  __m128i v = vec_load1q((__m128i *)(blocks[j].V));
+                  __m128i mx = vec_max8sh(m, v);
                   if(j < tail_mask_threshold){
-                      mx = _mm_and_si128(mx, head_mask);
+                      mx = vec_bitand1q(mx, head_mask);
                   }else{
-                      mx = _mm_and_si128(mx, tail_mask);
+                      mx = vec_bitand1q(mx, tail_mask);
                   }
-                  v_row_max = _mm_max_epi16(v_row_max, mx);
-                  _mm_store_si128((__m128i *)(X + (j << 3)), mx);
+                  v_row_max = vec_max8sh(v_row_max, mx);
+                  vec_store1q((__m128i *)(X + (j << 3)), mx);
               }
               mm_reduction_max_epi16(opt, v_row_max);
               __m128i v_opt; mm_set1_epi16(v_opt, opt);
-              __m128i v_count = _mm_setzero_si128();
+              __m128i v_count = vec_zero1q();
               /*
               if(dir){
                   position = 0;
                   for(int j = 0; j < block_num; ++j){
-                      __m128i m = _mm_load_si128((__m128i *)(X + (j << 3)));
-                      __m128i f = _mm_cmpeq_epi16(m, v_opt);
-                      v_count = _mm_add_epi8(v_count, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i m = vec_load1q((__m128i *)(X + (j << 3)));
+                      __m128i f = vec_compareeq8sh(m, v_opt);
+                      v_count = vec_add16sb(v_count, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int k = (31 - __builtin_clz(mask)) >> 1;
                       const int p = k * block_num + j;
                       if(mask){ position = max(p, position); }
@@ -466,10 +465,10 @@ void Solution7::process_with_start_clip(
               }else{
                   position = 0x7fffffff;
                   for(int j = block_num - 1; j >= 0; --j){
-                      __m128i m = _mm_load_si128((__m128i *)(X + (j << 3)));
-                      __m128i f = _mm_cmpeq_epi16(m, v_opt);
-                      v_count = _mm_add_epi8(v_count, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i m = vec_load1q((__m128i *)(X + (j << 3)));
+                      __m128i f = vec_compareeq8sh(m, v_opt);
+                      v_count = vec_add16sb(v_count, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int k = __builtin_ctz(mask) >> 1;
                       const int p = k * block_num + j;
                       if(mask){ position = min(p, position); }
@@ -478,17 +477,17 @@ void Solution7::process_with_start_clip(
               */
               te = 0;
               for(int j = 0; j < block_num; ++j){
-                  __m128i m = _mm_load_si128((__m128i *)(X + (j << 3)));
-                  __m128i f = _mm_cmpeq_epi16(m, v_opt);
-                  v_count = _mm_add_epi8(v_count, f);
-                  const int mask = _mm_movemask_epi8(f);
+                  __m128i m = vec_load1q((__m128i *)(X + (j << 3)));
+                  __m128i f = vec_compareeq8sh(m, v_opt);
+                  v_count = vec_add16sb(v_count, f);
+                  const int mask = vec_extractupperbit16sb(f);
                   const int k = (31 - __builtin_clz(mask)) >> 1;
                   const int p = k * block_num + j;
                   if(mask){ te = max(p, te); }
               }
-              v_count = _mm_sub_epi8(_mm_setzero_si128(), v_count);
-              v_count = _mm_sad_epu8(_mm_setzero_si128(), v_count);
-              n_best += (_mm_extract_epi16(v_count, 0) + _mm_extract_epi16(v_count, 4)) >> 1;
+              v_count = vec_subtract16sb(vec_zero1q(), v_count);
+              v_count = vec_sumabsdiffs16ub(vec_zero1q(), v_count);
+              n_best += (vec_extract8sh(v_count, 0) + vec_extract8sh(v_count, 4)) >> 1;
               qe = i;
               //position |= (i << 16);
           }
@@ -548,8 +547,8 @@ void Solution7::process_without_start_clip(
   }
 
   // setup constant vector
-  __m128i v_neg_inf; v_neg_inf = _mm_cmpeq_epi16(v_neg_inf, v_neg_inf);
-  v_neg_inf = _mm_slli_epi16(v_neg_inf, 15);
+  __m128i v_neg_inf; v_neg_inf = vec_compareeq8sh(v_neg_inf, v_neg_inf);
+  v_neg_inf = vec_shiftleftimmediate8sh(v_neg_inf, 15);
   __m128i v_mi; mm_set1_epi16(v_mi, mismatch_score);
   __m128i v_ma; mm_set1_epi16(v_ma, match_score - mismatch_score);
   __m128i v_gap_open; mm_set1_epi16(v_gap_open, gap_open);
@@ -557,10 +556,10 @@ void Solution7::process_without_start_clip(
 
   // clear line buffers
   for(int i = -1; i < block_num; ++i){
-      _mm_store_si128((__m128i *)(blocks[i].NM), _mm_setzero_si128());
-      _mm_store_si128((__m128i *)(blocks[i].M), _mm_setzero_si128());
-      _mm_store_si128((__m128i *)(blocks[i].V), v_neg_inf);
-      _mm_store_si128((__m128i *)(blocks[i].H), v_neg_inf);
+      vec_store1q((__m128i *)(blocks[i].NM), vec_zero1q());
+      vec_store1q((__m128i *)(blocks[i].M), vec_zero1q());
+      vec_store1q((__m128i *)(blocks[i].V), v_neg_inf);
+      vec_store1q((__m128i *)(blocks[i].H), v_neg_inf);
   }
 
   // calculate
@@ -582,9 +581,9 @@ void Solution7::process_without_start_clip(
         ((query_begin + 7) & ~7) - simd_begin;
 
       // prefetch data
-      __m128i m2 = _mm_srli_si128(_mm_load_si128((__m128i *)(blocks[block_begin - 1].M)), 14);
-      __m128i v2 = _mm_srli_si128(_mm_load_si128((__m128i *)(blocks[block_begin - 1].V)), 14);
-      __m128i h2 = _mm_srli_si128(_mm_load_si128((__m128i *)(blocks[block_begin - 1].H)), 14);
+      __m128i m2 = vec_shiftrightbytes1q(vec_load1q((__m128i *)(blocks[block_begin - 1].M)), 14);
+      __m128i v2 = vec_shiftrightbytes1q(vec_load1q((__m128i *)(blocks[block_begin - 1].V)), 14);
+      __m128i h2 = vec_shiftrightbytes1q(vec_load1q((__m128i *)(blocks[block_begin - 1].H)), 14);
       __m128i v_row_max = v_neg_inf;
 
       // update DP table
@@ -592,44 +591,44 @@ void Solution7::process_without_start_clip(
       for(int j = block_begin; j <= block_end; ++j){
           BlockI16 *block = blocks + j;
           /* compare target and query */
-          __m128i q  = _mm_load_si128((__m128i *)(aligned_query + (j << 3)));
-          __m128i t  = _mm_load_si128((__m128i *)(block->target));
-          __m128i f  = _mm_cmpeq_epi16(q, t);
-          __m128i b  = _mm_add_epi16(_mm_and_si128(f, v_ma), v_mi);
+          __m128i q  = vec_load1q((__m128i *)(aligned_query + (j << 3)));
+          __m128i t  = vec_load1q((__m128i *)(block->target));
+          __m128i f  = vec_compareeq8sh(q, t);
+          __m128i b  = vec_add8sh(vec_bitand1q(f, v_ma), v_mi);
           /* calculate M */
-          __m128i nm = _mm_load_si128((__m128i *)(block->NM));
-          __m128i m4 = _mm_adds_epi16(nm, b);
+          __m128i nm = vec_load1q((__m128i *)(block->NM));
+          __m128i m4 = vec_addsaturating8sh(nm, b);
           /* fetch old M */
-          __m128i m3 = _mm_load_si128((__m128i *)(block->M));
+          __m128i m3 = vec_load1q((__m128i *)(block->M));
           /* write new M */
-          _mm_store_si128((__m128i *)(block->M), m4);
+          vec_store1q((__m128i *)(block->M), m4);
           /* calculate V */
-          __m128i v3 = _mm_load_si128((__m128i *)(block->V));
-          __m128i m3o = _mm_adds_epi16(m3, v_gap_open);
-          __m128i v4 = _mm_adds_epi16(_mm_max_epi16(m3o, v3), v_gap_ext);
-          _mm_store_si128((__m128i *)(block->V), v4);
+          __m128i v3 = vec_load1q((__m128i *)(block->V));
+          __m128i m3o = vec_addsaturating8sh(m3, v_gap_open);
+          __m128i v4 = vec_addsaturating8sh(vec_max8sh(m3o, v3), v_gap_ext);
+          vec_store1q((__m128i *)(block->V), v4);
           /* calculate H */
-          __m128i h3 = _mm_load_si128((__m128i *)(block->H));
-          m2 = _mm_or_si128(m2, _mm_slli_si128(m3, 2));
-          h2 = _mm_or_si128(h2, _mm_slli_si128(h3, 2));
-          __m128i m2o = _mm_adds_epi16(m2, v_gap_open);
-          __m128i h4 = _mm_adds_epi16(_mm_max_epi16(m2o, h2), v_gap_ext);
-          _mm_store_si128((__m128i *)(block->H), h4);
+          __m128i h3 = vec_load1q((__m128i *)(block->H));
+          m2 = vec_bitor1q(m2, vec_shiftleftbytes1q(m3, 2));
+          h2 = vec_bitor1q(h2, vec_shiftleftbytes1q(h3, 2));
+          __m128i m2o = vec_addsaturating8sh(m2, v_gap_open);
+          __m128i h4 = vec_addsaturating8sh(vec_max8sh(m2o, h2), v_gap_ext);
+          vec_store1q((__m128i *)(block->H), h4);
           /* calculate NM */
-          v2 = _mm_or_si128(v2, _mm_slli_si128(v3, 2));
-          __m128i next_nm = _mm_max_epi16(_mm_max_epi16(h2, v2), m2);
-          _mm_store_si128((__m128i *)(block->NM), next_nm);
+          v2 = vec_bitor1q(v2, vec_shiftleftbytes1q(v3, 2));
+          __m128i next_nm = vec_max8sh(vec_max8sh(h2, v2), m2);
+          vec_store1q((__m128i *)(block->NM), next_nm);
           /* rotate m2, v2, h2 */
-          m2 = _mm_srli_si128(m3, 14);
-          v2 = _mm_srli_si128(v3, 14);
-          h2 = _mm_srli_si128(h3, 14);
+          m2 = vec_shiftrightbytes1q(m3, 14);
+          v2 = vec_shiftrightbytes1q(v3, 14);
+          h2 = vec_shiftrightbytes1q(h3, 14);
           /* calculate X */
           if(j == block_begin){
-              __m128i x4 = _mm_max_epi16(_mm_max_epi16(m4, v4), h4);
+              __m128i x4 = vec_max8sh(vec_max8sh(m4, v4), h4);
               v_row_max = x4;
-              _mm_store_si128((__m128i *)first_block_maxval, x4);
+              vec_store1q((__m128i *)first_block_maxval, x4);
           }else if(query_end_clip){
-              v_row_max = _mm_max_epi16(v_row_max, m4);
+              v_row_max = vec_max8sh(v_row_max, m4);
           }
       }
 
@@ -649,21 +648,21 @@ void Solution7::process_without_start_clip(
                   n_best = 0;
               }
               mm_set1_epi16(v_row_max, opt);
-              __m128i v_best_num = _mm_setzero_si128();
+              __m128i v_best_num = vec_zero1q();
               __m128i temporary_m0;
               if(block_begin == 0){
-                  temporary_m0 = _mm_load_si128((__m128i *)(blocks[0].M));
-                  __m128i first_block = _mm_load_si128((__m128i *)(first_block_maxval));
-                  _mm_store_si128((__m128i *)(blocks[0].M), first_block);
+                  temporary_m0 = vec_load1q((__m128i *)(blocks[0].M));
+                  __m128i first_block = vec_load1q((__m128i *)(first_block_maxval));
+                  vec_store1q((__m128i *)(blocks[0].M), first_block);
               }
               /*
               if(dir){
                   int local_position = 0;
                   for(int j = block_end; j >= block_begin; --j){
-                      __m128i x = _mm_load_si128((__m128i *)(blocks[j].M));
-                      __m128i f = _mm_cmpeq_epi16(v_row_max, x);
-                      v_best_num = _mm_add_epi8(v_best_num, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i x = vec_load1q((__m128i *)(blocks[j].M));
+                      __m128i f = vec_compareeq8sh(v_row_max, x);
+                      v_best_num = vec_add16sb(v_best_num, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int p = (j << 3) + (__builtin_ctz(mask) >> 1);
                       const int pos = ((i - p) << 16) | p;
                       local_position = mask ? pos : local_position;
@@ -672,10 +671,10 @@ void Solution7::process_without_start_clip(
               }else{
                   int local_position = 0;
                   for(int j = block_begin; j <= block_end; ++j){
-                      __m128i x = _mm_load_si128((__m128i *)(blocks[j].M));
-                      __m128i f = _mm_cmpeq_epi16(v_row_max, x);
-                      v_best_num = _mm_add_epi8(v_best_num, f);
-                      const int mask = _mm_movemask_epi8(f);
+                      __m128i x = vec_load1q((__m128i *)(blocks[j].M));
+                      __m128i f = vec_compareeq8sh(v_row_max, x);
+                      v_best_num = vec_add16sb(v_best_num, f);
+                      const int mask = vec_extractupperbit16sb(f);
                       const int p = (j << 3) + ((31 - __builtin_clz(mask)) >> 1);
                       const int pos = ((i - p) << 16) | p;
                       local_position = mask ? pos : local_position;
@@ -685,20 +684,20 @@ void Solution7::process_without_start_clip(
               */
               int local_position = 0;
               for(int j = block_end; j >= block_begin; --j){
-                  __m128i x = _mm_load_si128((__m128i *)(blocks[j].M));
-                  __m128i f = _mm_cmpeq_epi16(v_row_max, x);
-                  v_best_num = _mm_add_epi8(v_best_num, f);
-                  const int mask = _mm_movemask_epi8(f);
+                  __m128i x = vec_load1q((__m128i *)(blocks[j].M));
+                  __m128i f = vec_compareeq8sh(v_row_max, x);
+                  v_best_num = vec_add16sb(v_best_num, f);
+                  const int mask = vec_extractupperbit16sb(f);
                   const int p = (j << 3) + (__builtin_ctz(mask) >> 1);
                   const int pos = ((i - p) << 16) | p;
                   local_position = mask ? pos : local_position;
               }
               te = max(te, local_position);
-              __m128i m = _mm_sub_epi8(_mm_setzero_si128(), v_best_num);
-              __m128i sad = _mm_sad_epu8(_mm_setzero_si128(), m);
-              n_best += (_mm_extract_epi16(sad, 0) + _mm_extract_epi16(sad, 4)) >> 1;
+              __m128i m = vec_subtract16sb(vec_zero1q(), v_best_num);
+              __m128i sad = vec_sumabsdiffs16ub(vec_zero1q(), m);
+              n_best += (vec_extract8sh(sad, 0) + vec_extract8sh(sad, 4)) >> 1;
               if(block_begin == 0){
-                  _mm_store_si128((__m128i *)(blocks[0].M), temporary_m0);
+                  vec_store1q((__m128i *)(blocks[0].M), temporary_m0);
               }
           }
       }else{
@@ -711,44 +710,44 @@ void Solution7::process_without_start_clip(
       __m128i v_opt = v_neg_inf;
       for(int i = n; i & 7; ++i){ X[i] = NEG_INF; }
       for(int i = block_begin; i < block_end; ++i){
-          __m128i x = _mm_load_si128((__m128i *)(X + (i << 3)));
-          v_opt = _mm_max_epi16(v_opt, x);
+          __m128i x = vec_load1q((__m128i *)(X + (i << 3)));
+          v_opt = vec_max8sh(v_opt, x);
       }
       mm_reduction_max_epi16(opt, v_opt);
       mm_set1_epi16(v_opt, opt);
-      __m128i v_count = _mm_setzero_si128();
+      __m128i v_count = vec_zero1q();
       /*
       if(dir){
           for(int i = block_begin; i < block_end; ++i){
-              __m128i x = _mm_load_si128((__m128i *)(X + (i << 3)));
-              __m128i f = _mm_cmpeq_epi16(x, v_opt);
-              v_count = _mm_add_epi8(v_count, f);
-              const int mask = _mm_movemask_epi8(f);
+              __m128i x = vec_load1q((__m128i *)(X + (i << 3)));
+              __m128i f = vec_compareeq8sh(x, v_opt);
+              v_count = vec_add16sb(v_count, f);
+              const int mask = vec_extractupperbit16sb(f);
               const int k = (31 - __builtin_clz(mask)) >> 1;
               if(mask){ position = (i << 3) + k; }
           }
       }else{
           for(int i = block_end - 1; i >= 0; --i){
-              __m128i x = _mm_load_si128((__m128i *)(X + (i << 3)));
-              __m128i f = _mm_cmpeq_epi16(x, v_opt);
-              v_count = _mm_add_epi8(v_count, f);
-              const int mask = _mm_movemask_epi8(f);
+              __m128i x = vec_load1q((__m128i *)(X + (i << 3)));
+              __m128i f = vec_compareeq8sh(x, v_opt);
+              v_count = vec_add16sb(v_count, f);
+              const int mask = vec_extractupperbit16sb(f);
               const int k = __builtin_ctz(mask) >> 1;
               if(mask){ position = (i << 3) + k; }
           }
       }
       */
       for(int i = block_begin; i < block_end; ++i){
-          __m128i x = _mm_load_si128((__m128i *)(X + (i << 3)));
-          __m128i f = _mm_cmpeq_epi16(x, v_opt);
-          v_count = _mm_add_epi8(v_count, f);
-          const int mask = _mm_movemask_epi8(f);
+          __m128i x = vec_load1q((__m128i *)(X + (i << 3)));
+          __m128i f = vec_compareeq8sh(x, v_opt);
+          v_count = vec_add16sb(v_count, f);
+          const int mask = vec_extractupperbit16sb(f);
           const int k = (31 - __builtin_clz(mask)) >> 1;
           if(mask){ te = (i << 3) + k; }
       }
-      v_count = _mm_sub_epi8(_mm_setzero_si128(), v_count);
-      v_count = _mm_sad_epu8(_mm_setzero_si128(), v_count);
-      n_best = (_mm_extract_epi16(v_count, 0) + _mm_extract_epi16(v_count, 4)) >> 1;
+      v_count = vec_subtract16sb(vec_zero1q(), v_count);
+      v_count = vec_sumabsdiffs16ub(vec_zero1q(), v_count);
+      n_best = (vec_extract8sh(v_count, 0) + vec_extract8sh(v_count, 4)) >> 1;
       //position |= (m - 1) << 16;
       qe = (m - 1) << 16;
   }
--- src/sw/lib/Solution7.h
+++ src/sw/lib/Solution7.h
@@ -14,35 +14,34 @@
 #include <cstdio>
 #include <cstdlib>
 #include <cstring>
-#include <xmmintrin.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include "Solution.h"
 
 using namespace std;
 
 #define mm_set1_epi8(x, a) { \
-        x = _mm_insert_epi16(x, a, 0); \
-        x = _mm_unpacklo_epi8(x, x); \
-        x = _mm_shufflelo_epi16(x, 0); \
-        x = _mm_shuffle_epi32(x, 0); \
+        x = vec_insert8sh(x, a, 0); \
+        x = vec_unpacklow88sb(x, x); \
+        x = vec_permutelower4sh(x, 0); \
+        x = vec_permute4sw(x, 0); \
     }
 #define mm_set1_epi16(x, a) { \
-        x = _mm_insert_epi16(x, a, 0); \
-        x = _mm_shufflelo_epi16(x, 0); \
-        x = _mm_shuffle_epi32(x, 0); \
+        x = vec_insert8sh(x, a, 0); \
+        x = vec_permutelower4sh(x, 0); \
+        x = vec_permute4sw(x, 0); \
     }
 #define mm_reduction_max_epu8(dst, src) { \
-        __m128i tmp = _mm_max_epu8(_mm_srli_si128(src, 1), src); \
-        tmp = _mm_max_epu8(_mm_srli_si128(tmp, 2), tmp); \
-        tmp = _mm_max_epu8(_mm_srli_si128(tmp, 4), tmp); \
-        tmp = _mm_max_epu8(_mm_srli_si128(tmp, 8), tmp); \
-        dst = static_cast<uint8_t>(_mm_extract_epi16(tmp, 0) & 0xff); \
+        __m128i tmp = vec_max16ub(vec_shiftrightbytes1q(src, 1), src); \
+        tmp = vec_max16ub(vec_shiftrightbytes1q(tmp, 2), tmp); \
+        tmp = vec_max16ub(vec_shiftrightbytes1q(tmp, 4), tmp); \
+        tmp = vec_max16ub(vec_shiftrightbytes1q(tmp, 8), tmp); \
+        dst = static_cast<uint8_t>(vec_extract8sh(tmp, 0) & 0xff); \
     }
 #define mm_reduction_max_epi16(dst, src) { \
-        __m128i tmp = _mm_max_epi16(_mm_srli_si128(src, 2), src); \
-        tmp = _mm_max_epi16(_mm_srli_si128(tmp, 4), tmp); \
-        tmp = _mm_max_epi16(_mm_srli_si128(tmp, 8), tmp); \
-        dst = static_cast<int16_t>(_mm_extract_epi16(tmp, 0)); \
+        __m128i tmp = vec_max8sh(vec_shiftrightbytes1q(src, 2), src); \
+        tmp = vec_max8sh(vec_shiftrightbytes1q(tmp, 4), tmp); \
+        tmp = vec_max8sh(vec_shiftrightbytes1q(tmp, 8), tmp); \
+        dst = static_cast<int16_t>(vec_extract8sh(tmp, 0)); \
     }
 
 class Solution7 : public Solution {
--- src/sw/lib/Solution8.cpp
+++ src/sw/lib/Solution8.cpp
@@ -29,13 +29,12 @@
 #include <ext/hash_set>
 #include <ext/hash_map>
 #include <ext/pool_allocator.h>
-#include <emmintrin.h>
-#include <pmmintrin.h>
+#include <vec128int.h>
 #include <stdint.h>
 #include <limits>
 #include "Solution8.h"
 
-#define _mm_loadu_si128 _mm_lddqu_si128
+#define vec_load1qu vec_load1qu
 //#define volatile
 
 using namespace std;
@@ -60,14 +59,14 @@ typedef uint8_t u8;
 
 static inline ostream& operator<<(ostream& out, const __m128i& v)
 {
-  out << "( " << _mm_extract_epi16(v, 0)
-    << ", " << _mm_extract_epi16(v, 1)
-    << ", " << _mm_extract_epi16(v, 2)
-    << ", " << _mm_extract_epi16(v, 3)
-    << ", " << _mm_extract_epi16(v, 4)
-    << ", " << _mm_extract_epi16(v, 5)
-    << ", " << _mm_extract_epi16(v, 6)
-    << ", " << _mm_extract_epi16(v, 7)
+  out << "( " << vec_extract8sh(v, 0)
+    << ", " << vec_extract8sh(v, 1)
+    << ", " << vec_extract8sh(v, 2)
+    << ", " << vec_extract8sh(v, 3)
+    << ", " << vec_extract8sh(v, 4)
+    << ", " << vec_extract8sh(v, 5)
+    << ", " << vec_extract8sh(v, 6)
+    << ", " << vec_extract8sh(v, 7)
     << " )";
   return out;
 }
@@ -237,445 +236,445 @@ void process(size_t n, size_t m)
   int o = go, e = ge, oe = o+e;
 
   if ( 0 ) {
-      __m128i minf = _mm_set1_epi16(-INF);
+      __m128i minf = vec_splat8sh(-INF);
       if ( qec ) {
           for ( size_t j = 0; j < n; j += 8 ) {
-              _mm_store_si128((__m128i*)(bv+j), minf);
-              _mm_store_si128((__m128i*)(bn+j), minf);
-              _mm_store_si128((__m128i*)(br+j), minf);
+              vec_store1q((__m128i*)(bv+j), minf);
+              vec_store1q((__m128i*)(bn+j), minf);
+              vec_store1q((__m128i*)(br+j), minf);
           }
       }
   }
 
 #define STEPqec0()                                                      \
     {                                                                   \
-      __m128i M = _mm_loadu_si128((__m128i*)(ar-(r-2)+j));            \
+      __m128i M = vec_load1qu((__m128i*)(ar-(r-2)+j));            \
       __m128i wb = *((volatile __m128i*)(bf+j));                      \
       __m128i m_ma_over_mi = *((volatile __m128i*)&SNS::m_ma_over_mi); \
       __m128i m_mi = *((volatile __m128i*)&SNS::m_mi);                \
-      __m128i Rp = _mm_loadu_si128((__m128i*)(Rr+j-1));               \
-      __m128i H = _mm_loadu_si128((__m128i*)(Hr+j-1));                \
-      M = _mm_cmpeq_epi16(M, wb);                                     \
-      M = _mm_and_si128(M, m_ma_over_mi);                             \
-      M = _mm_add_epi16(M, m_mi);                                     \
-      M = _mm_add_epi16(M, Rp);                                       \
-      if ( qsc ) M = _mm_max_epi16(M, _mm_setzero_si128());           \
+      __m128i Rp = vec_load1qu((__m128i*)(Rr+j-1));               \
+      __m128i H = vec_load1qu((__m128i*)(Hr+j-1));                \
+      M = vec_compareeq8sh(M, wb);                                     \
+      M = vec_bitand1q(M, m_ma_over_mi);                             \
+      M = vec_add8sh(M, m_mi);                                     \
+      M = vec_add8sh(M, Rp);                                       \
+      if ( qsc ) M = vec_max8sh(M, vec_zero1q());           \
       __m128i R = *((volatile __m128i*)(Vr+j));                       \
       __m128i V = *((volatile __m128i*)(Vr+j));                       \
       __m128i m_oe = *((volatile __m128i*)&SNS::m_oe);                \
       __m128i m_e = *((volatile __m128i*)&SNS::m_e);                  \
-      R = _mm_max_epi16(R, H);                                        \
-      R = _mm_max_epi16(R, M);                                        \
-      _mm_store_si128((__m128i*)(Rr+j), R);                           \
-      M = _mm_add_epi16(M, m_oe);                                     \
-      V = _mm_add_epi16(V, m_e);                                      \
-      H = _mm_add_epi16(H, m_e);                                      \
-      V = _mm_max_epi16(V, M);                                        \
-      H = _mm_max_epi16(H, M);                                        \
-      _mm_store_si128((__m128i*)(Vr+j), V);                           \
-      _mm_store_si128((__m128i*)(Hr+j), H);                           \
+      R = vec_max8sh(R, H);                                        \
+      R = vec_max8sh(R, M);                                        \
+      vec_store1q((__m128i*)(Rr+j), R);                           \
+      M = vec_add8sh(M, m_oe);                                     \
+      V = vec_add8sh(V, m_e);                                      \
+      H = vec_add8sh(H, m_e);                                      \
+      V = vec_max8sh(V, M);                                        \
+      H = vec_max8sh(H, M);                                        \
+      vec_store1q((__m128i*)(Vr+j), V);                           \
+      vec_store1q((__m128i*)(Hr+j), H);                           \
     }
 #define STEPqec0p()                                                     \
     {                                                                   \
-      __m128i M = _mm_loadu_si128(arp);                               \
+      __m128i M = vec_load1qu(arp);                               \
       __m128i wb = *((volatile __m128i*)(Hrp+TO*4));                  \
       __m128i m_ma_over_mi = *((volatile __m128i*)&SNS::m_ma_over_mi); \
       __m128i m_mi = *((volatile __m128i*)&SNS::m_mi);                \
-      __m128i Rp = _mm_loadu_si128((__m128i*)((i16*)Rrp-1));          \
-      __m128i H = _mm_loadu_si128((__m128i*)((i16*)Hrp-1));           \
-      M = _mm_cmpeq_epi16(M, wb);                                     \
-      M = _mm_and_si128(M, m_ma_over_mi);                             \
-      M = _mm_add_epi16(M, m_mi);                                     \
-      M = _mm_add_epi16(M, Rp);                                       \
-      if ( qsc ) M = _mm_max_epi16(M, _mm_setzero_si128());           \
+      __m128i Rp = vec_load1qu((__m128i*)((i16*)Rrp-1));          \
+      __m128i H = vec_load1qu((__m128i*)((i16*)Hrp-1));           \
+      M = vec_compareeq8sh(M, wb);                                     \
+      M = vec_bitand1q(M, m_ma_over_mi);                             \
+      M = vec_add8sh(M, m_mi);                                     \
+      M = vec_add8sh(M, Rp);                                       \
+      if ( qsc ) M = vec_max8sh(M, vec_zero1q());           \
       __m128i R = *((volatile __m128i*)(Hrp+TO));                     \
       __m128i V = *((volatile __m128i*)(Hrp+TO));                     \
       __m128i m_oe = *((volatile __m128i*)&SNS::m_oe);                \
       __m128i m_e = *((volatile __m128i*)&SNS::m_e);                  \
-      R = _mm_max_epi16(R, H);                                        \
-      R = _mm_max_epi16(R, M);                                        \
-      _mm_store_si128(Rrp, R);                                        \
-      M = _mm_add_epi16(M, m_oe);                                     \
-      V = _mm_add_epi16(V, m_e);                                      \
-      H = _mm_add_epi16(H, m_e);                                      \
-      V = _mm_max_epi16(V, M);                                        \
-      H = _mm_max_epi16(H, M);                                        \
-      _mm_store_si128(Hrp+TO, V);                                     \
-      _mm_store_si128(Hrp, H);                                        \
+      R = vec_max8sh(R, H);                                        \
+      R = vec_max8sh(R, M);                                        \
+      vec_store1q(Rrp, R);                                        \
+      M = vec_add8sh(M, m_oe);                                     \
+      V = vec_add8sh(V, m_e);                                      \
+      H = vec_add8sh(H, m_e);                                      \
+      V = vec_max8sh(V, M);                                        \
+      H = vec_max8sh(H, M);                                        \
+      vec_store1q(Hrp+TO, V);                                     \
+      vec_store1q(Hrp, H);                                        \
     }
 #define STEPqec0p2()                                                    \
     {                                                                   \
-      __m128i M1 = _mm_loadu_si128(arp-1);                            \
+      __m128i M1 = vec_load1qu(arp-1);                            \
       __m128i wb1 = *((volatile __m128i*)(Hrp+TO*4-1));               \
-      __m128i M2 = _mm_loadu_si128(arp  );                            \
+      __m128i M2 = vec_load1qu(arp  );                            \
       __m128i wb2 = *((volatile __m128i*)(Hrp+TO*4  ));               \
       __m128i m_ma_over_mi = *((volatile __m128i*)&SNS::m_ma_over_mi); \
       __m128i m_mi = *((volatile __m128i*)&SNS::m_mi);                \
-      M1 = _mm_cmpeq_epi16(M1, wb1);                                  \
-      M2 = _mm_cmpeq_epi16(M2, wb2);                                  \
-      __m128i Rp1 = _mm_loadu_si128((__m128i*)((i16*)Rrp-9));         \
-      __m128i Rp2 = _mm_loadu_si128((__m128i*)((i16*)Rrp-1));         \
-      M1 = _mm_and_si128(M1, m_ma_over_mi);                           \
-      M2 = _mm_and_si128(M2, m_ma_over_mi);                           \
-      M1 = _mm_add_epi16(M1, m_mi);                                   \
-      M2 = _mm_add_epi16(M2, m_mi);                                   \
-      M1 = _mm_add_epi16(M1, Rp1);                                    \
-      M2 = _mm_add_epi16(M2, Rp2);                                    \
-      __m128i H1 = _mm_loadu_si128((__m128i*)((i16*)Hrp-9));          \
-      __m128i H2 = _mm_loadu_si128((__m128i*)((i16*)Hrp-1));          \
+      M1 = vec_compareeq8sh(M1, wb1);                                  \
+      M2 = vec_compareeq8sh(M2, wb2);                                  \
+      __m128i Rp1 = vec_load1qu((__m128i*)((i16*)Rrp-9));         \
+      __m128i Rp2 = vec_load1qu((__m128i*)((i16*)Rrp-1));         \
+      M1 = vec_bitand1q(M1, m_ma_over_mi);                           \
+      M2 = vec_bitand1q(M2, m_ma_over_mi);                           \
+      M1 = vec_add8sh(M1, m_mi);                                   \
+      M2 = vec_add8sh(M2, m_mi);                                   \
+      M1 = vec_add8sh(M1, Rp1);                                    \
+      M2 = vec_add8sh(M2, Rp2);                                    \
+      __m128i H1 = vec_load1qu((__m128i*)((i16*)Hrp-9));          \
+      __m128i H2 = vec_load1qu((__m128i*)((i16*)Hrp-1));          \
       if ( qsc ) {                                                    \
-          __m128i zero = _mm_setzero_si128();                         \
-          M1 = _mm_max_epi16(M1, zero);                               \
-          M2 = _mm_max_epi16(M2, zero);                               \
+          __m128i zero = vec_zero1q();                         \
+          M1 = vec_max8sh(M1, zero);                               \
+          M2 = vec_max8sh(M2, zero);                               \
       }                                                               \
       __m128i R1 = *((volatile __m128i*)(Hrp+TO-1));                  \
       __m128i R2 = *((volatile __m128i*)(Hrp+TO  ));                  \
-      R1 = _mm_max_epi16(R1, H1);                                     \
-      R2 = _mm_max_epi16(R2, H2);                                     \
-      R1 = _mm_max_epi16(R1, M1);                                     \
-      R2 = _mm_max_epi16(R2, M2);                                     \
-      _mm_store_si128(Rrp-1, R1);                                     \
-      _mm_store_si128(Rrp  , R2);                                     \
+      R1 = vec_max8sh(R1, H1);                                     \
+      R2 = vec_max8sh(R2, H2);                                     \
+      R1 = vec_max8sh(R1, M1);                                     \
+      R2 = vec_max8sh(R2, M2);                                     \
+      vec_store1q(Rrp-1, R1);                                     \
+      vec_store1q(Rrp  , R2);                                     \
       __m128i m_oe = *((volatile __m128i*)&SNS::m_oe);                \
       __m128i m_e = *((volatile __m128i*)&SNS::m_e);                  \
       __m128i V1 = *((volatile __m128i*)(Hrp+TO-1));                  \
       __m128i V2 = *((volatile __m128i*)(Hrp+TO  ));                  \
-      M1 = _mm_add_epi16(M1, m_oe);                                   \
-      M2 = _mm_add_epi16(M2, m_oe);                                   \
-      H1 = _mm_add_epi16(H1, m_e);                                    \
-      H2 = _mm_add_epi16(H2, m_e);                                    \
-      V1 = _mm_add_epi16(V1, m_e);                                    \
-      V2 = _mm_add_epi16(V2, m_e);                                    \
-      H1 = _mm_max_epi16(H1, M1);                                     \
-      H2 = _mm_max_epi16(H2, M2);                                     \
-      V1 = _mm_max_epi16(V1, M1);                                     \
-      V2 = _mm_max_epi16(V2, M2);                                     \
-      _mm_store_si128(Hrp-1, H1);                                     \
-      _mm_store_si128(Hrp  , H2);                                     \
-      _mm_store_si128(Hrp+TO-1, V1);                                  \
-      _mm_store_si128(Hrp+TO  , V2);                                  \
+      M1 = vec_add8sh(M1, m_oe);                                   \
+      M2 = vec_add8sh(M2, m_oe);                                   \
+      H1 = vec_add8sh(H1, m_e);                                    \
+      H2 = vec_add8sh(H2, m_e);                                    \
+      V1 = vec_add8sh(V1, m_e);                                    \
+      V2 = vec_add8sh(V2, m_e);                                    \
+      H1 = vec_max8sh(H1, M1);                                     \
+      H2 = vec_max8sh(H2, M2);                                     \
+      V1 = vec_max8sh(V1, M1);                                     \
+      V2 = vec_max8sh(V2, M2);                                     \
+      vec_store1q(Hrp-1, H1);                                     \
+      vec_store1q(Hrp  , H2);                                     \
+      vec_store1q(Hrp+TO-1, V1);                                  \
+      vec_store1q(Hrp+TO  , V2);                                  \
     }
 #define STEPqec1p()                                                     \
     {                                                                   \
-      __m128i M = _mm_loadu_si128(arp);                               \
+      __m128i M = vec_load1qu(arp);                               \
       __m128i wb = *((volatile __m128i*)(Hrp+TO*4));                  \
       __m128i m_ma_over_mi = *((volatile __m128i*)&SNS::m_ma_over_mi); \
       __m128i m_mi = *((volatile __m128i*)&SNS::m_mi);                \
-      __m128i Rp = _mm_loadu_si128((__m128i*)((i16*)Rrp-1));          \
-      __m128i H = _mm_loadu_si128((__m128i*)((i16*)Hrp-1));           \
-      M = _mm_cmpeq_epi16(M, wb);                                     \
-      M = _mm_and_si128(M, m_ma_over_mi);                             \
-      M = _mm_add_epi16(M, m_mi);                                     \
-      M = _mm_add_epi16(M, Rp);                                       \
-      if ( qsc ) M = _mm_max_epi16(M, _mm_setzero_si128());           \
+      __m128i Rp = vec_load1qu((__m128i*)((i16*)Rrp-1));          \
+      __m128i H = vec_load1qu((__m128i*)((i16*)Hrp-1));           \
+      M = vec_compareeq8sh(M, wb);                                     \
+      M = vec_bitand1q(M, m_ma_over_mi);                             \
+      M = vec_add8sh(M, m_mi);                                     \
+      M = vec_add8sh(M, Rp);                                       \
+      if ( qsc ) M = vec_max8sh(M, vec_zero1q());           \
       __m128i R = *((volatile __m128i*)(Hrp+TO));                     \
       __m128i V = *((volatile __m128i*)(Hrp+TO));                     \
       __m128i m_oe = *((volatile __m128i*)&SNS::m_oe);                \
       __m128i m_e = *((volatile __m128i*)&SNS::m_e);                  \
-      R = _mm_max_epi16(R, H);                                        \
-      R = _mm_max_epi16(R, M);                                        \
-      _mm_store_si128(Rrp, R);                                        \
-      M = _mm_add_epi16(M, m_oe);                                     \
-      V = _mm_add_epi16(V, m_e);                                      \
-      H = _mm_add_epi16(H, m_e);                                      \
-      V = _mm_max_epi16(V, M);                                        \
-      H = _mm_max_epi16(H, M);                                        \
-      _mm_store_si128(Hrp+TO, V);                                     \
-      _mm_store_si128(Hrp, H);                                        \
+      R = vec_max8sh(R, H);                                        \
+      R = vec_max8sh(R, M);                                        \
+      vec_store1q(Rrp, R);                                        \
+      M = vec_add8sh(M, m_oe);                                     \
+      V = vec_add8sh(V, m_e);                                      \
+      H = vec_add8sh(H, m_e);                                      \
+      V = vec_max8sh(V, M);                                        \
+      H = vec_max8sh(H, M);                                        \
+      vec_store1q(Hrp+TO, V);                                     \
+      vec_store1q(Hrp, H);                                        \
       __m128i n = R;                                                  \
       __m128i vc = R;                                                 \
-      __m128i v = _mm_load_si128(bvp);                                \
-      __m128i rp = _mm_load_si128(bvp+TO*2);                          \
-      __m128i np = _mm_load_si128(bvp+TO);                            \
+      __m128i v = vec_load1q(bvp);                                \
+      __m128i rp = vec_load1q(bvp+TO*2);                          \
+      __m128i np = vec_load1q(bvp+TO);                            \
       if ( dir ) {                                                    \
-          n = _mm_cmpgt_epi16(n, v);                                  \
-          v = _mm_max_epi16(v, vc);                                   \
-          _mm_store_si128(bvp, v);                                    \
-          vc = _mm_cmpeq_epi16(vc, v);                                \
-          n = _mm_andnot_si128(n, np);                                \
-          n = _mm_sub_epi16(n, vc);                                   \
-          _mm_store_si128(bvp+TO, n);                                 \
-          vc = _mm_and_si128(vc, r4);                                 \
-          rp = _mm_max_epi16(rp, vc);                                 \
-          _mm_store_si128(bvp+TO*2, rp);                              \
+          n = vec_comparegt8sh(n, v);                                  \
+          v = vec_max8sh(v, vc);                                   \
+          vec_store1q(bvp, v);                                    \
+          vc = vec_compareeq8sh(vc, v);                                \
+          n = vec_bitandnotleft1q(n, np);                                \
+          n = vec_subtract8sh(n, vc);                                   \
+          vec_store1q(bvp+TO, n);                                 \
+          vc = vec_bitand1q(vc, r4);                                 \
+          rp = vec_max8sh(rp, vc);                                 \
+          vec_store1q(bvp+TO*2, rp);                              \
       }                                                               \
       else {                                                          \
-          n = _mm_cmpgt_epi16(n, v);                                  \
-          v = _mm_max_epi16(v, vc);                                   \
-          _mm_store_si128(bvp, v);                                    \
-          vc = _mm_cmpeq_epi16(vc, v);                                \
-          rp = _mm_or_si128(rp, n);                                   \
-          n = _mm_andnot_si128(n, np);                                \
-          n = _mm_sub_epi16(n, vc);                                   \
-          _mm_store_si128(bvp+TO, n);                                 \
-          rp = _mm_max_epi16(rp, r4);                                 \
-          _mm_store_si128(bvp+TO*2, rp);                              \
+          n = vec_comparegt8sh(n, v);                                  \
+          v = vec_max8sh(v, vc);                                   \
+          vec_store1q(bvp, v);                                    \
+          vc = vec_compareeq8sh(vc, v);                                \
+          rp = vec_bitor1q(rp, n);                                   \
+          n = vec_bitandnotleft1q(n, np);                                \
+          n = vec_subtract8sh(n, vc);                                   \
+          vec_store1q(bvp+TO, n);                                 \
+          rp = vec_max8sh(rp, r4);                                 \
+          vec_store1q(bvp+TO*2, rp);                              \
       }                                                               \
     }
 #define STEPqec1p2()                                                    \
     {                                                                   \
-      __m128i M1 = _mm_loadu_si128(arp);                              \
-      __m128i M2 = _mm_loadu_si128(arp+1);                            \
+      __m128i M1 = vec_load1qu(arp);                              \
+      __m128i M2 = vec_load1qu(arp+1);                            \
       __m128i wb1 = *((volatile __m128i*)(Hrp+TO*4));                 \
       __m128i wb2 = *((volatile __m128i*)(Hrp+TO*4+1));               \
       __m128i m_ma_over_mi = *((volatile __m128i*)&SNS::m_ma_over_mi); \
       __m128i m_mi = *((volatile __m128i*)&SNS::m_mi);                \
-      M1 = _mm_cmpeq_epi16(M1, wb1);                                  \
-      M2 = _mm_cmpeq_epi16(M2, wb2);                                  \
-      __m128i Rp1 = _mm_loadu_si128((__m128i*)((i16*)Rrp-1));         \
-      __m128i Rp2 = _mm_loadu_si128((__m128i*)((i16*)Rrp+7));         \
-      M1 = _mm_and_si128(M1, m_ma_over_mi);                           \
-      M2 = _mm_and_si128(M2, m_ma_over_mi);                           \
-      M1 = _mm_add_epi16(M1, m_mi);                                   \
-      M2 = _mm_add_epi16(M2, m_mi);                                   \
-      M1 = _mm_add_epi16(M1, Rp1);                                    \
-      M2 = _mm_add_epi16(M2, Rp2);                                    \
-      __m128i H1 = _mm_loadu_si128((__m128i*)((i16*)Hrp-1));          \
-      __m128i H2 = _mm_loadu_si128((__m128i*)((i16*)Hrp+7));          \
+      M1 = vec_compareeq8sh(M1, wb1);                                  \
+      M2 = vec_compareeq8sh(M2, wb2);                                  \
+      __m128i Rp1 = vec_load1qu((__m128i*)((i16*)Rrp-1));         \
+      __m128i Rp2 = vec_load1qu((__m128i*)((i16*)Rrp+7));         \
+      M1 = vec_bitand1q(M1, m_ma_over_mi);                           \
+      M2 = vec_bitand1q(M2, m_ma_over_mi);                           \
+      M1 = vec_add8sh(M1, m_mi);                                   \
+      M2 = vec_add8sh(M2, m_mi);                                   \
+      M1 = vec_add8sh(M1, Rp1);                                    \
+      M2 = vec_add8sh(M2, Rp2);                                    \
+      __m128i H1 = vec_load1qu((__m128i*)((i16*)Hrp-1));          \
+      __m128i H2 = vec_load1qu((__m128i*)((i16*)Hrp+7));          \
       if ( qsc ) {                                                    \
-          __m128i zero = _mm_setzero_si128();                         \
-          M1 = _mm_max_epi16(M1, zero);                               \
-          M2 = _mm_max_epi16(M2, zero);                               \
+          __m128i zero = vec_zero1q();                         \
+          M1 = vec_max8sh(M1, zero);                               \
+          M2 = vec_max8sh(M2, zero);                               \
       }                                                               \
       __m128i R1 = *((volatile __m128i*)(Hrp+TO));                    \
       __m128i R2 = *((volatile __m128i*)(Hrp+TO+1));                  \
-      R1 = _mm_max_epi16(R1, H1);                                     \
-      R2 = _mm_max_epi16(R2, H2);                                     \
-      R1 = _mm_max_epi16(R1, M1);                                     \
-      R2 = _mm_max_epi16(R2, M2);                                     \
-      _mm_store_si128(Rrp, R1);                                       \
-      _mm_store_si128(Rrp+1, R2);                                     \
+      R1 = vec_max8sh(R1, H1);                                     \
+      R2 = vec_max8sh(R2, H2);                                     \
+      R1 = vec_max8sh(R1, M1);                                     \
+      R2 = vec_max8sh(R2, M2);                                     \
+      vec_store1q(Rrp, R1);                                       \
+      vec_store1q(Rrp+1, R2);                                     \
       __m128i m_oe = *((volatile __m128i*)&SNS::m_oe);                \
       __m128i m_e = *((volatile __m128i*)&SNS::m_e);                  \
       __m128i V1 = *((volatile __m128i*)(Hrp+TO));                    \
       __m128i V2 = *((volatile __m128i*)(Hrp+TO+1));                  \
-      M1 = _mm_add_epi16(M1, m_oe);                                   \
-      M2 = _mm_add_epi16(M2, m_oe);                                   \
-      H1 = _mm_add_epi16(H1, m_e);                                    \
-      H2 = _mm_add_epi16(H2, m_e);                                    \
-      V1 = _mm_add_epi16(V1, m_e);                                    \
-      V2 = _mm_add_epi16(V2, m_e);                                    \
-      H1 = _mm_max_epi16(H1, M1);                                     \
-      H2 = _mm_max_epi16(H2, M2);                                     \
-      V1 = _mm_max_epi16(V1, M1);                                     \
-      V2 = _mm_max_epi16(V2, M2);                                     \
-      _mm_store_si128(Hrp, H1);                                       \
-      _mm_store_si128(Hrp+1, H2);                                     \
-      _mm_store_si128(Hrp+TO, V1);                                    \
-      _mm_store_si128(Hrp+TO+1, V2);                                  \
+      M1 = vec_add8sh(M1, m_oe);                                   \
+      M2 = vec_add8sh(M2, m_oe);                                   \
+      H1 = vec_add8sh(H1, m_e);                                    \
+      H2 = vec_add8sh(H2, m_e);                                    \
+      V1 = vec_add8sh(V1, m_e);                                    \
+      V2 = vec_add8sh(V2, m_e);                                    \
+      H1 = vec_max8sh(H1, M1);                                     \
+      H2 = vec_max8sh(H2, M2);                                     \
+      V1 = vec_max8sh(V1, M1);                                     \
+      V2 = vec_max8sh(V2, M2);                                     \
+      vec_store1q(Hrp, H1);                                       \
+      vec_store1q(Hrp+1, H2);                                     \
+      vec_store1q(Hrp+TO, V1);                                    \
+      vec_store1q(Hrp+TO+1, V2);                                  \
       abort();                                                        \
       __m128i n = R;                                                  \
       __m128i vc = R;                                                 \
-      __m128i v = _mm_load_si128(bvp);                                \
-      __m128i rp = _mm_load_si128(bvp+TO*2);                          \
-      __m128i np = _mm_load_si128(bvp+TO);                            \
+      __m128i v = vec_load1q(bvp);                                \
+      __m128i rp = vec_load1q(bvp+TO*2);                          \
+      __m128i np = vec_load1q(bvp+TO);                            \
       if ( dir ) {                                                    \
-          n = _mm_cmpgt_epi16(n, v);                                  \
-          v = _mm_max_epi16(v, vc);                                   \
-          _mm_store_si128(bvp, v);                                    \
-          vc = _mm_cmpeq_epi16(vc, v);                                \
-          n = _mm_andnot_si128(n, np);                                \
-          n = _mm_sub_epi16(n, vc);                                   \
-          _mm_store_si128(bvp+TO, n);                                 \
-          vc = _mm_and_si128(vc, r4);                                 \
-          rp = _mm_max_epi16(rp, vc);                                 \
-          _mm_store_si128(bvp+TO*2, rp);                              \
+          n = vec_comparegt8sh(n, v);                                  \
+          v = vec_max8sh(v, vc);                                   \
+          vec_store1q(bvp, v);                                    \
+          vc = vec_compareeq8sh(vc, v);                                \
+          n = vec_bitandnotleft1q(n, np);                                \
+          n = vec_subtract8sh(n, vc);                                   \
+          vec_store1q(bvp+TO, n);                                 \
+          vc = vec_bitand1q(vc, r4);                                 \
+          rp = vec_max8sh(rp, vc);                                 \
+          vec_store1q(bvp+TO*2, rp);                              \
       }                                                               \
       else {                                                          \
-          n = _mm_cmpgt_epi16(n, v);                                  \
-          v = _mm_max_epi16(v, vc);                                   \
-          _mm_store_si128(bvp, v);                                    \
-          vc = _mm_cmpeq_epi16(vc, v);                                \
-          rp = _mm_or_si128(rp, n);                                   \
-          n = _mm_andnot_si128(n, np);                                \
-          n = _mm_sub_epi16(n, vc);                                   \
-          _mm_store_si128(bvp+TO, n);                                 \
-          rp = _mm_max_epi16(rp, r4);                                 \
-          _mm_store_si128(bvp+TO*2, rp);                              \
+          n = vec_comparegt8sh(n, v);                                  \
+          v = vec_max8sh(v, vc);                                   \
+          vec_store1q(bvp, v);                                    \
+          vc = vec_compareeq8sh(vc, v);                                \
+          rp = vec_bitor1q(rp, n);                                   \
+          n = vec_bitandnotleft1q(n, np);                                \
+          n = vec_subtract8sh(n, vc);                                   \
+          vec_store1q(bvp+TO, n);                                 \
+          rp = vec_max8sh(rp, r4);                                 \
+          vec_store1q(bvp+TO*2, rp);                              \
       }                                                               \
     }
 
 #define STEPqec1()                                              \
     {                                                           \
-      __m128i M = _mm_loadu_si128((__m128i*)(ar-(r-2)+j));    \
-      __m128i wb = _mm_load_si128((__m128i*)(bf+j));          \
-      __m128i Rp = _mm_loadu_si128((__m128i*)(Rr+j-1));       \
-      __m128i H = _mm_loadu_si128((__m128i*)(Hr+j-1));        \
-      M = _mm_cmpeq_epi16(M, wb);                             \
-      M = _mm_and_si128(M, m_ma_over_mi);                     \
-      M = _mm_add_epi16(M, m_mi);                             \
-      M = _mm_add_epi16(M, Rp);                               \
-      if ( qsc ) M = _mm_max_epi16(M, _mm_setzero_si128());   \
-      __m128i R = _mm_load_si128((__m128i*)(Vr+j));           \
-      __m128i V = _mm_load_si128((__m128i*)(Vr+j));           \
-      R = _mm_max_epi16(R, H);                                \
-      R = _mm_max_epi16(R, M);                                \
-      _mm_store_si128((__m128i*)(Rr+j), R);                   \
-      M = _mm_add_epi16(M, m_oe);                             \
-      V = _mm_add_epi16(V, m_e);                              \
-      H = _mm_add_epi16(H, m_e);                              \
-      V = _mm_max_epi16(V, M);                                \
-      H = _mm_max_epi16(H, M);                                \
-      _mm_store_si128((__m128i*)(Vr+j), V);                   \
-      _mm_store_si128((__m128i*)(Hr+j), H);                   \
+      __m128i M = vec_load1qu((__m128i*)(ar-(r-2)+j));    \
+      __m128i wb = vec_load1q((__m128i*)(bf+j));          \
+      __m128i Rp = vec_load1qu((__m128i*)(Rr+j-1));       \
+      __m128i H = vec_load1qu((__m128i*)(Hr+j-1));        \
+      M = vec_compareeq8sh(M, wb);                             \
+      M = vec_bitand1q(M, m_ma_over_mi);                     \
+      M = vec_add8sh(M, m_mi);                             \
+      M = vec_add8sh(M, Rp);                               \
+      if ( qsc ) M = vec_max8sh(M, vec_zero1q());   \
+      __m128i R = vec_load1q((__m128i*)(Vr+j));           \
+      __m128i V = vec_load1q((__m128i*)(Vr+j));           \
+      R = vec_max8sh(R, H);                                \
+      R = vec_max8sh(R, M);                                \
+      vec_store1q((__m128i*)(Rr+j), R);                   \
+      M = vec_add8sh(M, m_oe);                             \
+      V = vec_add8sh(V, m_e);                              \
+      H = vec_add8sh(H, m_e);                              \
+      V = vec_max8sh(V, M);                                \
+      H = vec_max8sh(H, M);                                \
+      vec_store1q((__m128i*)(Vr+j), V);                   \
+      vec_store1q((__m128i*)(Hr+j), H);                   \
         {                                                       \
-          __m128i rv = _mm_load_si128((__m128i*)(bv+j));      \
-          __m128i rr = _mm_load_si128((__m128i*)(br+j));      \
-          __m128i gt = _mm_cmpgt_epi16(R, rv);                \
-          rv = _mm_max_epi16(rv, R);                          \
-          _mm_store_si128((__m128i*)(bv+j), rv);              \
-          __m128i lt = _mm_cmplt_epi16(R, rv);                \
-          __m128i inc_n = _mm_load_si128(&m_one);             \
-          inc_n = _mm_add_epi16(inc_n, lt);                   \
+          __m128i rv = vec_load1q((__m128i*)(bv+j));      \
+          __m128i rr = vec_load1q((__m128i*)(br+j));      \
+          __m128i gt = vec_comparegt8sh(R, rv);                \
+          rv = vec_max8sh(rv, R);                          \
+          vec_store1q((__m128i*)(bv+j), rv);              \
+          __m128i lt = vec_comparelt8sh(R, rv);                \
+          __m128i inc_n = vec_load1q(&m_one);             \
+          inc_n = vec_add8sh(inc_n, lt);                   \
           if ( dir ) {                                        \
-              lt = _mm_or_si128(lt, r4);                      \
-              rr = _mm_max_epi16(rr, lt);                     \
+              lt = vec_bitor1q(lt, r4);                      \
+              rr = vec_max8sh(rr, lt);                     \
           }                                                   \
           else {                                              \
-              rr = _mm_or_si128(rr, gt);                      \
-              rr = _mm_max_epi16(rr, r4);                     \
+              rr = vec_bitor1q(rr, gt);                      \
+              rr = vec_max8sh(rr, r4);                     \
           }                                                   \
-          __m128i rn = _mm_load_si128((__m128i*)(bn+j));      \
-          rn = _mm_andnot_si128(gt, rn);                      \
-          _mm_store_si128((__m128i*)(br+j), rr);              \
-          rn = _mm_add_epi16(rn, inc_n);                      \
-          _mm_store_si128((__m128i*)(bn+j), rn);              \
+          __m128i rn = vec_load1q((__m128i*)(bn+j));      \
+          rn = vec_bitandnotleft1q(gt, rn);                      \
+          vec_store1q((__m128i*)(br+j), rr);              \
+          rn = vec_add8sh(rn, inc_n);                      \
+          vec_store1q((__m128i*)(bn+j), rn);              \
         }                                                       \
     }
 #define STEPr()                                         \
   if ( dir ) {                                        \
-      __m128i n = _mm_load_si128((__m128i*)(Rr+j));   \
-      __m128i vc = _mm_load_si128((__m128i*)(Rr+j));  \
-      __m128i v = _mm_load_si128((__m128i*)(bv+j));   \
-      __m128i rp = _mm_load_si128((__m128i*)(br+j));  \
-      __m128i np = _mm_load_si128((__m128i*)(bn+j));  \
-      n = _mm_cmpgt_epi16(n, v);                      \
-      v = _mm_max_epi16(v, vc);                       \
-      _mm_store_si128((__m128i*)(bv+j), v);           \
-      vc = _mm_cmpeq_epi16(vc, v);                    \
-      n = _mm_andnot_si128(n, np);                    \
-      n = _mm_sub_epi16(n, vc);                       \
-      _mm_store_si128((__m128i*)(bn+j), n);           \
-      vc = _mm_and_si128(vc, r4);                     \
-      rp = _mm_max_epi16(rp, vc);                     \
-      _mm_store_si128((__m128i*)(br+j), rp);          \
+      __m128i n = vec_load1q((__m128i*)(Rr+j));   \
+      __m128i vc = vec_load1q((__m128i*)(Rr+j));  \
+      __m128i v = vec_load1q((__m128i*)(bv+j));   \
+      __m128i rp = vec_load1q((__m128i*)(br+j));  \
+      __m128i np = vec_load1q((__m128i*)(bn+j));  \
+      n = vec_comparegt8sh(n, v);                      \
+      v = vec_max8sh(v, vc);                       \
+      vec_store1q((__m128i*)(bv+j), v);           \
+      vc = vec_compareeq8sh(vc, v);                    \
+      n = vec_bitandnotleft1q(n, np);                    \
+      n = vec_subtract8sh(n, vc);                       \
+      vec_store1q((__m128i*)(bn+j), n);           \
+      vc = vec_bitand1q(vc, r4);                     \
+      rp = vec_max8sh(rp, vc);                     \
+      vec_store1q((__m128i*)(br+j), rp);          \
   }                                                   \
   else {                                              \
-      __m128i n = _mm_load_si128((__m128i*)(Rr+j));   \
-      __m128i vc = _mm_load_si128((__m128i*)(Rr+j));  \
-      __m128i v = _mm_load_si128((__m128i*)(bv+j));   \
-      __m128i rp = _mm_load_si128((__m128i*)(br+j));  \
-      __m128i np = _mm_load_si128((__m128i*)(bn+j));  \
-      n = _mm_cmpgt_epi16(n, v);                      \
-      v = _mm_max_epi16(v, vc);                       \
-      _mm_store_si128((__m128i*)(bv+j), v);           \
-      vc = _mm_cmpeq_epi16(vc, v);                    \
-      rp = _mm_or_si128(rp, n);                       \
-      n = _mm_andnot_si128(n, np);                    \
-      n = _mm_sub_epi16(n, vc);                       \
-      _mm_store_si128((__m128i*)(bn+j), n);           \
-      rp = _mm_max_epi16(rp, r4);                     \
-      _mm_store_si128((__m128i*)(br+j), rp);          \
+      __m128i n = vec_load1q((__m128i*)(Rr+j));   \
+      __m128i vc = vec_load1q((__m128i*)(Rr+j));  \
+      __m128i v = vec_load1q((__m128i*)(bv+j));   \
+      __m128i rp = vec_load1q((__m128i*)(br+j));  \
+      __m128i np = vec_load1q((__m128i*)(bn+j));  \
+      n = vec_comparegt8sh(n, v);                      \
+      v = vec_max8sh(v, vc);                       \
+      vec_store1q((__m128i*)(bv+j), v);           \
+      vc = vec_compareeq8sh(vc, v);                    \
+      rp = vec_bitor1q(rp, n);                       \
+      n = vec_bitandnotleft1q(n, np);                    \
+      n = vec_subtract8sh(n, vc);                       \
+      vec_store1q((__m128i*)(bn+j), n);           \
+      rp = vec_max8sh(rp, r4);                     \
+      vec_store1q((__m128i*)(br+j), rp);          \
   }
 #define STEPrp()                                \
   if ( dir ) {                                \
-      __m128i n = _mm_load_si128(Rrp);        \
-      __m128i vc = _mm_load_si128(Rrp);       \
-      __m128i v = _mm_load_si128(bvp);        \
-      __m128i rp = _mm_load_si128(bvp+TO*2);  \
-      __m128i np = _mm_load_si128(bvp+TO);    \
-      n = _mm_cmpgt_epi16(n, v);              \
-      v = _mm_max_epi16(v, vc);               \
-      _mm_store_si128(bvp, v);                \
-      vc = _mm_cmpeq_epi16(vc, v);            \
-      n = _mm_andnot_si128(n, np);            \
-      n = _mm_sub_epi16(n, vc);               \
-      _mm_store_si128(bvp+TO, n);             \
-      vc = _mm_and_si128(vc, r4);             \
-      rp = _mm_max_epi16(rp, vc);             \
-      _mm_store_si128(bvp+TO*2, rp);          \
+      __m128i n = vec_load1q(Rrp);        \
+      __m128i vc = vec_load1q(Rrp);       \
+      __m128i v = vec_load1q(bvp);        \
+      __m128i rp = vec_load1q(bvp+TO*2);  \
+      __m128i np = vec_load1q(bvp+TO);    \
+      n = vec_comparegt8sh(n, v);              \
+      v = vec_max8sh(v, vc);               \
+      vec_store1q(bvp, v);                \
+      vc = vec_compareeq8sh(vc, v);            \
+      n = vec_bitandnotleft1q(n, np);            \
+      n = vec_subtract8sh(n, vc);               \
+      vec_store1q(bvp+TO, n);             \
+      vc = vec_bitand1q(vc, r4);             \
+      rp = vec_max8sh(rp, vc);             \
+      vec_store1q(bvp+TO*2, rp);          \
   }                                           \
   else {                                      \
-      __m128i n = _mm_load_si128(Rrp);        \
-      __m128i vc = _mm_load_si128(Rrp);       \
-      __m128i v = _mm_load_si128(bvp);        \
-      __m128i rp = _mm_load_si128(bvp+TO*2);  \
-      __m128i np = _mm_load_si128(bvp+TO);    \
-      n = _mm_cmpgt_epi16(n, v);              \
-      v = _mm_max_epi16(v, vc);               \
-      _mm_store_si128(bvp, v);                \
-      vc = _mm_cmpeq_epi16(vc, v);            \
-      rp = _mm_or_si128(rp, n);               \
-      n = _mm_andnot_si128(n, np);            \
-      n = _mm_sub_epi16(n, vc);               \
-      _mm_store_si128(bvp+TO, n);             \
-      rp = _mm_max_epi16(rp, r4);             \
-      _mm_store_si128(bvp+TO*2, rp);          \
+      __m128i n = vec_load1q(Rrp);        \
+      __m128i vc = vec_load1q(Rrp);       \
+      __m128i v = vec_load1q(bvp);        \
+      __m128i rp = vec_load1q(bvp+TO*2);  \
+      __m128i np = vec_load1q(bvp+TO);    \
+      n = vec_comparegt8sh(n, v);              \
+      v = vec_max8sh(v, vc);               \
+      vec_store1q(bvp, v);                \
+      vc = vec_compareeq8sh(vc, v);            \
+      rp = vec_bitor1q(rp, n);               \
+      n = vec_bitandnotleft1q(n, np);            \
+      n = vec_subtract8sh(n, vc);               \
+      vec_store1q(bvp+TO, n);             \
+      rp = vec_max8sh(rp, r4);             \
+      vec_store1q(bvp+TO*2, rp);          \
   }
 #define STEPrp2()                                   \
   if ( dir ) {                                    \
-      __m128i n1 = _mm_load_si128(Rrp-1);         \
-      __m128i n2 = _mm_load_si128(Rrp  );         \
-      __m128i v1 = _mm_load_si128(bvp-1);         \
-      __m128i v2 = _mm_load_si128(bvp  );         \
-      __m128i vc1 = _mm_load_si128(Rrp-1);        \
-      __m128i vc2 = _mm_load_si128(Rrp  );        \
-      n1 = _mm_cmpgt_epi16(n1, v1);               \
-      n2 = _mm_cmpgt_epi16(n2, v2);               \
-      v1 = _mm_max_epi16(v1, vc1);                \
-      v2 = _mm_max_epi16(v2, vc2);                \
-      _mm_store_si128(bvp-1, v1);                 \
-      _mm_store_si128(bvp  , v2);                 \
-      vc1 = _mm_cmpeq_epi16(vc1, v1);             \
-      vc2 = _mm_cmpeq_epi16(vc2, v2);             \
-      __m128i np1 = _mm_load_si128(bvp+TO-1);     \
-      __m128i np2 = _mm_load_si128(bvp+TO  );     \
-      n1 = _mm_andnot_si128(n1, np1);             \
-      n2 = _mm_andnot_si128(n2, np2);             \
-      n1 = _mm_sub_epi16(n1, vc1);                \
-      n2 = _mm_sub_epi16(n2, vc2);                \
-      __m128i rp1 = _mm_load_si128(bvp+TO*2-1);   \
-      __m128i rp2 = _mm_load_si128(bvp+TO*2  );   \
-      _mm_store_si128(bvp+TO-1, n1);              \
-      _mm_store_si128(bvp+TO  , n2);              \
-      vc1 = _mm_and_si128(vc1, r4);               \
-      vc2 = _mm_and_si128(vc2, r4);               \
-      rp1 = _mm_max_epi16(rp1, vc1);              \
-      rp2 = _mm_max_epi16(rp2, vc2);              \
-      _mm_store_si128(bvp+TO*2-1, rp1);           \
-      _mm_store_si128(bvp+TO*2  , rp2);           \
+      __m128i n1 = vec_load1q(Rrp-1);         \
+      __m128i n2 = vec_load1q(Rrp  );         \
+      __m128i v1 = vec_load1q(bvp-1);         \
+      __m128i v2 = vec_load1q(bvp  );         \
+      __m128i vc1 = vec_load1q(Rrp-1);        \
+      __m128i vc2 = vec_load1q(Rrp  );        \
+      n1 = vec_comparegt8sh(n1, v1);               \
+      n2 = vec_comparegt8sh(n2, v2);               \
+      v1 = vec_max8sh(v1, vc1);                \
+      v2 = vec_max8sh(v2, vc2);                \
+      vec_store1q(bvp-1, v1);                 \
+      vec_store1q(bvp  , v2);                 \
+      vc1 = vec_compareeq8sh(vc1, v1);             \
+      vc2 = vec_compareeq8sh(vc2, v2);             \
+      __m128i np1 = vec_load1q(bvp+TO-1);     \
+      __m128i np2 = vec_load1q(bvp+TO  );     \
+      n1 = vec_bitandnotleft1q(n1, np1);             \
+      n2 = vec_bitandnotleft1q(n2, np2);             \
+      n1 = vec_subtract8sh(n1, vc1);                \
+      n2 = vec_subtract8sh(n2, vc2);                \
+      __m128i rp1 = vec_load1q(bvp+TO*2-1);   \
+      __m128i rp2 = vec_load1q(bvp+TO*2  );   \
+      vec_store1q(bvp+TO-1, n1);              \
+      vec_store1q(bvp+TO  , n2);              \
+      vc1 = vec_bitand1q(vc1, r4);               \
+      vc2 = vec_bitand1q(vc2, r4);               \
+      rp1 = vec_max8sh(rp1, vc1);              \
+      rp2 = vec_max8sh(rp2, vc2);              \
+      vec_store1q(bvp+TO*2-1, rp1);           \
+      vec_store1q(bvp+TO*2  , rp2);           \
   }                                               \
   else {                                          \
-      __m128i n1 = _mm_load_si128(Rrp-1);         \
-      __m128i n2 = _mm_load_si128(Rrp  );         \
-      __m128i v1 = _mm_load_si128(bvp-1);         \
-      __m128i v2 = _mm_load_si128(bvp  );         \
-      __m128i vc1 = _mm_load_si128(Rrp-1);        \
-      __m128i vc2 = _mm_load_si128(Rrp  );        \
-      n1 = _mm_cmpgt_epi16(n1, v1);               \
-      n2 = _mm_cmpgt_epi16(n2, v2);               \
-      v1 = _mm_max_epi16(v1, vc1);                \
-      v2 = _mm_max_epi16(v2, vc2);                \
-      _mm_store_si128(bvp-1, v1);                 \
-      _mm_store_si128(bvp  , v2);                 \
-      vc1 = _mm_cmpeq_epi16(vc1, v1);             \
-      vc2 = _mm_cmpeq_epi16(vc2, v2);             \
-      __m128i rp1 = _mm_load_si128(bvp+TO*2-1);   \
-      __m128i rp2 = _mm_load_si128(bvp+TO*2  );   \
-      __m128i np1 = _mm_load_si128(bvp+TO-1);     \
-      __m128i np2 = _mm_load_si128(bvp+TO  );     \
-      rp1 = _mm_or_si128(rp1, n1);                \
-      rp2 = _mm_or_si128(rp2, n2);                \
-      n1 = _mm_andnot_si128(n1, np1);             \
-      n2 = _mm_andnot_si128(n2, np2);             \
-      n1 = _mm_sub_epi16(n1, vc1);                \
-      n2 = _mm_sub_epi16(n2, vc2);                \
-      _mm_store_si128(bvp+TO-1, n1);              \
-      _mm_store_si128(bvp+TO  , n2);              \
-      rp1 = _mm_max_epi16(rp1, r4);               \
-      rp2 = _mm_max_epi16(rp2, r4);               \
-      _mm_store_si128(bvp+TO*2-1, rp1);           \
-      _mm_store_si128(bvp+TO*2  , rp2);           \
+      __m128i n1 = vec_load1q(Rrp-1);         \
+      __m128i n2 = vec_load1q(Rrp  );         \
+      __m128i v1 = vec_load1q(bvp-1);         \
+      __m128i v2 = vec_load1q(bvp  );         \
+      __m128i vc1 = vec_load1q(Rrp-1);        \
+      __m128i vc2 = vec_load1q(Rrp  );        \
+      n1 = vec_comparegt8sh(n1, v1);               \
+      n2 = vec_comparegt8sh(n2, v2);               \
+      v1 = vec_max8sh(v1, vc1);                \
+      v2 = vec_max8sh(v2, vc2);                \
+      vec_store1q(bvp-1, v1);                 \
+      vec_store1q(bvp  , v2);                 \
+      vc1 = vec_compareeq8sh(vc1, v1);             \
+      vc2 = vec_compareeq8sh(vc2, v2);             \
+      __m128i rp1 = vec_load1q(bvp+TO*2-1);   \
+      __m128i rp2 = vec_load1q(bvp+TO*2  );   \
+      __m128i np1 = vec_load1q(bvp+TO-1);     \
+      __m128i np2 = vec_load1q(bvp+TO  );     \
+      rp1 = vec_bitor1q(rp1, n1);                \
+      rp2 = vec_bitor1q(rp2, n2);                \
+      n1 = vec_bitandnotleft1q(n1, np1);             \
+      n2 = vec_bitandnotleft1q(n2, np2);             \
+      n1 = vec_subtract8sh(n1, vc1);                \
+      n2 = vec_subtract8sh(n2, vc2);                \
+      vec_store1q(bvp+TO-1, n1);              \
+      vec_store1q(bvp+TO  , n2);              \
+      rp1 = vec_max8sh(rp1, r4);               \
+      rp2 = vec_max8sh(rp2, r4);               \
+      vec_store1q(bvp+TO*2-1, rp1);           \
+      vec_store1q(bvp+TO*2  , rp2);           \
   }
 #define STEP() if ( qec ) { STEPqec1(); } else { STEPqec0(); }
 
@@ -727,7 +726,7 @@ void process(size_t n, size_t m)
               __m128i* Hrp = (__m128i*)(Hr+js);
               __m128i* Hre = (__m128i*)(Hr+je);
               if ( use_qec1 && qec ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   do {
                       STEPqec1p();
@@ -764,7 +763,7 @@ void process(size_t n, size_t m)
           if ( qec ) {
               if ( !use_qec1 ) {
                   const int js = ((r-3)&-8);
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   __m128i* bve = (__m128i*)(bv+je);
                   __m128i* Rrp = (__m128i*)(Rr+js);
@@ -808,7 +807,7 @@ void process(size_t n, size_t m)
               __m128i* Hrp = (__m128i*)(Hr+js);
               __m128i* Hre = (__m128i*)(Hr+je);
               if ( use_qec1 && qec ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   do {
                       STEPqec1p();
@@ -842,7 +841,7 @@ void process(size_t n, size_t m)
           if ( qec ) {
               if ( !use_qec1 ) {
                   const int js = ((r-3)&-8);
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   __m128i* bve = (__m128i*)(bv+je);
                   __m128i* Rrp = (__m128i*)(Rr+js);
@@ -899,7 +898,7 @@ void process(size_t n, size_t m)
               __m128i* Hrp = (__m128i*)(Hr+js);
               __m128i* Hre = (__m128i*)(Hr+je);
               if ( use_qec1 && qec ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   do {
                       STEPqec1p();
@@ -929,7 +928,7 @@ void process(size_t n, size_t m)
           CHECK_ROW(r,R,Rr);
           if ( qec ) {
               if ( !use_qec1 ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   __m128i* bve = (__m128i*)(bv+je);
                   __m128i* Rrp = (__m128i*)(Rr+js);
@@ -1018,7 +1017,7 @@ void process(size_t n, size_t m)
               __m128i* Hrp = (__m128i*)(Hr+js);
               __m128i* Hre = (__m128i*)(Hr+je);
               if ( use_qec1 && qec ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   do {
                       STEPqec1p();
@@ -1059,7 +1058,7 @@ void process(size_t n, size_t m)
           if ( qec ) {
               if ( !use_qec1 ) {
                   const int js = ((r-3)&-8);
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   __m128i* bve = (__m128i*)(bv+je);
                   __m128i* Rrp = (__m128i*)(Rr+js);
@@ -1099,7 +1098,7 @@ void process(size_t n, size_t m)
               __m128i* Hrp = (__m128i*)(Hr+js);
               __m128i* Hre = (__m128i*)(Hr+je);
               if ( use_qec1 && qec ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   do {
                       STEPqec1p();
@@ -1135,7 +1134,7 @@ void process(size_t n, size_t m)
           if ( qec ) {
               if ( !use_qec1 ) {
                   const int js = ((r-3)&-8);
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   __m128i* bve = (__m128i*)(bv+je);
                   __m128i* Rrp = (__m128i*)(Rr+js);
@@ -1173,7 +1172,7 @@ void process(size_t n, size_t m)
               __m128i* Hrp = (__m128i*)(Hr+js);
               __m128i* Hre = (__m128i*)(Hr+je);
               if ( use_qec1 && qec ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   do {
                       STEPqec1p();
@@ -1203,7 +1202,7 @@ void process(size_t n, size_t m)
           CHECK_ROW(r,R,Rr);
           if ( qec ) {
               if ( !use_qec1 ) {
-                  __m128i r4 = _mm_set1_epi16(dir? r: INF-r);
+                  __m128i r4 = vec_splat8sh(dir? r: INF-r);
                   __m128i* bvp = (__m128i*)(bv+js);
                   __m128i* bve = (__m128i*)(bv+je);
                   __m128i* Rrp = (__m128i*)(Rr+js);
@@ -1321,13 +1320,13 @@ inline void process(const string& t, const string& q, int qsc, int qec,
   SNS::mi = mi;
   SNS::go = go;
   SNS::ge = ge;
-  m_e = _mm_set1_epi16(ge);
-  m_o = _mm_set1_epi16(go);
-  m_oe = _mm_set1_epi16(go+ge);
-  m_mi = _mm_set1_epi16(mi);
-  m_ma = _mm_set1_epi16(ma);
-  m_ma_over_mi = _mm_set1_epi16(ma-mi);
-  m_one = _mm_set1_epi16(1);
+  m_e = vec_splat8sh(ge);
+  m_o = vec_splat8sh(go);
+  m_oe = vec_splat8sh(go+ge);
+  m_mi = vec_splat8sh(mi);
+  m_ma = vec_splat8sh(ma);
+  m_ma_over_mi = vec_splat8sh(ma-mi);
+  m_one = vec_splat8sh(1);
 
   forN ( k, 8 ) {
       bf[k-8] = 'b'<<8;
--- src/sw/lib/Solution9.cpp
+++ src/sw/lib/Solution9.cpp
@@ -5,7 +5,8 @@
 #include <sstream>
 #include <string>
 #include <cstring>
-#include <pmmintrin.h>
+#include <vec128int.h>
+#include <vecmisc.h>
 #include <cstdio>
 #include "Solution9.h"
 
@@ -146,9 +147,9 @@ static int WorkQueue[SIMDARRAYWIDTH / SIMDTYPEMULTIPLE + 1];
 
 
 char __attribute__((aligned(128))) alignmentForce[128];    // Following consts don't get aligned to 16 without this hmm
-const __m128i NegInfiniteSIMD16 = _mm_set1_epi16(-INFINITE16);
-const __m128i TwiceInfinitesSIMD16 = _mm_set1_epi16(INFINITE16 * 2);
-//const __m128i negInfiniteSIMDShiftedBig16 = _mm_srli_si128(negInfiniteSIMD16, SIMDBIGSHIFT16);    // Should be this but it triggers a bug in old GCC
+const __m128i NegInfiniteSIMD16 = vec_splat8sh(-INFINITE16);
+const __m128i TwiceInfinitesSIMD16 = vec_splat8sh(INFINITE16 * 2);
+//const __m128i negInfiniteSIMDShiftedBig16 = vec_shiftrightbytes1q(negInfiniteSIMD16, SIMDBIGSHIFT16);    // Should be this but it triggers a bug in old GCC
 const char __attribute__((aligned (16))) NegInfiniteSIMDShiftedBigData16[16] = {0x68, 0xC5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
 const short int* NegInfiniteSIMDShiftedBigDataShort16 = ((short int*)(NegInfiniteSIMDShiftedBigData16));
 const __m128i NegInfiniteSIMDShiftedBig16 = *((__m128i*)(NegInfiniteSIMDShiftedBigData16));
@@ -216,40 +217,40 @@ void inline Solution9::CheckOpt16(const short int m, const int i, const int j) {
 template<bool DIRECTION>
 void Solution9::ExtractOpts8(const __m128i newM, const int ltMask, const int i, const int jStart) {
     if((ltMask & 0x00FF) != 0x0000) {
-        if(ltMask & 0x0001) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 0) & 0xFF, i, jStart + 0);
-        if(ltMask & 0x0002) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 0) >> 8,   i, jStart + 1);
-        if(ltMask & 0x0004) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 1) & 0xFF, i, jStart + 2);
-        if(ltMask & 0x0008) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 1) >> 8,   i, jStart + 3);
-        if(ltMask & 0x0010) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 2) & 0xFF, i, jStart + 4);
-        if(ltMask & 0x0020) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 2) >> 8,   i, jStart + 5);
-        if(ltMask & 0x0040) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 3) & 0xFF, i, jStart + 6);
-        if(ltMask & 0x0080) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 3) >> 8,   i, jStart + 7);
+        if(ltMask & 0x0001) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 0) & 0xFF, i, jStart + 0);
+        if(ltMask & 0x0002) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 0) >> 8,   i, jStart + 1);
+        if(ltMask & 0x0004) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 1) & 0xFF, i, jStart + 2);
+        if(ltMask & 0x0008) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 1) >> 8,   i, jStart + 3);
+        if(ltMask & 0x0010) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 2) & 0xFF, i, jStart + 4);
+        if(ltMask & 0x0020) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 2) >> 8,   i, jStart + 5);
+        if(ltMask & 0x0040) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 3) & 0xFF, i, jStart + 6);
+        if(ltMask & 0x0080) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 3) >> 8,   i, jStart + 7);
     }
     if((ltMask & 0xFF00) != 0x0000) {
-        if(ltMask & 0x0100) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 4) & 0xFF, i, jStart + 8);
-        if(ltMask & 0x0200) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 4) >> 8,   i, jStart + 9);
-        if(ltMask & 0x0400) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 5) & 0xFF, i, jStart + 10);
-        if(ltMask & 0x0800) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 5) >> 8,   i, jStart + 11);
-        if(ltMask & 0x1000) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 6) & 0xFF, i, jStart + 12);
-        if(ltMask & 0x2000) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 6) >> 8,   i, jStart + 13);
-        if(ltMask & 0x4000) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 7) & 0xFF, i, jStart + 14);
-        if(ltMask & 0x8000) CheckOpt8<DIRECTION>(_mm_extract_epi16(newM, 7) >> 8,   i, jStart + 15);
+        if(ltMask & 0x0100) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 4) & 0xFF, i, jStart + 8);
+        if(ltMask & 0x0200) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 4) >> 8,   i, jStart + 9);
+        if(ltMask & 0x0400) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 5) & 0xFF, i, jStart + 10);
+        if(ltMask & 0x0800) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 5) >> 8,   i, jStart + 11);
+        if(ltMask & 0x1000) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 6) & 0xFF, i, jStart + 12);
+        if(ltMask & 0x2000) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 6) >> 8,   i, jStart + 13);
+        if(ltMask & 0x4000) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 7) & 0xFF, i, jStart + 14);
+        if(ltMask & 0x8000) CheckOpt8<DIRECTION>(vec_extract8sh(newM, 7) >> 8,   i, jStart + 15);
     }    
 }
 
 template<bool DIRECTION>
 void Solution9::ExtractOpts16(const __m128i newM, const int ltMask, const int i, const int jStart) {
     if((ltMask & 0x00FF) != 0x00FF) {
-        if(!(ltMask & 0x0001)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 0), i, jStart + 0);
-        if(!(ltMask & 0x0004)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 1), i, jStart + 1);
-        if(!(ltMask & 0x0010)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 2), i, jStart + 2);
-        if(!(ltMask & 0x0040)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 3), i, jStart + 3);
+        if(!(ltMask & 0x0001)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 0), i, jStart + 0);
+        if(!(ltMask & 0x0004)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 1), i, jStart + 1);
+        if(!(ltMask & 0x0010)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 2), i, jStart + 2);
+        if(!(ltMask & 0x0040)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 3), i, jStart + 3);
     }
     if((ltMask & 0xFF00) != 0xFF00) {
-        if(!(ltMask & 0x0100)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 4), i, jStart + 4);
-        if(!(ltMask & 0x0400)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 5), i, jStart + 5);
-        if(!(ltMask & 0x1000)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 6), i, jStart + 6);
-        if(!(ltMask & 0x4000)) CheckOpt16<DIRECTION>(_mm_extract_epi16(newM, 7), i, jStart + 7);
+        if(!(ltMask & 0x0100)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 4), i, jStart + 4);
+        if(!(ltMask & 0x0400)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 5), i, jStart + 5);
+        if(!(ltMask & 0x1000)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 6), i, jStart + 6);
+        if(!(ltMask & 0x4000)) CheckOpt16<DIRECTION>(vec_extract8sh(newM, 7), i, jStart + 7);
     }    
 }
 
@@ -280,56 +281,56 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
     MHVTRealType H = HData;
     MHVTRealType TargetLookup = TargetLookupData;
     //__m128i* QueueNewMPlusOE = (__m128i*)QueueNewMPlusOEData;
-    __m128i optSIMD = _mm_set1_epi8(Opt & 0xFF);
+    __m128i optSIMD = vec_splat16sb(Opt & 0xFF);
 
 
-    const __m128i negGapExtensionSIMD = _mm_set1_epi8(-GapExtension);
-    const __m128i negGapOpenPlusgapExtensionSIMD = _mm_set1_epi8(-(GapOpen + GapExtension));
-    const __m128i negGapExtensionTimes16SIMD = _mm_set1_epi8(-GapExtension * 16);
-    const __m128i negGapExtensionTimes8SIMD = _mm_set1_epi8(-GapExtension * 8);
-    const __m128i negGapExtensionTimes4SIMD = _mm_set1_epi8(-GapExtension * 4);
-    const __m128i negGapExtensionTimes2SIMD = _mm_set1_epi8(-GapExtension * 2);
-    const __m128i negMismatchSIMD = _mm_set1_epi8(-MismatchScore);
-    const __m128i thresholdSIMD = _mm_set1_epi8((char)(threshold - 1));
+    const __m128i negGapExtensionSIMD = vec_splat16sb(-GapExtension);
+    const __m128i negGapOpenPlusgapExtensionSIMD = vec_splat16sb(-(GapOpen + GapExtension));
+    const __m128i negGapExtensionTimes16SIMD = vec_splat16sb(-GapExtension * 16);
+    const __m128i negGapExtensionTimes8SIMD = vec_splat16sb(-GapExtension * 8);
+    const __m128i negGapExtensionTimes4SIMD = vec_splat16sb(-GapExtension * 4);
+    const __m128i negGapExtensionTimes2SIMD = vec_splat16sb(-GapExtension * 2);
+    const __m128i negMismatchSIMD = vec_splat16sb(-MismatchScore);
+    const __m128i thresholdSIMD = vec_splat16sb((char)(threshold - 1));
 
     int jBlockSetup = 0;
     do {
-        _mm_store_si128(M[0] + jBlockSetup, _mm_setzero_si128());
-        _mm_store_si128(H[0] + jBlockSetup, _mm_setzero_si128());    // -Infinite = zero for 8bit version
-        _mm_store_si128(V[0] + jBlockSetup, _mm_setzero_si128());
+        vec_store1q(M[0] + jBlockSetup, vec_zero1q());
+        vec_store1q(H[0] + jBlockSetup, vec_zero1q());    // -Infinite = zero for 8bit version
+        vec_store1q(V[0] + jBlockSetup, vec_zero1q());
     } while(++jBlockSetup < (TargetSizeInSIMDBlocks + 2));
 
 
     for(int i = 1; i <= QuerySize; ++i) {
         int jSecondBlock = 0;
         do {
-            __m128i newM = _mm_load_si128(M[(i - 1) & 0x1] + jSecondBlock);
-            __m128i newM2 = _mm_load_si128(M[(i - 1) & 0x1] + jSecondBlock + 1);
-            __m128i newMPlusOE = _mm_subs_epu8(newM, negGapOpenPlusgapExtensionSIMD);    // Unsigned saturared subtraction
-            __m128i newMPlusOE2 = _mm_subs_epu8(newM2, negGapOpenPlusgapExtensionSIMD);
-
-            __m128i oldV = _mm_load_si128(V[(i - 1) & 0x1] + jSecondBlock);
-            __m128i oldV2 = _mm_load_si128(V[(i - 1) & 0x1] + jSecondBlock + 1);
-            oldV = _mm_subs_epu8(oldV, negGapExtensionSIMD);
-            oldV2 = _mm_subs_epu8(oldV2, negGapExtensionSIMD);
-            __m128i newV = _mm_max_epu8(oldV, newMPlusOE);
-            __m128i newV2 = _mm_max_epu8(oldV2, newMPlusOE2);
-            _mm_store_si128(V[i & 0x1] + jSecondBlock, newV);
-            _mm_store_si128(V[i & 0x1] + jSecondBlock + 1, newV2);
+            __m128i newM = vec_load1q(M[(i - 1) & 0x1] + jSecondBlock);
+            __m128i newM2 = vec_load1q(M[(i - 1) & 0x1] + jSecondBlock + 1);
+            __m128i newMPlusOE = vec_subtractsaturating16ub(newM, negGapOpenPlusgapExtensionSIMD);    // Unsigned saturared subtraction
+            __m128i newMPlusOE2 = vec_subtractsaturating16ub(newM2, negGapOpenPlusgapExtensionSIMD);
+
+            __m128i oldV = vec_load1q(V[(i - 1) & 0x1] + jSecondBlock);
+            __m128i oldV2 = vec_load1q(V[(i - 1) & 0x1] + jSecondBlock + 1);
+            oldV = vec_subtractsaturating16ub(oldV, negGapExtensionSIMD);
+            oldV2 = vec_subtractsaturating16ub(oldV2, negGapExtensionSIMD);
+            __m128i newV = vec_max16ub(oldV, newMPlusOE);
+            __m128i newV2 = vec_max16ub(oldV2, newMPlusOE2);
+            vec_store1q(V[i & 0x1] + jSecondBlock, newV);
+            vec_store1q(V[i & 0x1] + jSecondBlock + 1, newV2);
 
             if(DETECTREDUNDANT == true) {
                 // XXX do only when needed
-                _mm_store_si128(H[i & 0x1] + jSecondBlock, _mm_setzero_si128());
-                _mm_store_si128(H[i & 0x1] + jSecondBlock + 1, _mm_setzero_si128());
+                vec_store1q(H[i & 0x1] + jSecondBlock, vec_zero1q());
+                vec_store1q(H[i & 0x1] + jSecondBlock + 1, vec_zero1q());
             }
 
             jSecondBlock += 2;
         } while(jSecondBlock < TargetSizeInSIMDBlocks + 1);
-        __m128i fixupV = _mm_load_si128(V[i & 0x1] + 0);
-        int e = _mm_extract_epi16(fixupV, 0);    // No 8 bit extract...
+        __m128i fixupV = vec_load1q(V[i & 0x1] + 0);
+        int e = vec_extract8sh(fixupV, 0);    // No 8 bit extract...
         e = (e & 0xFF00);    // NEGATIVEINFINITE8 == 0
-        fixupV = _mm_insert_epi16(fixupV, e, 0);
-        _mm_store_si128(V[i & 0x1] + 0, fixupV);
+        fixupV = vec_insert8sh(fixupV, e, 0);
+        vec_store1q(V[i & 0x1] + 0, fixupV);
 
 
 
@@ -343,7 +344,7 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
 
         __m128i rollingNewM;
         if(VARIANT == 1 || VARIANT == 3) {
-            rollingNewM = _mm_setzero_si128();
+            rollingNewM = vec_zero1q();
         }
         if(VARIANT == 2 || VARIANT == 4) {
             assert(false);
@@ -353,53 +354,53 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
 
         int jInnerBlock = 0;
         do {
-            __m128i thisV = _mm_load_si128(V[(i - 1) & 0x1] + jInnerBlock);
-            __m128i thisH = _mm_load_si128(H[(i - 1) & 0x1] + jInnerBlock);
-            __m128i thisM = _mm_load_si128(M[(i - 1) & 0x1] + jInnerBlock);
-            __m128i mx = _mm_max_epu8(thisV, thisH);
-            mx = _mm_max_epu8(mx, thisM);
-            __m128i matchesSubMismatchScore = _mm_load_si128(targetLookupCurrent + jInnerBlock);    // Offset by the mismatch so it's all unsigned
-
-            __m128i newM = _mm_adds_epu8(mx, matchesSubMismatchScore);
-            newM = _mm_subs_epu8(newM, negMismatchSIMD);    // Unoffset again, there's a potential problem here if matches caused some saturation at the top end which is now subtracted from
-
-            __m128i thisV2 = _mm_load_si128(V[(i - 1) & 0x1] + jInnerBlock + 1);
-            __m128i thisH2 = _mm_load_si128(H[(i - 1) & 0x1] + jInnerBlock + 1);
-            __m128i thisM2 = _mm_load_si128(M[(i - 1) & 0x1] + jInnerBlock  + 1);                
-            __m128i mx2 = _mm_max_epu8(thisV2, thisH2);
-            mx2 = _mm_max_epu8(mx2, thisM2);
-            __m128i matchesSubMismatchScore2 = _mm_load_si128(targetLookupCurrent + jInnerBlock + 1);
-
-            __m128i newM2 = _mm_adds_epu8(mx2, matchesSubMismatchScore2);
-            newM2 = _mm_subs_epu8(newM2, negMismatchSIMD);
+            __m128i thisV = vec_load1q(V[(i - 1) & 0x1] + jInnerBlock);
+            __m128i thisH = vec_load1q(H[(i - 1) & 0x1] + jInnerBlock);
+            __m128i thisM = vec_load1q(M[(i - 1) & 0x1] + jInnerBlock);
+            __m128i mx = vec_max16ub(thisV, thisH);
+            mx = vec_max16ub(mx, thisM);
+            __m128i matchesSubMismatchScore = vec_load1q(targetLookupCurrent + jInnerBlock);    // Offset by the mismatch so it's all unsigned
+
+            __m128i newM = vec_addsaturating16ub(mx, matchesSubMismatchScore);
+            newM = vec_subtractsaturating16ub(newM, negMismatchSIMD);    // Unoffset again, there's a potential problem here if matches caused some saturation at the top end which is now subtracted from
+
+            __m128i thisV2 = vec_load1q(V[(i - 1) & 0x1] + jInnerBlock + 1);
+            __m128i thisH2 = vec_load1q(H[(i - 1) & 0x1] + jInnerBlock + 1);
+            __m128i thisM2 = vec_load1q(M[(i - 1) & 0x1] + jInnerBlock  + 1);                
+            __m128i mx2 = vec_max16ub(thisV2, thisH2);
+            mx2 = vec_max16ub(mx2, thisM2);
+            __m128i matchesSubMismatchScore2 = vec_load1q(targetLookupCurrent + jInnerBlock + 1);
+
+            __m128i newM2 = vec_addsaturating16ub(mx2, matchesSubMismatchScore2);
+            newM2 = vec_subtractsaturating16ub(newM2, negMismatchSIMD);
 
             if(VARIANT == 1 || VARIANT == 3) {
-                newM = _mm_max_epu8(newM, _mm_setzero_si128());
-                newM2 = _mm_max_epu8(newM2, _mm_setzero_si128());
+                newM = vec_max16ub(newM, vec_zero1q());
+                newM2 = vec_max16ub(newM2, vec_zero1q());
             }
 
-            __m128i toStoreM = _mm_slli_si128(newM, SIMDLITTLESHIFT8);
-            toStoreM = _mm_or_si128(toStoreM, rollingNewM);
-            rollingNewM = _mm_srli_si128(newM, SIMDBIGSHIFT8);
-            _mm_store_si128(M[i & 0x1] + jInnerBlock, toStoreM);
-            __m128i toStoreM2 = _mm_slli_si128(newM2, SIMDLITTLESHIFT8);
-            toStoreM2 = _mm_or_si128(toStoreM2, rollingNewM);
+            __m128i toStoreM = vec_shiftleftbytes1q(newM, SIMDLITTLESHIFT8);
+            toStoreM = vec_bitor1q(toStoreM, rollingNewM);
+            rollingNewM = vec_shiftrightbytes1q(newM, SIMDBIGSHIFT8);
+            vec_store1q(M[i & 0x1] + jInnerBlock, toStoreM);
+            __m128i toStoreM2 = vec_shiftleftbytes1q(newM2, SIMDLITTLESHIFT8);
+            toStoreM2 = vec_bitor1q(toStoreM2, rollingNewM);
 
             __m128i checkM;
             if(DETECTREDUNDANT == true) {
-                __m128i checkMA = _mm_max_epu8(toStoreM, toStoreM2);
+                __m128i checkMA = vec_max16ub(toStoreM, toStoreM2);
                 // Signed comparison only (grr) so it can be...
-                checkM = _mm_cmpgt_epi8(checkMA, negGapOpenPlusgapExtensionSIMD);    // Greater than a small +ve constant or...
-                __m128i checkMSignedOtherWay = _mm_cmplt_epi8(checkMA, _mm_setzero_si128()); // >=128 and hence less than 0 in the signed world
-                checkM = _mm_or_si128(checkM, checkMSignedOtherWay);
+                checkM = vec_comparegt16sb(checkMA, negGapOpenPlusgapExtensionSIMD);    // Greater than a small +ve constant or...
+                __m128i checkMSignedOtherWay = vec_comparelt16sb(checkMA, vec_zero1q()); // >=128 and hence less than 0 in the signed world
+                checkM = vec_bitor1q(checkM, checkMSignedOtherWay);
             }
 
-            rollingNewM = _mm_srli_si128(newM2, SIMDBIGSHIFT8);
-            _mm_store_si128(M[i & 0x1] + jInnerBlock +  1, toStoreM2);
+            rollingNewM = vec_shiftrightbytes1q(newM2, SIMDBIGSHIFT8);
+            vec_store1q(M[i & 0x1] + jInnerBlock +  1, toStoreM2);
 
             if(DETECTREDUNDANT == true) {
-                int mask = _mm_movemask_epi8(checkM);
-                //_mm_store_si128(QueueNewMPlusOE + queueLength, toStoreMPlusoe);
+                int mask = vec_extractupperbit16sb(checkM);
+                //vec_store1q(QueueNewMPlusOE + queueLength, toStoreMPlusoe);
                 WorkQueue[queueLength] = jInnerBlock;
                 queueLength += (mask > 0);
             }
@@ -414,7 +415,7 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
 
         __m128i rollingNewHZeroes;
         if(VARIANT == 1 || VARIANT == 3) {
-            rollingNewHZeroes = _mm_setzero_si128();
+            rollingNewHZeroes = vec_zero1q();
         }
 
 
@@ -434,94 +435,94 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
         while(continueThirdBlock) {
             //// Load dependencies
 
-            __m128i newM = _mm_load_si128(M[i & 0x1] + jThirdBlock);
-            __m128i newMPlusOE = _mm_subs_epu8(newM, negGapOpenPlusgapExtensionSIMD);
-            __m128i newM2 = _mm_load_si128(M[i & 0x1] + jThirdBlock + 1);
-            __m128i newMPlusOE2 = _mm_subs_epu8(newM2, negGapOpenPlusgapExtensionSIMD);
+            __m128i newM = vec_load1q(M[i & 0x1] + jThirdBlock);
+            __m128i newMPlusOE = vec_subtractsaturating16ub(newM, negGapOpenPlusgapExtensionSIMD);
+            __m128i newM2 = vec_load1q(M[i & 0x1] + jThirdBlock + 1);
+            __m128i newMPlusOE2 = vec_subtractsaturating16ub(newM2, negGapOpenPlusgapExtensionSIMD);
 
             int ltMask, ltMask2;
             if(VARIANT == 1 || VARIANT == 2) {
                 // Roundabout signed less-than comparison
-                __m128i optCheck = _mm_min_epu8(newM, optSIMD);
-                __m128i optCheck2 = _mm_min_epu8(newM2, optSIMD);
-                optCheck = _mm_cmpeq_epi8(optSIMD, optCheck);
-                optCheck2 = _mm_cmpeq_epi8(optSIMD, optCheck2);
+                __m128i optCheck = vec_min16ub(newM, optSIMD);
+                __m128i optCheck2 = vec_min16ub(newM2, optSIMD);
+                optCheck = vec_compareeq16sb(optSIMD, optCheck);
+                optCheck2 = vec_compareeq16sb(optSIMD, optCheck2);
 
-                ltMask = _mm_movemask_epi8(optCheck);
-                ltMask2 = _mm_movemask_epi8(optCheck2);
+                ltMask = vec_extractupperbit16sb(optCheck);
+                ltMask2 = vec_extractupperbit16sb(optCheck2);
             }
             if(VARIANT == 3 || VARIANT == 4) {
                 // Still need to check for saturation
-                __m128i optCheck = _mm_min_epu8(newM, thresholdSIMD);
-                __m128i optCheck2 = _mm_min_epu8(newM2, thresholdSIMD);
-                optCheck = _mm_cmpeq_epi8(optCheck, thresholdSIMD);
-                optCheck2 = _mm_cmpeq_epi8(optCheck2, thresholdSIMD);
+                __m128i optCheck = vec_min16ub(newM, thresholdSIMD);
+                __m128i optCheck2 = vec_min16ub(newM2, thresholdSIMD);
+                optCheck = vec_compareeq16sb(optCheck, thresholdSIMD);
+                optCheck2 = vec_compareeq16sb(optCheck2, thresholdSIMD);
 
-                ltMask = _mm_movemask_epi8(optCheck);
-                ltMask2 = _mm_movemask_epi8(optCheck2);
+                ltMask = vec_extractupperbit16sb(optCheck);
+                ltMask2 = vec_extractupperbit16sb(optCheck2);
             }
 
             __m128i rollingNewH = rollingNewHZeroes;
 
-            __m128i oldH = _mm_subs_epu8(rollingNewH, negGapExtensionSIMD);
-            __m128i newH = _mm_max_epu8(oldH, newMPlusOE);
+            __m128i oldH = vec_subtractsaturating16ub(rollingNewH, negGapExtensionSIMD);
+            __m128i newH = vec_max16ub(oldH, newMPlusOE);
 
-            __m128i newH8Shift = _mm_subs_epu8(newH, negGapExtensionTimes16SIMD);
-            __m128i newH2 = _mm_max_epu8(newMPlusOE2, newH8Shift);
+            __m128i newH8Shift = vec_subtractsaturating16ub(newH, negGapExtensionTimes16SIMD);
+            __m128i newH2 = vec_max16ub(newMPlusOE2, newH8Shift);
 
 
             //// Propogate Hs
 
-            __m128i newH4Shift = _mm_subs_epu8(newH, negGapExtensionTimes8SIMD);
-            __m128i newH4Shift2 = _mm_subs_epu8(newH2, negGapExtensionTimes8SIMD);
-            __m128i inbetweenH4Shift = _mm_srli_si128(newH4Shift, 8);
-            newH4Shift = _mm_slli_si128(newH4Shift, 8);
-            newH4Shift2 = _mm_slli_si128(newH4Shift2, 8);
-            newH = _mm_max_epu8(newH, newH4Shift);
-            newH2 = _mm_max_epu8(newH2, newH4Shift2);
-            newH2 = _mm_max_epu8(newH2, inbetweenH4Shift);
-
-            __m128i newH2Shift = _mm_subs_epu8(newH, negGapExtensionTimes4SIMD);
-            __m128i newH2Shift2 = _mm_subs_epu8(newH2, negGapExtensionTimes4SIMD);
-            __m128i inbetweenH2Shift = _mm_srli_si128(newH2Shift, 12);
-            newH2Shift = _mm_slli_si128(newH2Shift, 4);
-            newH2Shift2 = _mm_slli_si128(newH2Shift2, 4);
-            newH = _mm_max_epu8(newH, newH2Shift);
-            newH2 = _mm_max_epu8(newH2, newH2Shift2);
-            newH2 = _mm_max_epu8(newH2, inbetweenH2Shift);
-
-            __m128i newH1Shift = _mm_subs_epu8(newH, negGapExtensionTimes2SIMD);
-            __m128i newH1Shift2 = _mm_subs_epu8(newH2, negGapExtensionTimes2SIMD);
-            __m128i inbetweenH1Shift = _mm_srli_si128(newH1Shift, 14);
-            newH1Shift = _mm_slli_si128(newH1Shift, 2);
-            newH1Shift2 = _mm_slli_si128(newH1Shift2, 2);
-            newH = _mm_max_epu8(newH, newH1Shift);
-            newH2 = _mm_max_epu8(newH2, newH1Shift2);
-            newH2 = _mm_max_epu8(newH2, inbetweenH1Shift);
-
-
-            __m128i newH05Shift = _mm_subs_epu8(newH, negGapExtensionSIMD);
-            __m128i newH05Shift2 = _mm_subs_epu8(newH2, negGapExtensionSIMD);
-            __m128i inbetweenH05Shift = _mm_srli_si128(newH05Shift, 15);
-            newH05Shift = _mm_slli_si128(newH05Shift, 1);
-            newH05Shift2 = _mm_slli_si128(newH05Shift2, 1);
-            newH = _mm_max_epu8(newH, newH05Shift);
-            newH2 = _mm_max_epu8(newH2, newH05Shift2);
-            newH2 = _mm_max_epu8(newH2, inbetweenH05Shift);
+            __m128i newH4Shift = vec_subtractsaturating16ub(newH, negGapExtensionTimes8SIMD);
+            __m128i newH4Shift2 = vec_subtractsaturating16ub(newH2, negGapExtensionTimes8SIMD);
+            __m128i inbetweenH4Shift = vec_shiftrightbytes1q(newH4Shift, 8);
+            newH4Shift = vec_shiftleftbytes1q(newH4Shift, 8);
+            newH4Shift2 = vec_shiftleftbytes1q(newH4Shift2, 8);
+            newH = vec_max16ub(newH, newH4Shift);
+            newH2 = vec_max16ub(newH2, newH4Shift2);
+            newH2 = vec_max16ub(newH2, inbetweenH4Shift);
+
+            __m128i newH2Shift = vec_subtractsaturating16ub(newH, negGapExtensionTimes4SIMD);
+            __m128i newH2Shift2 = vec_subtractsaturating16ub(newH2, negGapExtensionTimes4SIMD);
+            __m128i inbetweenH2Shift = vec_shiftrightbytes1q(newH2Shift, 12);
+            newH2Shift = vec_shiftleftbytes1q(newH2Shift, 4);
+            newH2Shift2 = vec_shiftleftbytes1q(newH2Shift2, 4);
+            newH = vec_max16ub(newH, newH2Shift);
+            newH2 = vec_max16ub(newH2, newH2Shift2);
+            newH2 = vec_max16ub(newH2, inbetweenH2Shift);
+
+            __m128i newH1Shift = vec_subtractsaturating16ub(newH, negGapExtensionTimes2SIMD);
+            __m128i newH1Shift2 = vec_subtractsaturating16ub(newH2, negGapExtensionTimes2SIMD);
+            __m128i inbetweenH1Shift = vec_shiftrightbytes1q(newH1Shift, 14);
+            newH1Shift = vec_shiftleftbytes1q(newH1Shift, 2);
+            newH1Shift2 = vec_shiftleftbytes1q(newH1Shift2, 2);
+            newH = vec_max16ub(newH, newH1Shift);
+            newH2 = vec_max16ub(newH2, newH1Shift2);
+            newH2 = vec_max16ub(newH2, inbetweenH1Shift);
+
+
+            __m128i newH05Shift = vec_subtractsaturating16ub(newH, negGapExtensionSIMD);
+            __m128i newH05Shift2 = vec_subtractsaturating16ub(newH2, negGapExtensionSIMD);
+            __m128i inbetweenH05Shift = vec_shiftrightbytes1q(newH05Shift, 15);
+            newH05Shift = vec_shiftleftbytes1q(newH05Shift, 1);
+            newH05Shift2 = vec_shiftleftbytes1q(newH05Shift2, 1);
+            newH = vec_max16ub(newH, newH05Shift);
+            newH2 = vec_max16ub(newH2, newH05Shift2);
+            newH2 = vec_max16ub(newH2, inbetweenH05Shift);
 
 
             //// Store new Hs
 
-            __m128i toStoreH = _mm_slli_si128(newH, SIMDLITTLESHIFT8);
-            __m128i toStoreH2 = _mm_slli_si128(newH2, SIMDLITTLESHIFT8);
-            toStoreH = _mm_or_si128(toStoreH, rollingNewHZeroes);
-            rollingNewHZeroes = _mm_srli_si128(newH, SIMDBIGSHIFT8);
-            toStoreH2 = _mm_or_si128(toStoreH2, rollingNewHZeroes);
-            rollingNewHZeroes = _mm_srli_si128(newH2, SIMDBIGSHIFT8);;
+            __m128i toStoreH = vec_shiftleftbytes1q(newH, SIMDLITTLESHIFT8);
+            __m128i toStoreH2 = vec_shiftleftbytes1q(newH2, SIMDLITTLESHIFT8);
+            toStoreH = vec_bitor1q(toStoreH, rollingNewHZeroes);
+            rollingNewHZeroes = vec_shiftrightbytes1q(newH, SIMDBIGSHIFT8);
+            toStoreH2 = vec_bitor1q(toStoreH2, rollingNewHZeroes);
+            rollingNewHZeroes = vec_shiftrightbytes1q(newH2, SIMDBIGSHIFT8);;
 
 
-            _mm_store_si128(H[i & 0x1] + jThirdBlock, toStoreH);
-            _mm_store_si128(H[i & 0x1] + jThirdBlock + 1, toStoreH2);
+            vec_store1q(H[i & 0x1] + jThirdBlock, toStoreH);
+            vec_store1q(H[i & 0x1] + jThirdBlock + 1, toStoreH2);
 
 
             if(VARIANT == 1 || VARIANT == 2) {
@@ -530,7 +531,7 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
                     if(ltMask != 0x0000) ExtractOpts8<DIRECTION>(newM, ltMask, i, (jThirdBlock * SIMDMULTIPLE8));
                     if(ltMask2 != 0x0000) ExtractOpts8<DIRECTION>(newM2, ltMask2, i, ((jThirdBlock + 1) * SIMDMULTIPLE8));
                     assert(Opt >= 0 && Opt < 256);
-                    optSIMD = _mm_set1_epi8(Opt & 0xFF);
+                    optSIMD = vec_splat16sb(Opt & 0xFF);
                 }
             }
             if(VARIANT == 3 || VARIANT == 4) {
@@ -561,7 +562,7 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
             }
 
             if(DETECTREDUNDANT == true) {
-                int nextH = _mm_extract_epi16(rollingNewHZeroes, 0) & 0xFF;
+                int nextH = vec_extract8sh(rollingNewHZeroes, 0) & 0xFF;
                 q++;
                 int nextJThirdBlock = jThirdBlock + 2;
                 jThirdBlock = WorkQueue[q];
@@ -584,32 +585,32 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
     if(VARIANT == 3 || VARIANT == 4) {
         int jOptCheckBlock = 0;
         do {
-            __m128i finalM = _mm_load_si128(M[QuerySize & 0x1] + jOptCheckBlock);
-            __m128i finalV = _mm_load_si128(V[QuerySize & 0x1] + jOptCheckBlock);
-            __m128i finalH = _mm_load_si128(H[QuerySize & 0x1] + jOptCheckBlock);
-            __m128i maxMVH = _mm_max_epu8(_mm_max_epu8(finalM, finalV), finalH);
+            __m128i finalM = vec_load1q(M[QuerySize & 0x1] + jOptCheckBlock);
+            __m128i finalV = vec_load1q(V[QuerySize & 0x1] + jOptCheckBlock);
+            __m128i finalH = vec_load1q(H[QuerySize & 0x1] + jOptCheckBlock);
+            __m128i maxMVH = vec_max16ub(vec_max16ub(finalM, finalV), finalH);
 
-            __m128i finalM2 = _mm_load_si128(M[QuerySize & 0x1] + jOptCheckBlock + 1);
-            __m128i finalV2 = _mm_load_si128(V[QuerySize & 0x1] + jOptCheckBlock + 1);
-            __m128i finalH2 = _mm_load_si128(H[QuerySize & 0x1] + jOptCheckBlock + 1);
-            __m128i maxMVH2 = _mm_max_epu8(_mm_max_epu8(finalM2, finalV2), finalH2);
+            __m128i finalM2 = vec_load1q(M[QuerySize & 0x1] + jOptCheckBlock + 1);
+            __m128i finalV2 = vec_load1q(V[QuerySize & 0x1] + jOptCheckBlock + 1);
+            __m128i finalH2 = vec_load1q(H[QuerySize & 0x1] + jOptCheckBlock + 1);
+            __m128i maxMVH2 = vec_max16ub(vec_max16ub(finalM2, finalV2), finalH2);
 
 
-            __m128i optCheck = _mm_min_epu8(maxMVH, optSIMD);
-            __m128i optCheck2 = _mm_min_epu8(maxMVH2, optSIMD);
+            __m128i optCheck = vec_min16ub(maxMVH, optSIMD);
+            __m128i optCheck2 = vec_min16ub(maxMVH2, optSIMD);
 
-            optCheck = _mm_cmpeq_epi8(optSIMD, optCheck);
-            optCheck2 = _mm_cmpeq_epi8(optSIMD, optCheck2);
+            optCheck = vec_compareeq16sb(optSIMD, optCheck);
+            optCheck2 = vec_compareeq16sb(optSIMD, optCheck2);
 
-            int ltMask = _mm_movemask_epi8(optCheck);
-            int ltMask2 = _mm_movemask_epi8(optCheck2);
+            int ltMask = vec_extractupperbit16sb(optCheck);
+            int ltMask2 = vec_extractupperbit16sb(optCheck2);
 
             if((ltMask | ltMask2) != 0x0000) {
                 // Order is important because of DIRECTION
                 if(ltMask != 0x0000) ExtractOpts8<DIRECTION>(maxMVH, ltMask, QuerySize, (jOptCheckBlock * SIMDMULTIPLE8));
                 if(ltMask2 != 0x0000) ExtractOpts8<DIRECTION>(maxMVH2, ltMask2, QuerySize, (jOptCheckBlock + 1) * SIMDMULTIPLE8);
                 assert(Opt >= 0 && Opt < 256);
-                optSIMD = _mm_set1_epi8(Opt & 0xFF);
+                optSIMD = vec_splat16sb(Opt & 0xFF);
             }
 
             jOptCheckBlock += 2;
@@ -634,23 +635,23 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
     MHVTRealType TargetLookup = TargetLookupData;
     //__m128i* QueueNewMPlusOE = (__m128i*)QueueNewMPlusOEData;
 
-    __m128i optSIMD = _mm_set1_epi16(Opt & 0xFF);
+    __m128i optSIMD = vec_splat8sh(Opt & 0xFF);
 
-    const __m128i gapExtensionSIMD = _mm_set1_epi16(GapExtension);
-    const __m128i gapOpenPlusgapExtensionSIMD = _mm_set1_epi16(GapOpen + GapExtension);
-    const __m128i negGapOpenPlusgapExtensionSIMD = _mm_set1_epi16(-(GapOpen + GapExtension));
-    const __m128i twiceInfinitesPlusOESIMD = _mm_add_epi16(gapOpenPlusgapExtensionSIMD, TwiceInfinitesSIMD16);
-    const __m128i gapExtensionTimes8SIMD = _mm_slli_epi16(gapExtensionSIMD, 3);
-    const __m128i gapExtensionTimes4SIMD = _mm_slli_epi16(gapExtensionSIMD, 2);
-    const __m128i gapExtensionTimes2SIMD = _mm_slli_epi16(gapExtensionSIMD, 1);
+    const __m128i gapExtensionSIMD = vec_splat8sh(GapExtension);
+    const __m128i gapOpenPlusgapExtensionSIMD = vec_splat8sh(GapOpen + GapExtension);
+    const __m128i negGapOpenPlusgapExtensionSIMD = vec_splat8sh(-(GapOpen + GapExtension));
+    const __m128i twiceInfinitesPlusOESIMD = vec_add8sh(gapOpenPlusgapExtensionSIMD, TwiceInfinitesSIMD16);
+    const __m128i gapExtensionTimes8SIMD = vec_shiftleftimmediate8sh(gapExtensionSIMD, 3);
+    const __m128i gapExtensionTimes4SIMD = vec_shiftleftimmediate8sh(gapExtensionSIMD, 2);
+    const __m128i gapExtensionTimes2SIMD = vec_shiftleftimmediate8sh(gapExtensionSIMD, 1);
 
 
 
     int jBlockSetup = 0;
     do {
-        _mm_store_si128(M[0] + jBlockSetup, _mm_setzero_si128());
-        _mm_store_si128(H[0] + jBlockSetup, NegInfiniteSIMD16);
-        _mm_store_si128(V[0] + jBlockSetup, NegInfiniteSIMD16);
+        vec_store1q(M[0] + jBlockSetup, vec_zero1q());
+        vec_store1q(H[0] + jBlockSetup, NegInfiniteSIMD16);
+        vec_store1q(V[0] + jBlockSetup, NegInfiniteSIMD16);
     } while(++jBlockSetup < (TargetSizeInSIMDBlocks + 2));
 
 
@@ -658,31 +659,31 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
     for(int i = 1; i <= QuerySize; ++i) {
         int jSecondBlock = 0;
         do {
-            __m128i newM = _mm_load_si128(M[(i - 1) & 0x1] + jSecondBlock);
-            __m128i newM2 = _mm_load_si128(M[(i - 1) & 0x1] + jSecondBlock + 1);
-            __m128i newMPlusOE = _mm_add_epi16(newM, gapOpenPlusgapExtensionSIMD);
-            __m128i newMPlusOE2 = _mm_add_epi16(newM2, gapOpenPlusgapExtensionSIMD);
-
-            __m128i oldV = _mm_load_si128(V[(i - 1) & 0x1] + jSecondBlock);
-            __m128i oldV2 = _mm_load_si128(V[(i - 1) & 0x1] + jSecondBlock + 1);
-            oldV = _mm_add_epi16(oldV, gapExtensionSIMD);
-            oldV2 = _mm_add_epi16(oldV2, gapExtensionSIMD);
-            __m128i newV = _mm_max_epi16(oldV, newMPlusOE);
-            __m128i newV2 = _mm_max_epi16(oldV2, newMPlusOE2);
-            _mm_store_si128(V[i & 0x1] + jSecondBlock, newV);
-            _mm_store_si128(V[i & 0x1] + jSecondBlock + 1, newV2);
+            __m128i newM = vec_load1q(M[(i - 1) & 0x1] + jSecondBlock);
+            __m128i newM2 = vec_load1q(M[(i - 1) & 0x1] + jSecondBlock + 1);
+            __m128i newMPlusOE = vec_add8sh(newM, gapOpenPlusgapExtensionSIMD);
+            __m128i newMPlusOE2 = vec_add8sh(newM2, gapOpenPlusgapExtensionSIMD);
+
+            __m128i oldV = vec_load1q(V[(i - 1) & 0x1] + jSecondBlock);
+            __m128i oldV2 = vec_load1q(V[(i - 1) & 0x1] + jSecondBlock + 1);
+            oldV = vec_add8sh(oldV, gapExtensionSIMD);
+            oldV2 = vec_add8sh(oldV2, gapExtensionSIMD);
+            __m128i newV = vec_max8sh(oldV, newMPlusOE);
+            __m128i newV2 = vec_max8sh(oldV2, newMPlusOE2);
+            vec_store1q(V[i & 0x1] + jSecondBlock, newV);
+            vec_store1q(V[i & 0x1] + jSecondBlock + 1, newV2);
 
             if(DETECTREDUNDANT == true) {
                 // XXX do only when needed
-                _mm_store_si128(H[i & 0x1] + jSecondBlock, _mm_setzero_si128());
-                _mm_store_si128(H[i & 0x1] + jSecondBlock + 1, _mm_setzero_si128());
+                vec_store1q(H[i & 0x1] + jSecondBlock, vec_zero1q());
+                vec_store1q(H[i & 0x1] + jSecondBlock + 1, vec_zero1q());
             }
 
             jSecondBlock += 2;
         } while(jSecondBlock < TargetSizeInSIMDBlocks + 1);
-        __m128i fixupV = _mm_load_si128(V[i & 0x1] + 0);
-        fixupV = _mm_insert_epi16(fixupV, -INFINITE16, 0);
-        _mm_store_si128(V[i & 0x1] + 0, fixupV);
+        __m128i fixupV = vec_load1q(V[i & 0x1] + 0);
+        fixupV = vec_insert8sh(fixupV, -INFINITE16, 0);
+        vec_store1q(V[i & 0x1] + 0, fixupV);
 
 
 
@@ -696,7 +697,7 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
 
         __m128i rollingNewM;
         if(VARIANT == 1 || VARIANT == 3) {
-            rollingNewM = _mm_setzero_si128();
+            rollingNewM = vec_zero1q();
         }
         if(VARIANT == 2 || VARIANT == 4) {
             rollingNewM = NegInfiniteSIMDShiftedBig16;
@@ -706,47 +707,47 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
 
         int jInnerBlock = 0;
         do {
-            __m128i thisV = _mm_load_si128(V[(i - 1) & 0x1] + jInnerBlock);
-            __m128i thisH = _mm_load_si128(H[(i - 1) & 0x1] + jInnerBlock);
-            __m128i thisM = _mm_load_si128(M[(i - 1) & 0x1] + jInnerBlock);
-            __m128i mx = _mm_max_epi16(thisV, thisH);
-            mx = _mm_max_epi16(mx, thisM);
-            __m128i matches = _mm_load_si128(targetLookupCurrent + jInnerBlock);
-            __m128i newM = _mm_add_epi16(mx, matches);
-
-
-            __m128i thisV2 = _mm_load_si128(V[(i - 1) & 0x1] + jInnerBlock + 1);
-            __m128i thisH2 = _mm_load_si128(H[(i - 1) & 0x1] + jInnerBlock + 1);
-            __m128i thisM2 = _mm_load_si128(M[(i - 1) & 0x1] + jInnerBlock  + 1);                
-            __m128i mx2 = _mm_max_epi16(thisV2, thisH2);
-            mx2 = _mm_max_epi16(mx2, thisM2);
-            __m128i matches2 = _mm_load_si128(targetLookupCurrent + jInnerBlock + 1);
-            __m128i newM2 = _mm_add_epi16(mx2, matches2);
+            __m128i thisV = vec_load1q(V[(i - 1) & 0x1] + jInnerBlock);
+            __m128i thisH = vec_load1q(H[(i - 1) & 0x1] + jInnerBlock);
+            __m128i thisM = vec_load1q(M[(i - 1) & 0x1] + jInnerBlock);
+            __m128i mx = vec_max8sh(thisV, thisH);
+            mx = vec_max8sh(mx, thisM);
+            __m128i matches = vec_load1q(targetLookupCurrent + jInnerBlock);
+            __m128i newM = vec_add8sh(mx, matches);
+
+
+            __m128i thisV2 = vec_load1q(V[(i - 1) & 0x1] + jInnerBlock + 1);
+            __m128i thisH2 = vec_load1q(H[(i - 1) & 0x1] + jInnerBlock + 1);
+            __m128i thisM2 = vec_load1q(M[(i - 1) & 0x1] + jInnerBlock  + 1);                
+            __m128i mx2 = vec_max8sh(thisV2, thisH2);
+            mx2 = vec_max8sh(mx2, thisM2);
+            __m128i matches2 = vec_load1q(targetLookupCurrent + jInnerBlock + 1);
+            __m128i newM2 = vec_add8sh(mx2, matches2);
 
             if(VARIANT == 1 || VARIANT == 3) {
-                newM = _mm_max_epi16(newM, _mm_setzero_si128());
-                newM2 = _mm_max_epi16(newM2, _mm_setzero_si128());
+                newM = vec_max8sh(newM, vec_zero1q());
+                newM2 = vec_max8sh(newM2, vec_zero1q());
             }
 
-            __m128i toStoreM = _mm_slli_si128(newM, SIMDLITTLESHIFT16);
-            toStoreM = _mm_or_si128(toStoreM, rollingNewM);
-            rollingNewM = _mm_srli_si128(newM, SIMDBIGSHIFT16);
-            _mm_store_si128(M[i & 0x1] + jInnerBlock, toStoreM);
-            __m128i toStoreM2 = _mm_slli_si128(newM2, SIMDLITTLESHIFT16);
-            toStoreM2 = _mm_or_si128(toStoreM2, rollingNewM);
+            __m128i toStoreM = vec_shiftleftbytes1q(newM, SIMDLITTLESHIFT16);
+            toStoreM = vec_bitor1q(toStoreM, rollingNewM);
+            rollingNewM = vec_shiftrightbytes1q(newM, SIMDBIGSHIFT16);
+            vec_store1q(M[i & 0x1] + jInnerBlock, toStoreM);
+            __m128i toStoreM2 = vec_shiftleftbytes1q(newM2, SIMDLITTLESHIFT16);
+            toStoreM2 = vec_bitor1q(toStoreM2, rollingNewM);
 
             __m128i checkM;
             if(DETECTREDUNDANT == true) {
-                checkM = _mm_max_epi16(toStoreM, toStoreM2);
-                checkM = _mm_cmpgt_epi16(checkM, negGapOpenPlusgapExtensionSIMD);
+                checkM = vec_max8sh(toStoreM, toStoreM2);
+                checkM = vec_comparegt8sh(checkM, negGapOpenPlusgapExtensionSIMD);
             }
 
-            rollingNewM = _mm_srli_si128(newM2, SIMDBIGSHIFT16);
-            _mm_store_si128(M[i & 0x1] + jInnerBlock +  1, toStoreM2);
+            rollingNewM = vec_shiftrightbytes1q(newM2, SIMDBIGSHIFT16);
+            vec_store1q(M[i & 0x1] + jInnerBlock +  1, toStoreM2);
 
             if(DETECTREDUNDANT == true) {
-                int mask = _mm_movemask_epi8(checkM);
-                //_mm_store_si128(QueueNewMPlusOE + queueLength, toStoreMPlusoe);
+                int mask = vec_extractupperbit16sb(checkM);
+                //vec_store1q(QueueNewMPlusOE + queueLength, toStoreMPlusoe);
                 WorkQueue[queueLength] = jInnerBlock;
                 queueLength += (mask > 0);
             }
@@ -760,10 +761,10 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
 
         __m128i rollingNewHZeroes;
         if(VARIANT == 1 || VARIANT == 3) {
-            rollingNewHZeroes = _mm_insert_epi16(_mm_setzero_si128(), -INFINITE16 + INFINITE16 * 2, 0);
+            rollingNewHZeroes = vec_insert8sh(vec_zero1q(), -INFINITE16 + INFINITE16 * 2, 0);
         }
         if(VARIANT == 2 || VARIANT == 4) {
-            rollingNewHZeroes = _mm_insert_epi16(_mm_setzero_si128(), GapOpen + (GapExtension * i) + INFINITE16 * 2, 0);
+            rollingNewHZeroes = vec_insert8sh(vec_zero1q(), GapOpen + (GapExtension * i) + INFINITE16 * 2, 0);
         }
 
         bool continueThirdBlock;
@@ -782,73 +783,73 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
             //// Load dependencies, Bias everything 2 * INFINITE16 so shifted in zeroes will always vanish after max()
             // XXX could get rid of the bias for variants 1 and 3
 
-            __m128i newM = _mm_load_si128(M[i & 0x1] + jThirdBlock);
-            __m128i newMPlusOE = _mm_add_epi16(newM, twiceInfinitesPlusOESIMD);
-            __m128i newM2 = _mm_load_si128(M[i & 0x1] + jThirdBlock + 1);
-            __m128i newMPlusOE2 = _mm_add_epi16(newM2, twiceInfinitesPlusOESIMD);    
+            __m128i newM = vec_load1q(M[i & 0x1] + jThirdBlock);
+            __m128i newMPlusOE = vec_add8sh(newM, twiceInfinitesPlusOESIMD);
+            __m128i newM2 = vec_load1q(M[i & 0x1] + jThirdBlock + 1);
+            __m128i newMPlusOE2 = vec_add8sh(newM2, twiceInfinitesPlusOESIMD);    
 
             int ltMask, ltMask2;
             if(VARIANT == 1 || VARIANT == 2) {
-                __m128i optCheck = _mm_cmplt_epi16(newM, optSIMD);
-                __m128i optCheck2 = _mm_cmplt_epi16(newM2, optSIMD);
-                ltMask = _mm_movemask_epi8(optCheck);
-                ltMask2 = _mm_movemask_epi8(optCheck2);
+                __m128i optCheck = vec_comparelt8sh(newM, optSIMD);
+                __m128i optCheck2 = vec_comparelt8sh(newM2, optSIMD);
+                ltMask = vec_extractupperbit16sb(optCheck);
+                ltMask2 = vec_extractupperbit16sb(optCheck2);
             }
 
             __m128i rollingNewH = rollingNewHZeroes;
 
-            __m128i oldH = _mm_add_epi16(rollingNewH, gapExtensionSIMD);
-            __m128i newH = _mm_max_epi16(oldH, newMPlusOE);
+            __m128i oldH = vec_add8sh(rollingNewH, gapExtensionSIMD);
+            __m128i newH = vec_max8sh(oldH, newMPlusOE);
 
-            __m128i newH8Shift = _mm_add_epi16(newH, gapExtensionTimes8SIMD);
-            __m128i newH2 = _mm_max_epi16(newMPlusOE2, newH8Shift);
+            __m128i newH8Shift = vec_add8sh(newH, gapExtensionTimes8SIMD);
+            __m128i newH2 = vec_max8sh(newMPlusOE2, newH8Shift);
 
 
             //// Propogate Hs
 
-            __m128i newH4Shift = _mm_add_epi16(newH, gapExtensionTimes4SIMD);
-            __m128i newH4Shift2 = _mm_add_epi16(newH2, gapExtensionTimes4SIMD);
-            __m128i inbetweenH4Shift = _mm_srli_si128(newH4Shift, 8);
-            newH4Shift = _mm_slli_si128(newH4Shift, 8);
-            newH4Shift2 = _mm_slli_si128(newH4Shift2, 8);
-            newH = _mm_max_epi16(newH, newH4Shift);
-            newH2 = _mm_max_epi16(newH2, newH4Shift2);
-            newH2 = _mm_max_epi16(newH2, inbetweenH4Shift);
-
-            __m128i newH2Shift = _mm_add_epi16(newH, gapExtensionTimes2SIMD);
-            __m128i newH2Shift2 = _mm_add_epi16(newH2, gapExtensionTimes2SIMD);
-            __m128i inbetweenH2Shift = _mm_srli_si128(newH2Shift, 12);
-            newH2Shift = _mm_slli_si128(newH2Shift, 4);
-            newH2Shift2 = _mm_slli_si128(newH2Shift2, 4);
-            newH = _mm_max_epi16(newH, newH2Shift);
-            newH2 = _mm_max_epi16(newH2, newH2Shift2);
-            newH2 = _mm_max_epi16(newH2, inbetweenH2Shift);
-
-            __m128i newH1Shift = _mm_add_epi16(newH, gapExtensionSIMD);
-            __m128i newH1Shift2 = _mm_add_epi16(newH2, gapExtensionSIMD);
-            __m128i inbetweenH1Shift = _mm_srli_si128(newH1Shift, 14);
-            newH1Shift = _mm_slli_si128(newH1Shift, 2);
-            newH1Shift2 = _mm_slli_si128(newH1Shift2, 2);
-            newH = _mm_max_epi16(newH, newH1Shift);
-            newH2 = _mm_max_epi16(newH2, newH1Shift2);
-            newH2 = _mm_max_epi16(newH2, inbetweenH1Shift);
+            __m128i newH4Shift = vec_add8sh(newH, gapExtensionTimes4SIMD);
+            __m128i newH4Shift2 = vec_add8sh(newH2, gapExtensionTimes4SIMD);
+            __m128i inbetweenH4Shift = vec_shiftrightbytes1q(newH4Shift, 8);
+            newH4Shift = vec_shiftleftbytes1q(newH4Shift, 8);
+            newH4Shift2 = vec_shiftleftbytes1q(newH4Shift2, 8);
+            newH = vec_max8sh(newH, newH4Shift);
+            newH2 = vec_max8sh(newH2, newH4Shift2);
+            newH2 = vec_max8sh(newH2, inbetweenH4Shift);
+
+            __m128i newH2Shift = vec_add8sh(newH, gapExtensionTimes2SIMD);
+            __m128i newH2Shift2 = vec_add8sh(newH2, gapExtensionTimes2SIMD);
+            __m128i inbetweenH2Shift = vec_shiftrightbytes1q(newH2Shift, 12);
+            newH2Shift = vec_shiftleftbytes1q(newH2Shift, 4);
+            newH2Shift2 = vec_shiftleftbytes1q(newH2Shift2, 4);
+            newH = vec_max8sh(newH, newH2Shift);
+            newH2 = vec_max8sh(newH2, newH2Shift2);
+            newH2 = vec_max8sh(newH2, inbetweenH2Shift);
+
+            __m128i newH1Shift = vec_add8sh(newH, gapExtensionSIMD);
+            __m128i newH1Shift2 = vec_add8sh(newH2, gapExtensionSIMD);
+            __m128i inbetweenH1Shift = vec_shiftrightbytes1q(newH1Shift, 14);
+            newH1Shift = vec_shiftleftbytes1q(newH1Shift, 2);
+            newH1Shift2 = vec_shiftleftbytes1q(newH1Shift2, 2);
+            newH = vec_max8sh(newH, newH1Shift);
+            newH2 = vec_max8sh(newH2, newH1Shift2);
+            newH2 = vec_max8sh(newH2, inbetweenH1Shift);
 
 
             //// Store new Hs, unbias by 2 * INFINITE16
 
-            __m128i toStoreH = _mm_slli_si128(newH, SIMDLITTLESHIFT16);
-            __m128i toStoreH2 = _mm_slli_si128(newH2, SIMDLITTLESHIFT16);
-            toStoreH = _mm_or_si128(toStoreH, rollingNewHZeroes);
-            rollingNewHZeroes = _mm_srli_si128(newH, SIMDBIGSHIFT16);
-            toStoreH2 = _mm_or_si128(toStoreH2, rollingNewHZeroes);
-            rollingNewHZeroes = _mm_srli_si128(newH2, SIMDBIGSHIFT16);;
+            __m128i toStoreH = vec_shiftleftbytes1q(newH, SIMDLITTLESHIFT16);
+            __m128i toStoreH2 = vec_shiftleftbytes1q(newH2, SIMDLITTLESHIFT16);
+            toStoreH = vec_bitor1q(toStoreH, rollingNewHZeroes);
+            rollingNewHZeroes = vec_shiftrightbytes1q(newH, SIMDBIGSHIFT16);
+            toStoreH2 = vec_bitor1q(toStoreH2, rollingNewHZeroes);
+            rollingNewHZeroes = vec_shiftrightbytes1q(newH2, SIMDBIGSHIFT16);;
 
-            toStoreH = _mm_sub_epi16(toStoreH, TwiceInfinitesSIMD16);
-            toStoreH2 = _mm_sub_epi16(toStoreH2, TwiceInfinitesSIMD16);
+            toStoreH = vec_subtract8sh(toStoreH, TwiceInfinitesSIMD16);
+            toStoreH2 = vec_subtract8sh(toStoreH2, TwiceInfinitesSIMD16);
 
 
-            _mm_store_si128(H[i & 0x1] + jThirdBlock, toStoreH);
-            _mm_store_si128(H[i & 0x1] + jThirdBlock + 1, toStoreH2);
+            vec_store1q(H[i & 0x1] + jThirdBlock, toStoreH);
+            vec_store1q(H[i & 0x1] + jThirdBlock + 1, toStoreH2);
 
 
             if(VARIANT == 1 || VARIANT == 2) {
@@ -856,13 +857,13 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
                     // Order is important because of DIRECTION
                     if(ltMask != 0xFFFF) ExtractOpts16<DIRECTION>(newM, ltMask, i, (jThirdBlock * SIMDMULTIPLE16));
                     if(ltMask2 != 0xFFFF) ExtractOpts16<DIRECTION>(newM2, ltMask2, i, ((jThirdBlock + 1) * SIMDMULTIPLE16));
-                    optSIMD = _mm_set1_epi16(Opt);
+                    optSIMD = vec_splat8sh(Opt);
                 }
             }
 
 
             if(DETECTREDUNDANT == true) {
-                int nextH = _mm_extract_epi16(rollingNewHZeroes, 0);
+                int nextH = vec_extract8sh(rollingNewHZeroes, 0);
                 q++;
                 int nextJThirdBlock = jThirdBlock + 2;
                 jThirdBlock = WorkQueue[q];
@@ -886,26 +887,26 @@ template<int VARIANT, bool DIRECTION, bool DETECTREDUNDANT> void Solution9::DoVa
     if(VARIANT == 3 || VARIANT == 4) {
         int jOptCheckBlock = 0;
         do {
-            __m128i finalM = _mm_load_si128(M[QuerySize & 0x1] + jOptCheckBlock);
-            __m128i finalV = _mm_load_si128(V[QuerySize & 0x1] + jOptCheckBlock);
-            __m128i finalH = _mm_load_si128(H[QuerySize & 0x1] + jOptCheckBlock);
-            __m128i maxMVH = _mm_max_epi16(_mm_max_epi16(finalM, finalV), finalH);
+            __m128i finalM = vec_load1q(M[QuerySize & 0x1] + jOptCheckBlock);
+            __m128i finalV = vec_load1q(V[QuerySize & 0x1] + jOptCheckBlock);
+            __m128i finalH = vec_load1q(H[QuerySize & 0x1] + jOptCheckBlock);
+            __m128i maxMVH = vec_max8sh(vec_max8sh(finalM, finalV), finalH);
 
-            __m128i finalM2 = _mm_load_si128(M[QuerySize & 0x1] + jOptCheckBlock + 1);
-            __m128i finalV2 = _mm_load_si128(V[QuerySize & 0x1] + jOptCheckBlock + 1);
-            __m128i finalH2 = _mm_load_si128(H[QuerySize & 0x1] + jOptCheckBlock + 1);
-            __m128i maxMVH2 = _mm_max_epi16(_mm_max_epi16(finalM2, finalV2), finalH2);
+            __m128i finalM2 = vec_load1q(M[QuerySize & 0x1] + jOptCheckBlock + 1);
+            __m128i finalV2 = vec_load1q(V[QuerySize & 0x1] + jOptCheckBlock + 1);
+            __m128i finalH2 = vec_load1q(H[QuerySize & 0x1] + jOptCheckBlock + 1);
+            __m128i maxMVH2 = vec_max8sh(vec_max8sh(finalM2, finalV2), finalH2);
 
-            __m128i optCheck = _mm_cmplt_epi16(maxMVH, optSIMD);
-            __m128i optCheck2 = _mm_cmplt_epi16(maxMVH2, optSIMD);
-            int ltMask = _mm_movemask_epi8(optCheck);
-            int ltMask2 = _mm_movemask_epi8(optCheck2);
+            __m128i optCheck = vec_comparelt8sh(maxMVH, optSIMD);
+            __m128i optCheck2 = vec_comparelt8sh(maxMVH2, optSIMD);
+            int ltMask = vec_extractupperbit16sb(optCheck);
+            int ltMask2 = vec_extractupperbit16sb(optCheck2);
 
             if((ltMask & ltMask2) != 0xFFFF) {
                 // Order is important because of DIRECTION
                 if(ltMask != 0xFFFF) ExtractOpts16<DIRECTION>(maxMVH, ltMask, QuerySize, (jOptCheckBlock * SIMDMULTIPLE16));
                 if(ltMask2 != 0xFFFF) ExtractOpts16<DIRECTION>(maxMVH2, ltMask2, QuerySize, (jOptCheckBlock + 1) * SIMDMULTIPLE16);
-                optSIMD = _mm_set1_epi16(Opt);
+                optSIMD = vec_splat8sh(Opt);
             }
 
             jOptCheckBlock += 2;
@@ -923,10 +924,10 @@ void Solution9::DoSetup8(const string& target, const string& query) {
     TargetSizeInSIMDBlocks = targetSizeRoundedUpForSIMD / SIMDMULTIPLE8;
 
     for(int jBlock = 0; jBlock < TargetSizeInSIMDBlocks + 1; jBlock += 8) {
-        _mm_prefetch(TargetLookup[0] + jBlock, _MM_HINT_T0);
-        _mm_prefetch(TargetLookup[1] + jBlock, _MM_HINT_T0);
-        _mm_prefetch(TargetLookup[2] + jBlock, _MM_HINT_T0);
-        _mm_prefetch(TargetLookup[3] + jBlock, _MM_HINT_T0);
+        vec_prefetch(TargetLookup[0] + jBlock, vec_HINT_T0);
+        vec_prefetch(TargetLookup[1] + jBlock, vec_HINT_T0);
+        vec_prefetch(TargetLookup[2] + jBlock, vec_HINT_T0);
+        vec_prefetch(TargetLookup[3] + jBlock, vec_HINT_T0);
     }
 
     // Mismatch in this table will equal zero, match will be the match score plus the magnitude of the mismatch, so all values are offset by mismtch and postitive
@@ -934,26 +935,26 @@ void Solution9::DoSetup8(const string& target, const string& query) {
     for(int i = 0; i < QuerySize; i++) QueryLookup[i] = CharMap[int(query[i])];
 
     for(int j = 0; j < TargetSizeInSIMDBlocks; j++) {
-        _mm_store_si128(TargetLookup[0] + j, _mm_setzero_si128());
-        _mm_store_si128(TargetLookup[1] + j, _mm_setzero_si128());
-        _mm_store_si128(TargetLookup[2] + j, _mm_setzero_si128());
-        _mm_store_si128(TargetLookup[3] + j, _mm_setzero_si128());
+        vec_store1q(TargetLookup[0] + j, vec_zero1q());
+        vec_store1q(TargetLookup[1] + j, vec_zero1q());
+        vec_store1q(TargetLookup[2] + j, vec_zero1q());
+        vec_store1q(TargetLookup[3] + j, vec_zero1q());
     }
 
     for(int j = 0; j < TargetSize; j++) ((unsigned char*)(TargetLookup[CharMap[int(target[j])]]))[j] = (unsigned char)(MatchScore - MismatchScore);
 
 
     for(int jBlock = 0; jBlock < TargetSizeInSIMDBlocks + 1; jBlock += 8) {
-        _mm_prefetch(((__m128i*)MData[0]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)HData[0]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)VData[0]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)MData[1]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)HData[1]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)VData[1]) + jBlock, _MM_HINT_T0);
+        vec_prefetch(((__m128i*)MData[0]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)HData[0]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)VData[0]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)MData[1]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)HData[1]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)VData[1]) + jBlock, vec_HINT_T0);
     }
 
     Opt = MINIMUMASSUMEDOPT;
-    //OptSIMD = _mm_set1_epi8(Opt);
+    //OptSIMD = vec_splat16sb(Opt);
     QueryEnd = TargetEnd = NBest = -1;
 }
 
@@ -964,34 +965,34 @@ void Solution9::DoSetup16(const string& target, const string& query) {
     TargetSizeInSIMDBlocks = targetSizeRoundedUpForSIMD / SIMDMULTIPLE16;
 
     for(int jBlock = 0; jBlock < TargetSizeInSIMDBlocks + 1; jBlock += 4) {
-        _mm_prefetch(TargetLookup[0] + jBlock, _MM_HINT_T0);
-        _mm_prefetch(TargetLookup[1] + jBlock, _MM_HINT_T0);
-        _mm_prefetch(TargetLookup[2] + jBlock, _MM_HINT_T0);
-        _mm_prefetch(TargetLookup[3] + jBlock, _MM_HINT_T0);
+        vec_prefetch(TargetLookup[0] + jBlock, vec_HINT_T0);
+        vec_prefetch(TargetLookup[1] + jBlock, vec_HINT_T0);
+        vec_prefetch(TargetLookup[2] + jBlock, vec_HINT_T0);
+        vec_prefetch(TargetLookup[3] + jBlock, vec_HINT_T0);
     }
 
     for(int i = 0; i < QuerySize; i++) QueryLookup[i] = CharMap[int(query[i])];
-    __m128i targetLookupBuilder = _mm_set1_epi16(MismatchScore);
+    __m128i targetLookupBuilder = vec_splat8sh(MismatchScore);
     for(int j = 0; j < TargetSizeInSIMDBlocks; j++) {
-        _mm_store_si128(TargetLookup[0] + j, targetLookupBuilder);
-        _mm_store_si128(TargetLookup[1] + j, targetLookupBuilder);
-        _mm_store_si128(TargetLookup[2] + j, targetLookupBuilder);
-        _mm_store_si128(TargetLookup[3] + j, targetLookupBuilder);
+        vec_store1q(TargetLookup[0] + j, targetLookupBuilder);
+        vec_store1q(TargetLookup[1] + j, targetLookupBuilder);
+        vec_store1q(TargetLookup[2] + j, targetLookupBuilder);
+        vec_store1q(TargetLookup[3] + j, targetLookupBuilder);
     }
 
     for(int j = 0; j < TargetSize; j++) ((short int*)(TargetLookup[CharMap[int(target[j])]]))[j] = MatchScore;
 
     for(int jBlock = 0; jBlock < TargetSizeInSIMDBlocks + 1; jBlock += 4) {
-        _mm_prefetch(((__m128i*)MData[0]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)HData[0]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)VData[0]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)MData[1]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)HData[1]) + jBlock, _MM_HINT_T0);
-        _mm_prefetch(((__m128i*)VData[1]) + jBlock, _MM_HINT_T0);
+        vec_prefetch(((__m128i*)MData[0]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)HData[0]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)VData[0]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)MData[1]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)HData[1]) + jBlock, vec_HINT_T0);
+        vec_prefetch(((__m128i*)VData[1]) + jBlock, vec_HINT_T0);
     }
 
     Opt = MINIMUMASSUMEDOPT;
-    //OptSIMD = _mm_set1_epi16(Opt);
+    //OptSIMD = vec_splat8sh(Opt);
     QueryEnd = TargetEnd = NBest = -1;
 
 }
--- src/sw/lib/sw-vector.cpp
+++ src/sw/lib/sw-vector.cpp
@@ -12,10 +12,7 @@
 #include <unistd.h>
 #include <zlib.h>
 
-#include <mmintrin.h>	/* MMX */
-#include <xmmintrin.h>	/* SSE */
-#include <emmintrin.h>	/* SSE2 */
-//#include <pmmintrin.h>/* SSE3 */
+#include <vec128int.h>
 
 #include <sys/time.h>
 
@@ -76,21 +73,21 @@ vect_sw_diff_gap(int8_t *seqA, int lena, int8_t *seqB, int lenb,
   (void)initbp;
 
 #define SET16(a, e7, e6, e5, e4, e3, e2, e1, e0)      \
-  _mm_set_epi16((int16_t)a[e7], (int16_t)a[e6], \
+  vec_set8sh((int16_t)a[e7], (int16_t)a[e6], \
                 (int16_t)a[e5], (int16_t)a[e4], \
                 (int16_t)a[e3], (int16_t)a[e2], \
                 (int16_t)a[e1], (int16_t)a[e0])
 
-  v_score		 = _mm_setzero_si128();
-  v_zero		 = _mm_setzero_si128();
+  v_score		 = vec_zero1q();
+  v_zero		 = vec_zero1q();
   v_match		 = SET16((&match), 0, 0, 0, 0, 0, 0, 0, 0);
   v_mismatch	 = SET16((&mismatch), 0, 0, 0, 0, 0, 0, 0, 0);
   v_a_gap_ext	 = SET16((&a_gap_ext), 0, 0, 0, 0, 0, 0, 0, 0);
   v_a_gap_open_ext = SET16((&a_gap_open), 0, 0, 0, 0, 0, 0, 0, 0);
-  v_a_gap_open_ext = _mm_add_epi16(v_a_gap_open_ext, v_a_gap_ext);
+  v_a_gap_open_ext = vec_add8sh(v_a_gap_open_ext, v_a_gap_ext);
   v_b_gap_ext	 = SET16((&b_gap_ext), 0, 0, 0, 0, 0, 0, 0, 0);
   v_b_gap_open_ext = SET16((&b_gap_open), 0, 0, 0, 0, 0, 0, 0, 0);
-  v_b_gap_open_ext = _mm_add_epi16(v_b_gap_open_ext, v_b_gap_ext);
+  v_b_gap_open_ext = vec_add8sh(v_b_gap_open_ext, v_b_gap_ext);
 
   for (i = 0; i < lena + 14; i++) {
       nogap[i] = 0;
@@ -106,48 +103,48 @@ vect_sw_diff_gap(int8_t *seqA, int lena, int8_t *seqB, int lenb,
       v_seq_b = SET16(seqB, k+7, k+6, k+5, k+4, k+3, k+2, k+1, k+0);
 
       v_a_gap = v_a_gap_ext;
-      v_a_gap = _mm_sub_epi16(v_a_gap, v_a_gap_open_ext);
+      v_a_gap = vec_subtract8sh(v_a_gap, v_a_gap_open_ext);
 
-      v_last_nogap = _mm_setzero_si128();
-      v_prev_nogap = _mm_setzero_si128();
+      v_last_nogap = vec_zero1q();
+      v_prev_nogap = vec_zero1q();
 
       for (j = 0; j < (lena + 7); j++) {
-          v_b_gap = _mm_slli_si128(v_b_gap, 2);
-          v_b_gap = _mm_insert_epi16(v_b_gap, b_gap[j+7], 0);
+          v_b_gap = vec_shiftleftbytes1q(v_b_gap, 2);
+          v_b_gap = vec_insert8sh(v_b_gap, b_gap[j+7], 0);
 
-          v_nogap = _mm_slli_si128(v_nogap, 2);
-          v_nogap = _mm_insert_epi16(v_nogap, nogap[j+7], 0);
+          v_nogap = vec_shiftleftbytes1q(v_nogap, 2);
+          v_nogap = vec_insert8sh(v_nogap, nogap[j+7], 0);
 
-          v_seq_a = _mm_slli_si128(v_seq_a, 2);
-          v_seq_a = _mm_insert_epi16(v_seq_a, seqA[j+7], 0);
+          v_seq_a = vec_shiftleftbytes1q(v_seq_a, 2);
+          v_seq_a = vec_insert8sh(v_seq_a, seqA[j+7], 0);
 
-          v_tmp = _mm_sub_epi16(v_last_nogap, v_a_gap_open_ext);
-          v_a_gap = _mm_sub_epi16(v_a_gap, v_a_gap_ext);
-          v_a_gap = _mm_max_epi16(v_a_gap, v_tmp);
+          v_tmp = vec_subtract8sh(v_last_nogap, v_a_gap_open_ext);
+          v_a_gap = vec_subtract8sh(v_a_gap, v_a_gap_ext);
+          v_a_gap = vec_max8sh(v_a_gap, v_tmp);
 
-          v_tmp = _mm_sub_epi16(v_nogap, v_b_gap_open_ext);
-          v_b_gap = _mm_sub_epi16(v_b_gap, v_b_gap_ext);
-          v_b_gap = _mm_max_epi16(v_b_gap, v_tmp);
+          v_tmp = vec_subtract8sh(v_nogap, v_b_gap_open_ext);
+          v_b_gap = vec_subtract8sh(v_b_gap, v_b_gap_ext);
+          v_b_gap = vec_max8sh(v_b_gap, v_tmp);
 
           /* compute the score (v_last_nogap is a tmp variable) */
-          v_last_nogap = _mm_cmpeq_epi16(v_seq_a, v_seq_b);
-          v_tmp = _mm_and_si128(v_last_nogap, v_match);
-          v_last_nogap = _mm_cmpeq_epi16(v_last_nogap, v_zero);
-          v_last_nogap = _mm_and_si128(v_last_nogap, v_mismatch);
-          v_tmp = _mm_or_si128(v_tmp, v_last_nogap);
+          v_last_nogap = vec_compareeq8sh(v_seq_a, v_seq_b);
+          v_tmp = vec_bitand1q(v_last_nogap, v_match);
+          v_last_nogap = vec_compareeq8sh(v_last_nogap, v_zero);
+          v_last_nogap = vec_bitand1q(v_last_nogap, v_mismatch);
+          v_tmp = vec_bitor1q(v_tmp, v_last_nogap);
 
-          v_last_nogap = _mm_add_epi16(v_prev_nogap, v_tmp);
-          v_last_nogap = _mm_max_epi16(v_last_nogap, v_zero);
-          v_last_nogap = _mm_max_epi16(v_last_nogap, v_a_gap);
-          v_last_nogap = _mm_max_epi16(v_last_nogap, v_b_gap);
+          v_last_nogap = vec_add8sh(v_prev_nogap, v_tmp);
+          v_last_nogap = vec_max8sh(v_last_nogap, v_zero);
+          v_last_nogap = vec_max8sh(v_last_nogap, v_a_gap);
+          v_last_nogap = vec_max8sh(v_last_nogap, v_b_gap);
 
           v_prev_nogap = v_nogap;
           v_nogap = v_last_nogap;
 
-          b_gap[j] = (int16_t)_mm_extract_epi16(v_b_gap, 7);
-          nogap[j] = (int16_t)_mm_extract_epi16(v_nogap, 7);
+          b_gap[j] = (int16_t)vec_extract8sh(v_b_gap, 7);
+          nogap[j] = (int16_t)vec_extract8sh(v_nogap, 7);
 
-          v_score = _mm_max_epi16(v_score, v_last_nogap);
+          v_score = vec_max8sh(v_score, v_last_nogap);
       }
   }
 
@@ -156,14 +153,14 @@ vect_sw_diff_gap(int8_t *seqA, int lena, int8_t *seqB, int lenb,
    * breaks strict-aliasing rules.
    */
   assert(score == 0);
-  score = MAX(score, _mm_extract_epi16(v_score, 0));
-  score = MAX(score, _mm_extract_epi16(v_score, 1));
-  score = MAX(score, _mm_extract_epi16(v_score, 2));
-  score = MAX(score, _mm_extract_epi16(v_score, 3));
-  score = MAX(score, _mm_extract_epi16(v_score, 4));
-  score = MAX(score, _mm_extract_epi16(v_score, 5));
-  score = MAX(score, _mm_extract_epi16(v_score, 6));
-  score = MAX(score, _mm_extract_epi16(v_score, 7));
+  score = MAX(score, vec_extract8sh(v_score, 0));
+  score = MAX(score, vec_extract8sh(v_score, 1));
+  score = MAX(score, vec_extract8sh(v_score, 2));
+  score = MAX(score, vec_extract8sh(v_score, 3));
+  score = MAX(score, vec_extract8sh(v_score, 4));
+  score = MAX(score, vec_extract8sh(v_score, 5));
+  score = MAX(score, vec_extract8sh(v_score, 6));
+  score = MAX(score, vec_extract8sh(v_score, 7));
 
   return (score);
 }
@@ -196,21 +193,21 @@ vect_sw_same_gap(int8_t *seqA, int lena, int8_t *seqB, int lenb,
   (void)initbp;
 
 #define SET16(a, e7, e6, e5, e4, e3, e2, e1, e0)      \
-  _mm_set_epi16((int16_t)a[e7], (int16_t)a[e6], \
+  vec_set8sh((int16_t)a[e7], (int16_t)a[e6], \
                 (int16_t)a[e5], (int16_t)a[e4], \
                 (int16_t)a[e3], (int16_t)a[e2], \
                 (int16_t)a[e1], (int16_t)a[e0])
 
-  v_score		 = _mm_setzero_si128();
-  v_zero		 = _mm_setzero_si128();
+  v_score		 = vec_zero1q();
+  v_zero		 = vec_zero1q();
   v_match		 = SET16((&match), 0, 0, 0, 0, 0, 0, 0, 0);
   v_mismatch	 = SET16((&mismatch), 0, 0, 0, 0, 0, 0, 0, 0);
   v_a_gap_ext	 = SET16((&a_gap_ext), 0, 0, 0, 0, 0, 0, 0, 0);
   v_a_gap_open_ext = SET16((&a_gap_open), 0, 0, 0, 0, 0, 0, 0, 0);
-  v_a_gap_open_ext = _mm_add_epi16(v_a_gap_open_ext, v_a_gap_ext);
+  v_a_gap_open_ext = vec_add8sh(v_a_gap_open_ext, v_a_gap_ext);
   v_b_gap_ext	 = SET16((&b_gap_ext), 0, 0, 0, 0, 0, 0, 0, 0);
   v_b_gap_open_ext = SET16((&b_gap_open), 0, 0, 0, 0, 0, 0, 0, 0);
-  v_b_gap_open_ext = _mm_add_epi16(v_b_gap_open_ext, v_b_gap_ext);
+  v_b_gap_open_ext = vec_add8sh(v_b_gap_open_ext, v_b_gap_ext);
 
   for (i = 0; i < lena + 14; i++) {
       nogap[i] = 0;
@@ -226,48 +223,48 @@ vect_sw_same_gap(int8_t *seqA, int lena, int8_t *seqB, int lenb,
       v_seq_b = SET16(seqB, k+7, k+6, k+5, k+4, k+3, k+2, k+1, k+0);
 
       v_a_gap = v_a_gap_ext;
-      v_a_gap = _mm_sub_epi16(v_a_gap, v_a_gap_open_ext);
+      v_a_gap = vec_subtract8sh(v_a_gap, v_a_gap_open_ext);
 
-      v_last_nogap = _mm_setzero_si128();
-      v_prev_nogap = _mm_setzero_si128();
+      v_last_nogap = vec_zero1q();
+      v_prev_nogap = vec_zero1q();
 
       for (j = 0; j < (lena + 7); j++) {
-          v_b_gap = _mm_slli_si128(v_b_gap, 2);
-          v_b_gap = _mm_insert_epi16(v_b_gap, b_gap[j+7], 0);
+          v_b_gap = vec_shiftleftbytes1q(v_b_gap, 2);
+          v_b_gap = vec_insert8sh(v_b_gap, b_gap[j+7], 0);
 
-          v_nogap = _mm_slli_si128(v_nogap, 2);
-          v_nogap = _mm_insert_epi16(v_nogap, nogap[j+7], 0);
+          v_nogap = vec_shiftleftbytes1q(v_nogap, 2);
+          v_nogap = vec_insert8sh(v_nogap, nogap[j+7], 0);
 
-          v_seq_a = _mm_slli_si128(v_seq_a, 2);
-          v_seq_a = _mm_insert_epi16(v_seq_a, seqA[j+7], 0);
+          v_seq_a = vec_shiftleftbytes1q(v_seq_a, 2);
+          v_seq_a = vec_insert8sh(v_seq_a, seqA[j+7], 0);
 
-          v_tmp = _mm_sub_epi16(v_last_nogap, v_a_gap_open_ext);
-          v_a_gap = _mm_sub_epi16(v_a_gap, v_a_gap_ext);
-          v_a_gap = _mm_max_epi16(v_a_gap, v_tmp);
+          v_tmp = vec_subtract8sh(v_last_nogap, v_a_gap_open_ext);
+          v_a_gap = vec_subtract8sh(v_a_gap, v_a_gap_ext);
+          v_a_gap = vec_max8sh(v_a_gap, v_tmp);
 
-          v_tmp = _mm_sub_epi16(v_nogap, v_b_gap_open_ext);
-          v_b_gap = _mm_sub_epi16(v_b_gap, v_b_gap_ext);
-          v_b_gap = _mm_max_epi16(v_b_gap, v_tmp);
+          v_tmp = vec_subtract8sh(v_nogap, v_b_gap_open_ext);
+          v_b_gap = vec_subtract8sh(v_b_gap, v_b_gap_ext);
+          v_b_gap = vec_max8sh(v_b_gap, v_tmp);
 
           /* compute the score (v_last_nogap is a tmp variable) */
-          v_last_nogap = _mm_cmpeq_epi16(v_seq_a, v_seq_b);
-          v_tmp = _mm_and_si128(v_last_nogap, v_match);
-          v_last_nogap = _mm_cmpeq_epi16(v_last_nogap, v_zero);
-          v_last_nogap = _mm_and_si128(v_last_nogap, v_mismatch);
-          v_tmp = _mm_or_si128(v_tmp, v_last_nogap);
+          v_last_nogap = vec_compareeq8sh(v_seq_a, v_seq_b);
+          v_tmp = vec_bitand1q(v_last_nogap, v_match);
+          v_last_nogap = vec_compareeq8sh(v_last_nogap, v_zero);
+          v_last_nogap = vec_bitand1q(v_last_nogap, v_mismatch);
+          v_tmp = vec_bitor1q(v_tmp, v_last_nogap);
 
-          v_last_nogap = _mm_add_epi16(v_prev_nogap, v_tmp);
-          v_last_nogap = _mm_max_epi16(v_last_nogap, v_zero);
-          v_last_nogap = _mm_max_epi16(v_last_nogap, v_a_gap);
-          v_last_nogap = _mm_max_epi16(v_last_nogap, v_b_gap);
+          v_last_nogap = vec_add8sh(v_prev_nogap, v_tmp);
+          v_last_nogap = vec_max8sh(v_last_nogap, v_zero);
+          v_last_nogap = vec_max8sh(v_last_nogap, v_a_gap);
+          v_last_nogap = vec_max8sh(v_last_nogap, v_b_gap);
 
           v_prev_nogap = v_nogap;
           v_nogap = v_last_nogap;
 
-          b_gap[j] = (int16_t)_mm_extract_epi16(v_b_gap, 7);
-          nogap[j] = (int16_t)_mm_extract_epi16(v_nogap, 7);
+          b_gap[j] = (int16_t)vec_extract8sh(v_b_gap, 7);
+          nogap[j] = (int16_t)vec_extract8sh(v_nogap, 7);
 
-          v_score = _mm_max_epi16(v_score, v_last_nogap);
+          v_score = vec_max8sh(v_score, v_last_nogap);
       }
   }
 
@@ -276,14 +273,14 @@ vect_sw_same_gap(int8_t *seqA, int lena, int8_t *seqB, int lenb,
    * breaks strict-aliasing rules.
    */
   assert(score == 0);
-  score = MAX(score, _mm_extract_epi16(v_score, 0));
-  score = MAX(score, _mm_extract_epi16(v_score, 1));
-  score = MAX(score, _mm_extract_epi16(v_score, 2));
-  score = MAX(score, _mm_extract_epi16(v_score, 3));
-  score = MAX(score, _mm_extract_epi16(v_score, 4));
-  score = MAX(score, _mm_extract_epi16(v_score, 5));
-  score = MAX(score, _mm_extract_epi16(v_score, 6));
-  score = MAX(score, _mm_extract_epi16(v_score, 7));
+  score = MAX(score, vec_extract8sh(v_score, 0));
+  score = MAX(score, vec_extract8sh(v_score, 1));
+  score = MAX(score, vec_extract8sh(v_score, 2));
+  score = MAX(score, vec_extract8sh(v_score, 3));
+  score = MAX(score, vec_extract8sh(v_score, 4));
+  score = MAX(score, vec_extract8sh(v_score, 5));
+  score = MAX(score, vec_extract8sh(v_score, 6));
+  score = MAX(score, vec_extract8sh(v_score, 7));
 
   return (score);
 }
--- src/sw/lib/vsw.cpp
+++ src/sw/lib/vsw.cpp
@@ -26,7 +26,7 @@
 
 #include <stdlib.h>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <string.h>
 #include <unistd.h>
 #include <stdio.h>
--- src/sw/lib/vsw.h
+++ src/sw/lib/vsw.h
@@ -31,7 +31,7 @@
 
 #include <stdlib.h>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <unistd.h>
 
 //#define VSW_DEBUG 1
@@ -40,28 +40,28 @@
 #define __vsw_16(_val) (((_val) + 15) >> 4 << 4)
 
 /** Global SIMD macros **/
-#define __vsw_mm_store_si128(_dest, _val) _mm_store_si128(_dest, _val)
-#define __vsw_mm_load_si128(_src) _mm_load_si128(_src)
-#define __vsw_mm_slli_si128(_src, _imm) _mm_slli_si128(_src, _imm)
-#define __vsw_mm_srli_si128(_src, _imm) _mm_srli_si128(_src, _imm)
+#define __vsw_mm_store_si128(_dest, _val) vec_store1q(_dest, _val)
+#define __vsw_mm_load_si128(_src) vec_load1q(_src)
+#define __vsw_mm_slli_si128(_src, _imm) vec_shiftleftbytes1q(_src, _imm)
+#define __vsw_mm_srli_si128(_src, _imm) vec_shiftrightbytes1q(_src, _imm)
 
 /** VSW MM 16-byte mode (8 values per 128)  **/
-#define __vsw16_mm_store_si128(_dest, _val) _mm_store_si128(_dest, _val)
-#define __vsw16_mm_load_si128(_src) _mm_load_si128(_src)
-#define __vsw16_mm_slli_si128(_src, _imm) _mm_slli_si128(_src, _imm)
-#define __vsw16_mm_srli_si128(_src, _imm) _mm_srli_si128(_src, _imm)
-#define __vsw16_mm_set1_epi16(_val) _mm_set1_epi16(_val)
-#define __vsw16_mm_set_epi16(_r0, _r1, _r2, _r3, _r4, _r5, _r6, _r7) _mm_set_epi16(_r7, _r6, _r5, _r4, _r3, _r2, _r1, _r0)
-#define __vsw16_mm_adds_epi16(_a, _b) _mm_adds_epi16(_a, _b)
-#define __vsw16_mm_subs_epi16(_a, _b) _mm_subs_epi16(_a, _b)
-#define __vsw16_mm_max_epi16(_a, _b) _mm_max_epi16(_a, _b)
-#define __vsw16_mm_min_epi16(_a, _b) _mm_min_epi16(_a, _b)
-#define __vsw16_mm_movemask_epi16(_a) _mm_movemask_epi8(_a) // NB: this should be 8-bit...
-#define __vsw16_mm_cmpeq_epi16(_a, _b) _mm_cmpeq_epi16(_a, _b)
-#define __vsw16_mm_cmplt_epi16(_a, _b) _mm_cmplt_epi16(_a, _b)
-#define __vsw16_mm_cmpgt_epi16(_a, _b) _mm_cmpgt_epi16(_a, _b)
-#define __vsw16_mm_insert_epi16(_a, _b, _imm) _mm_insert_epi16(_a, _b, _imm)
-#define __vsw16_mm_extract_epi16(_a, _imm) _mm_extract_epi16(_a, _imm)
+#define __vsw16_mm_store_si128(_dest, _val) vec_store1q(_dest, _val)
+#define __vsw16_mm_load_si128(_src) vec_load1q(_src)
+#define __vsw16_mm_slli_si128(_src, _imm) vec_shiftleftbytes1q(_src, _imm)
+#define __vsw16_mm_srli_si128(_src, _imm) vec_shiftrightbytes1q(_src, _imm)
+#define __vsw16_mm_set1_epi16(_val) vec_splat8sh(_val)
+#define __vsw16_mm_set_epi16(_r0, _r1, _r2, _r3, _r4, _r5, _r6, _r7) vec_set8sh(_r7, _r6, _r5, _r4, _r3, _r2, _r1, _r0)
+#define __vsw16_mm_adds_epi16(_a, _b) vec_addsaturating8sh(_a, _b)
+#define __vsw16_mm_subs_epi16(_a, _b) vec_subtractsaturating8sh(_a, _b)
+#define __vsw16_mm_max_epi16(_a, _b) vec_max8sh(_a, _b)
+#define __vsw16_mm_min_epi16(_a, _b) vec_min8sh(_a, _b)
+#define __vsw16_mm_movemask_epi16(_a) vec_extractupperbit16sb(_a) // NB: this should be 8-bit...
+#define __vsw16_mm_cmpeq_epi16(_a, _b) vec_compareeq8sh(_a, _b)
+#define __vsw16_mm_cmplt_epi16(_a, _b) vec_comparelt8sh(_a, _b)
+#define __vsw16_mm_cmpgt_epi16(_a, _b) vec_comparegt8sh(_a, _b)
+#define __vsw16_mm_insert_epi16(_a, _b, _imm) vec_insert8sh(_a, _b, _imm)
+#define __vsw16_mm_extract_epi16(_a, _imm) vec_extract8sh(_a, _imm)
 #define vsw16_values_per_128_bits 8 
 #define vsw16_values_per_128_bits_log2 3 // should be log2(values_per_128_bits)
 #define vsw16_max_value INT16_MAX
@@ -74,16 +74,16 @@ typedef int16_t vsw16_int_t;
 #define __vsw16_calc_slen(_qlen) (((_qlen) + vsw16_values_per_128_bits - 1) >> vsw16_values_per_128_bits_log2)
 // returns the maximum value in stored in the vector
 #define __vsw16_max(ret, xx) do { \
-    (xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 8)); \
-    (xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 4)); \
-    (xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 2)); \
+    (xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 8)); \
+    (xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 4)); \
+    (xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 2)); \
     (ret) = (vsw16_int_t)(__vsw16_mm_extract_epi16((xx), 0) & 0xffff); \
 } while (0) 
 // returns the minimum value in stored in the vector
 #define __vsw16_min(ret, xx) do { \
-    (xx) = _mm_min_epi16((xx), _mm_srli_si128((xx), 8)); \
-    (xx) = _mm_min_epi16((xx), _mm_srli_si128((xx), 4)); \
-    (xx) = _mm_min_epi16((xx), _mm_srli_si128((xx), 2)); \
+    (xx) = vec_min8sh((xx), vec_shiftrightbytes1q((xx), 8)); \
+    (xx) = vec_min8sh((xx), vec_shiftrightbytes1q((xx), 4)); \
+    (xx) = vec_min8sh((xx), vec_shiftrightbytes1q((xx), 2)); \
     (ret) = (vsw16_int_t)(__vsw16_mm_extract_epi16((xx), 0) & 0xffff); \
 } while (0) 
 // Inserts a value into the given location in the vector.  This overcomes the immediate problem when compiling.
--- src/sw/lib/vsw16.cpp
+++ src/sw/lib/vsw16.cpp
@@ -27,7 +27,7 @@
 #include <stdlib.h>
 #include <stdint.h>
 #include <limits>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <unistd.h>
 #include <stdio.h>
 #include "vsw.h"
--- src/sw/lib/vsw16.h
+++ src/sw/lib/vsw16.h
@@ -30,7 +30,7 @@
 
 #include <stdlib.h>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <unistd.h>
 #include <stdio.h>
 #include "vsw.h"
--- src/sw/tmap_vsw.c
+++ src/sw/tmap_vsw.c
@@ -26,7 +26,7 @@
 
 #include <stdlib.h>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <unistd.h>
 #include <stdio.h>
 #include <config.h>
--- src/sw/tmap_vsw.h
+++ src/sw/tmap_vsw.h
@@ -32,7 +32,7 @@
 
 #include <stdlib.h>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <unistd.h>
 #include "tmap_vsw_definitions.h"
 #include "lib/AffineSWOptimizationWrapper.h"
--- src/sw/tmap_vsw_definitions.h
+++ src/sw/tmap_vsw_definitions.h
@@ -31,35 +31,35 @@
 
 #include <stdlib.h>
 #include <stdint.h>
-#include <emmintrin.h>
+#include <vec128int.h>
 #include <unistd.h>
 
 // Gives the # of bytes to align the memory in 16-byte increments
 #define __tmap_vsw_16(_val) (((_val) + 15) >> 4 << 4)
 
 /** Global SIMD macros **/
-#define __tmap_vsw_mm_store_si128(_dest, _val) _mm_store_si128(_dest, _val)
-#define __tmap_vsw_mm_load_si128(_src) _mm_load_si128(_src)
-#define __tmap_vsw_mm_slli_si128(_src, _imm) _mm_slli_si128(_src, _imm)
-#define __tmap_vsw_mm_srli_si128(_src, _imm) _mm_srli_si128(_src, _imm)
+#define __tmap_vsw_mm_store_si128(_dest, _val) vec_store1q(_dest, _val)
+#define __tmap_vsw_mm_load_si128(_src) vec_load1q(_src)
+#define __tmap_vsw_mm_slli_si128(_src, _imm) vec_shiftleftbytes1q(_src, _imm)
+#define __tmap_vsw_mm_srli_si128(_src, _imm) vec_shiftrightbytes1q(_src, _imm)
 
 /** VSW MM 16-byte mode (8 values per 128)  **/
-#define __tmap_vsw16_mm_store_si128(_dest, _val) _mm_store_si128(_dest, _val)
-#define __tmap_vsw16_mm_load_si128(_src) _mm_load_si128(_src)
-#define __tmap_vsw16_mm_slli_si128(_src, _imm) _mm_slli_si128(_src, _imm)
-#define __tmap_vsw16_mm_srli_si128(_src, _imm) _mm_srli_si128(_src, _imm)
-#define __tmap_vsw16_mm_set1_epi16(_val) _mm_set1_epi16(_val)
-#define __tmap_vsw16_mm_set_epi16(_r0, _r1, _r2, _r3, _r4, _r5, _r6, _r7) _mm_set_epi16(_r7, _r6, _r5, _r4, _r3, _r2, _r1, _r0)
-#define __tmap_vsw16_mm_adds_epi16(_a, _b) _mm_adds_epi16(_a, _b)
-#define __tmap_vsw16_mm_subs_epi16(_a, _b) _mm_subs_epi16(_a, _b)
-#define __tmap_vsw16_mm_max_epi16(_a, _b) _mm_max_epi16(_a, _b)
-#define __tmap_vsw16_mm_min_epi16(_a, _b) _mm_min_epi16(_a, _b)
-#define __tmap_vsw16_mm_movemask_epi16(_a) _mm_movemask_epi8(_a) // NB: this should be 8-bit...
-#define __tmap_vsw16_mm_cmpeq_epi16(_a, _b) _mm_cmpeq_epi16(_a, _b)
-#define __tmap_vsw16_mm_cmplt_epi16(_a, _b) _mm_cmplt_epi16(_a, _b)
-#define __tmap_vsw16_mm_cmpgt_epi16(_a, _b) _mm_cmpgt_epi16(_a, _b)
-#define __tmap_vsw16_mm_insert_epi16(_a, _b, _imm) _mm_insert_epi16(_a, _b, _imm)
-#define __tmap_vsw16_mm_extract_epi16(_a, _imm) _mm_extract_epi16(_a, _imm)
+#define __tmap_vsw16_mm_store_si128(_dest, _val) vec_store1q(_dest, _val)
+#define __tmap_vsw16_mm_load_si128(_src) vec_load1q(_src)
+#define __tmap_vsw16_mm_slli_si128(_src, _imm) vec_shiftleftbytes1q(_src, _imm)
+#define __tmap_vsw16_mm_srli_si128(_src, _imm) vec_shiftrightbytes1q(_src, _imm)
+#define __tmap_vsw16_mm_set1_epi16(_val) vec_splat8sh(_val)
+#define __tmap_vsw16_mm_set_epi16(_r0, _r1, _r2, _r3, _r4, _r5, _r6, _r7) vec_set8sh(_r7, _r6, _r5, _r4, _r3, _r2, _r1, _r0)
+#define __tmap_vsw16_mm_adds_epi16(_a, _b) vec_addsaturating8sh(_a, _b)
+#define __tmap_vsw16_mm_subs_epi16(_a, _b) vec_subtractsaturating8sh(_a, _b)
+#define __tmap_vsw16_mm_max_epi16(_a, _b) vec_max8sh(_a, _b)
+#define __tmap_vsw16_mm_min_epi16(_a, _b) vec_min8sh(_a, _b)
+#define __tmap_vsw16_mm_movemask_epi16(_a) vec_extractupperbit16sb(_a) // NB: this should be 8-bit...
+#define __tmap_vsw16_mm_cmpeq_epi16(_a, _b) vec_compareeq8sh(_a, _b)
+#define __tmap_vsw16_mm_cmplt_epi16(_a, _b) vec_comparelt8sh(_a, _b)
+#define __tmap_vsw16_mm_cmpgt_epi16(_a, _b) vec_comparegt8sh(_a, _b)
+#define __tmap_vsw16_mm_insert_epi16(_a, _b, _imm) vec_insert8sh(_a, _b, _imm)
+#define __tmap_vsw16_mm_extract_epi16(_a, _imm) vec_extract8sh(_a, _imm)
 #define tmap_vsw16_values_per_128_bits 8 
 #define tmap_vsw16_values_per_128_bits_log2 3 // should be log2(values_per_128_bits)
 #define tmap_vsw16_max_value INT16_MAX
@@ -72,16 +72,16 @@ typedef int16_t tmap_vsw16_int_t;
 #define __tmap_vsw16_calc_slen(_qlen) (((_qlen) + tmap_vsw16_values_per_128_bits - 1) >> tmap_vsw16_values_per_128_bits_log2)
 // returns the maximum value in stored in the vector
 #define __tmap_vsw16_max(ret, xx) do { \
-    (xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 8)); \
-    (xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 4)); \
-    (xx) = _mm_max_epi16((xx), _mm_srli_si128((xx), 2)); \
+    (xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 8)); \
+    (xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 4)); \
+    (xx) = vec_max8sh((xx), vec_shiftrightbytes1q((xx), 2)); \
     (ret) = (tmap_vsw16_int_t)(__tmap_vsw16_mm_extract_epi16((xx), 0) & 0xffff); \
 } while (0) 
 // returns the minimum value in stored in the vector
 #define __tmap_vsw16_min(ret, xx) do { \
-    (xx) = _mm_min_epi16((xx), _mm_srli_si128((xx), 8)); \
-    (xx) = _mm_min_epi16((xx), _mm_srli_si128((xx), 4)); \
-    (xx) = _mm_min_epi16((xx), _mm_srli_si128((xx), 2)); \
+    (xx) = vec_min8sh((xx), vec_shiftrightbytes1q((xx), 8)); \
+    (xx) = vec_min8sh((xx), vec_shiftrightbytes1q((xx), 4)); \
+    (xx) = vec_min8sh((xx), vec_shiftrightbytes1q((xx), 2)); \
     (ret) = (tmap_vsw16_int_t)(__tmap_vsw16_mm_extract_epi16((xx), 0) & 0xffff); \
 } while (0) 
 // Inserts a value into the given location in the vector.  This overcomes the immediate problem when compiling.
--- src/sw/lib/sw-vector.cpp.orig
+++ src/sw/lib/sw-vector.cpp
@@ -4,7 +4,6 @@
 #include <ctype.h>
 #include <errno.h>
 #include <math.h>
-#include <stdbool.h>
 #include <stdio.h>
 #include <stdlib.h>
 #include <stdint.h>
