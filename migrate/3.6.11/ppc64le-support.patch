--- src/SFMT-src-1.3.3/SFMT-sse2.h
+++ src/SFMT-src-1.3.3/SFMT-sse2.h
@@ -32,16 +32,16 @@
 				   __m128i c, __m128i d, __m128i mask) {
     __m128i v, x, y, z;
     
-    x = _mm_load_si128(a);
-    y = _mm_srli_epi32(*b, SR1);
-    z = _mm_srli_si128(c, SR2);
-    v = _mm_slli_epi32(d, SL1);
-    z = _mm_xor_si128(z, x);
-    z = _mm_xor_si128(z, v);
-    x = _mm_slli_si128(x, SL2);
-    y = _mm_and_si128(y, mask);
-    z = _mm_xor_si128(z, x);
-    z = _mm_xor_si128(z, y);
+    x = vec_load1q(a);
+    y = vec_shiftrightimmediate4sw(*b, SR1);
+    z = vec_shiftrightbytes1q(c, SR2);
+    v = vec_shiftleftimmediate4sw(d, SL1);
+    z = vec_bitxor1q(z, x);
+    z = vec_bitxor1q(z, v);
+    x = vec_shiftleftbytes1q(x, SL2);
+    y = vec_bitand1q(y, mask);
+    z = vec_bitxor1q(z, x);
+    z = vec_bitxor1q(z, y);
     return z;
 }
 
@@ -52,19 +52,19 @@
 inline static void gen_rand_all(void) {
     int i;
     __m128i r, r1, r2, mask;
-    mask = _mm_set_epi32(MSK4, MSK3, MSK2, MSK1);
+    mask = vec_set4sw(MSK4, MSK3, MSK2, MSK1);
 
-    r1 = _mm_load_si128(&sfmt[N - 2].si);
-    r2 = _mm_load_si128(&sfmt[N - 1].si);
+    r1 = vec_load1q(&sfmt[N - 2].si);
+    r2 = vec_load1q(&sfmt[N - 1].si);
     for (i = 0; i < N - POS1; i++) {
 	r = mm_recursion(&sfmt[i].si, &sfmt[i + POS1].si, r1, r2, mask);
-	_mm_store_si128(&sfmt[i].si, r);
+	vec_store1q(&sfmt[i].si, r);
 	r1 = r2;
 	r2 = r;
     }
     for (; i < N; i++) {
 	r = mm_recursion(&sfmt[i].si, &sfmt[i + POS1 - N].si, r1, r2, mask);
-	_mm_store_si128(&sfmt[i].si, r);
+	vec_store1q(&sfmt[i].si, r);
 	r1 = r2;
 	r2 = r;
     }
@@ -80,19 +80,19 @@
 inline static void gen_rand_array(w128_t *array, int size) {
     int i, j;
     __m128i r, r1, r2, mask;
-    mask = _mm_set_epi32(MSK4, MSK3, MSK2, MSK1);
+    mask = vec_set4sw(MSK4, MSK3, MSK2, MSK1);
 
-    r1 = _mm_load_si128(&sfmt[N - 2].si);
-    r2 = _mm_load_si128(&sfmt[N - 1].si);
+    r1 = vec_load1q(&sfmt[N - 2].si);
+    r2 = vec_load1q(&sfmt[N - 1].si);
     for (i = 0; i < N - POS1; i++) {
 	r = mm_recursion(&sfmt[i].si, &sfmt[i + POS1].si, r1, r2, mask);
-	_mm_store_si128(&array[i].si, r);
+	vec_store1q(&array[i].si, r);
 	r1 = r2;
 	r2 = r;
     }
     for (; i < N; i++) {
 	r = mm_recursion(&sfmt[i].si, &array[i + POS1 - N].si, r1, r2, mask);
-	_mm_store_si128(&array[i].si, r);
+	vec_store1q(&array[i].si, r);
 	r1 = r2;
 	r2 = r;
     }
@@ -100,19 +100,19 @@
     for (; i < size - N; i++) {
 	r = mm_recursion(&array[i - N].si, &array[i + POS1 - N].si, r1, r2,
 			 mask);
-	_mm_store_si128(&array[i].si, r);
+	vec_store1q(&array[i].si, r);
 	r1 = r2;
 	r2 = r;
     }
     for (j = 0; j < 2 * N - size; j++) {
-	r = _mm_load_si128(&array[j + size - N].si);
-	_mm_store_si128(&sfmt[j].si, r);
+	r = vec_load1q(&array[j + size - N].si);
+	vec_store1q(&sfmt[j].si, r);
     }
     for (; i < size; i++) {
 	r = mm_recursion(&array[i - N].si, &array[i + POS1 - N].si, r1, r2,
 			 mask);
-	_mm_store_si128(&array[i].si, r);
-	_mm_store_si128(&sfmt[j++].si, r);
+	vec_store1q(&array[i].si, r);
+	vec_store1q(&sfmt[j++].si, r);
 	r1 = r2;
 	r2 = r;
     }
--- src/SFMT-src-1.3.3/SFMT.c
+++ src/SFMT-src-1.3.3/SFMT.c
@@ -43,7 +43,7 @@
 typedef union W128_T w128_t;
 
 #elif defined(HAVE_SSE2)
-  #include <emmintrin.h>
+  #include <vec128int.h>
 
 /** 128-bit data structure */
 union W128_T {
--- src/configure
+++ src/configure
@@ -4589,7 +4589,7 @@
 
 $as_echo "#define HAVE_SSE2 1" >>confdefs.h
 
-  HARDWARECFLAGS="-msse2 -DSSE2"
+  HARDWARECFLAGS="-DSSE2"
 fi
 
 if test "$have_altivec" = "yes"; then
